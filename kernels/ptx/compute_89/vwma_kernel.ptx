//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_89
.address_size 64

	// .globl	vwma_prefix_pv_vol_f64_f32

.visible .entry vwma_prefix_pv_vol_f64_f32(
	.param .u64 vwma_prefix_pv_vol_f64_f32_param_0,
	.param .u64 vwma_prefix_pv_vol_f64_f32_param_1,
	.param .u32 vwma_prefix_pv_vol_f64_f32_param_2,
	.param .u32 vwma_prefix_pv_vol_f64_f32_param_3,
	.param .u64 vwma_prefix_pv_vol_f64_f32_param_4,
	.param .u64 vwma_prefix_pv_vol_f64_f32_param_5
)
{
	.reg .pred 	%p<34>;
	.reg .f32 	%f<21>;
	.reg .b32 	%r<46>;
	.reg .f64 	%fd<117>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd37, [vwma_prefix_pv_vol_f64_f32_param_0];
	ld.param.u64 	%rd38, [vwma_prefix_pv_vol_f64_f32_param_1];
	ld.param.u32 	%r20, [vwma_prefix_pv_vol_f64_f32_param_2];
	ld.param.u32 	%r21, [vwma_prefix_pv_vol_f64_f32_param_3];
	ld.param.u64 	%rd39, [vwma_prefix_pv_vol_f64_f32_param_4];
	ld.param.u64 	%rd40, [vwma_prefix_pv_vol_f64_f32_param_5];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r22, %tid.x;
	mov.u32 	%r23, %ctaid.x;
	or.b32  	%r24, %r22, %r23;
	setp.ne.s32 	%p1, %r24, 0;
	setp.lt.s32 	%p2, %r20, 1;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_40;

	max.s32 	%r1, %r21, 0;
	min.s32 	%r44, %r1, %r20;
	setp.lt.s32 	%p4, %r44, 1;
	@%p4 bra 	$L__BB0_8;

	add.s32 	%r26, %r44, -1;
	and.b32  	%r41, %r44, 3;
	setp.lt.u32 	%p5, %r26, 3;
	mov.u32 	%r40, 0;
	@%p5 bra 	$L__BB0_5;

	not.b32 	%r28, %r1;
	not.b32 	%r29, %r20;
	max.s32 	%r30, %r28, %r29;
	add.s32 	%r31, %r30, %r41;
	neg.s32 	%r38, %r31;
	mov.u32 	%r40, 0;
	mov.u64 	%rd54, %rd1;
	mov.u64 	%rd55, %rd2;

$L__BB0_4:
	mov.u64 	%rd41, 0;
	st.global.u64 	[%rd55], %rd41;
	st.global.u64 	[%rd54], %rd41;
	st.global.u64 	[%rd55+8], %rd41;
	st.global.u64 	[%rd54+8], %rd41;
	st.global.u64 	[%rd55+16], %rd41;
	st.global.u64 	[%rd54+16], %rd41;
	st.global.u64 	[%rd55+24], %rd41;
	st.global.u64 	[%rd54+24], %rd41;
	add.s32 	%r40, %r40, 4;
	add.s64 	%rd55, %rd55, 32;
	add.s64 	%rd54, %rd54, 32;
	add.s32 	%r38, %r38, -4;
	setp.ne.s32 	%p6, %r38, 1;
	@%p6 bra 	$L__BB0_4;

$L__BB0_5:
	setp.eq.s32 	%p7, %r41, 0;
	@%p7 bra 	$L__BB0_8;

	mul.wide.s32 	%rd42, %r40, 8;
	add.s64 	%rd57, %rd1, %rd42;
	add.s64 	%rd56, %rd2, %rd42;

$L__BB0_7:
	.pragma "nounroll";
	mov.u64 	%rd43, 0;
	st.global.u64 	[%rd56], %rd43;
	st.global.u64 	[%rd57], %rd43;
	add.s64 	%rd57, %rd57, 8;
	add.s64 	%rd56, %rd56, 8;
	add.s32 	%r41, %r41, -1;
	setp.ne.s32 	%p8, %r41, 0;
	@%p8 bra 	$L__BB0_7;

$L__BB0_8:
	setp.ge.s32 	%p9, %r1, %r20;
	@%p9 bra 	$L__BB0_40;

	not.b32 	%r32, %r1;
	add.s32 	%r33, %r32, %r20;
	add.s32 	%r34, %r33, 1;
	and.b32  	%r43, %r34, 3;
	setp.eq.s32 	%p10, %r43, 0;
	mov.f64 	%fd116, 0d0000000000000000;
	mov.f64 	%fd103, %fd116;
	@%p10 bra 	$L__BB0_17;

	mul.wide.s32 	%rd44, %r44, 8;
	add.s64 	%rd61, %rd1, %rd44;
	add.s64 	%rd60, %rd2, %rd44;
	cvta.to.global.u64 	%rd45, %rd38;
	mul.wide.s32 	%rd46, %r44, 4;
	add.s64 	%rd59, %rd45, %rd46;
	cvta.to.global.u64 	%rd47, %rd37;
	add.s64 	%rd58, %rd47, %rd46;
	mov.f64 	%fd116, 0d0000000000000000;

$L__BB0_11:
	.pragma "nounroll";
	mov.f64 	%fd2, %fd103;
	mov.f64 	%fd1, %fd116;
	ld.global.nc.f32 	%f1, [%rd59];
	ld.global.nc.f32 	%f2, [%rd58];
	abs.ftz.f32 	%f11, %f2;
	setp.gtu.ftz.f32 	%p11, %f11, 0f7F800000;
	mov.f64 	%fd103, 0d7FF8000000000000;
	mov.f64 	%fd116, 0d7FF8000000000000;
	@%p11 bra 	$L__BB0_16;

	abs.ftz.f32 	%f12, %f1;
	setp.gtu.ftz.f32 	%p12, %f12, 0f7F800000;
	@%p12 bra 	$L__BB0_16;

	abs.f64 	%fd37, %fd2;
	setp.gtu.f64 	%p13, %fd37, 0d7FF0000000000000;
	@%p13 bra 	$L__BB0_16;

	abs.f64 	%fd40, %fd1;
	setp.gtu.f64 	%p14, %fd40, 0d7FF0000000000000;
	@%p14 bra 	$L__BB0_16;

	cvt.ftz.f64.f32 	%fd41, %f2;
	cvt.ftz.f64.f32 	%fd42, %f1;
	fma.rn.f64 	%fd103, %fd41, %fd42, %fd2;
	add.f64 	%fd116, %fd1, %fd42;

$L__BB0_16:
	st.global.f64 	[%rd60], %fd103;
	st.global.f64 	[%rd61], %fd116;
	add.s32 	%r44, %r44, 1;
	add.s64 	%rd61, %rd61, 8;
	add.s64 	%rd60, %rd60, 8;
	add.s64 	%rd59, %rd59, 4;
	add.s64 	%rd58, %rd58, 4;
	add.s32 	%r43, %r43, -1;
	setp.ne.s32 	%p15, %r43, 0;
	@%p15 bra 	$L__BB0_11;

$L__BB0_17:
	setp.lt.u32 	%p16, %r33, 3;
	@%p16 bra 	$L__BB0_40;

	cvta.to.global.u64 	%rd48, %rd37;
	mul.wide.s32 	%rd49, %r44, 4;
	add.s64 	%rd62, %rd48, %rd49;
	cvta.to.global.u64 	%rd50, %rd38;
	add.s64 	%rd63, %rd50, %rd49;
	cvta.to.global.u64 	%rd51, %rd39;
	mul.wide.s32 	%rd52, %r44, 8;
	add.s64 	%rd64, %rd51, %rd52;
	cvta.to.global.u64 	%rd53, %rd40;
	add.s64 	%rd65, %rd53, %rd52;

$L__BB0_19:
	mov.u64 	%rd32, %rd65;
	mov.u64 	%rd31, %rd64;
	ld.global.nc.f32 	%f3, [%rd63];
	ld.global.nc.f32 	%f4, [%rd62];
	abs.ftz.f32 	%f13, %f4;
	setp.gtu.ftz.f32 	%p17, %f13, 0f7F800000;
	mov.f64 	%fd109, 0d7FF8000000000000;
	mov.f64 	%fd111, 0d7FF8000000000000;
	mov.f64 	%fd110, %fd111;
	@%p17 bra 	$L__BB0_24;

	abs.ftz.f32 	%f14, %f3;
	setp.gtu.ftz.f32 	%p18, %f14, 0f7F800000;
	@%p18 bra 	$L__BB0_24;

	abs.f64 	%fd49, %fd103;
	setp.gtu.f64 	%p19, %fd49, 0d7FF0000000000000;
	mov.f64 	%fd110, %fd111;
	@%p19 bra 	$L__BB0_24;

	abs.f64 	%fd52, %fd116;
	setp.gtu.f64 	%p20, %fd52, 0d7FF0000000000000;
	mov.f64 	%fd110, %fd111;
	@%p20 bra 	$L__BB0_24;

	cvt.ftz.f64.f32 	%fd53, %f4;
	cvt.ftz.f64.f32 	%fd54, %f3;
	fma.rn.f64 	%fd109, %fd53, %fd54, %fd103;
	add.f64 	%fd110, %fd116, %fd54;

$L__BB0_24:
	st.global.f64 	[%rd31], %fd109;
	st.global.f64 	[%rd32], %fd110;
	ld.global.nc.f32 	%f5, [%rd63+4];
	ld.global.nc.f32 	%f6, [%rd62+4];
	abs.ftz.f32 	%f15, %f6;
	setp.gtu.ftz.f32 	%p21, %f15, 0f7F800000;
	mov.f64 	%fd112, %fd111;
	@%p21 bra 	$L__BB0_29;

	abs.ftz.f32 	%f16, %f5;
	setp.gtu.ftz.f32 	%p22, %f16, 0f7F800000;
	mov.f64 	%fd112, %fd111;
	@%p22 bra 	$L__BB0_29;

	abs.f64 	%fd61, %fd109;
	setp.gtu.f64 	%p23, %fd61, 0d7FF0000000000000;
	mov.f64 	%fd112, %fd111;
	@%p23 bra 	$L__BB0_29;

	abs.f64 	%fd64, %fd110;
	setp.gtu.f64 	%p24, %fd64, 0d7FF0000000000000;
	mov.f64 	%fd112, %fd111;
	@%p24 bra 	$L__BB0_29;

	cvt.ftz.f64.f32 	%fd65, %f6;
	cvt.ftz.f64.f32 	%fd66, %f5;
	fma.rn.f64 	%fd111, %fd65, %fd66, %fd109;
	add.f64 	%fd112, %fd110, %fd66;

$L__BB0_29:
	st.global.f64 	[%rd31+8], %fd111;
	st.global.f64 	[%rd32+8], %fd112;
	ld.global.nc.f32 	%f7, [%rd63+8];
	ld.global.nc.f32 	%f8, [%rd62+8];
	abs.ftz.f32 	%f17, %f8;
	setp.gtu.ftz.f32 	%p25, %f17, 0f7F800000;
	mov.f64 	%fd113, 0d7FF8000000000000;
	mov.f64 	%fd103, 0d7FF8000000000000;
	mov.f64 	%fd114, %fd103;
	@%p25 bra 	$L__BB0_34;

	abs.ftz.f32 	%f18, %f7;
	setp.gtu.ftz.f32 	%p26, %f18, 0f7F800000;
	@%p26 bra 	$L__BB0_34;

	abs.f64 	%fd73, %fd111;
	setp.gtu.f64 	%p27, %fd73, 0d7FF0000000000000;
	mov.f64 	%fd114, %fd103;
	@%p27 bra 	$L__BB0_34;

	abs.f64 	%fd76, %fd112;
	setp.gtu.f64 	%p28, %fd76, 0d7FF0000000000000;
	mov.f64 	%fd114, %fd103;
	@%p28 bra 	$L__BB0_34;

	cvt.ftz.f64.f32 	%fd77, %f8;
	cvt.ftz.f64.f32 	%fd78, %f7;
	fma.rn.f64 	%fd113, %fd77, %fd78, %fd111;
	add.f64 	%fd114, %fd112, %fd78;

$L__BB0_34:
	st.global.f64 	[%rd31+16], %fd113;
	st.global.f64 	[%rd32+16], %fd114;
	ld.global.nc.f32 	%f9, [%rd63+12];
	ld.global.nc.f32 	%f10, [%rd62+12];
	abs.ftz.f32 	%f19, %f10;
	setp.gtu.ftz.f32 	%p29, %f19, 0f7F800000;
	mov.f64 	%fd116, %fd103;
	@%p29 bra 	$L__BB0_39;

	abs.ftz.f32 	%f20, %f9;
	setp.gtu.ftz.f32 	%p30, %f20, 0f7F800000;
	mov.f64 	%fd116, %fd103;
	@%p30 bra 	$L__BB0_39;

	abs.f64 	%fd85, %fd113;
	setp.gtu.f64 	%p31, %fd85, 0d7FF0000000000000;
	mov.f64 	%fd116, %fd103;
	@%p31 bra 	$L__BB0_39;

	abs.f64 	%fd88, %fd114;
	setp.gtu.f64 	%p32, %fd88, 0d7FF0000000000000;
	mov.f64 	%fd116, %fd103;
	@%p32 bra 	$L__BB0_39;

	cvt.ftz.f64.f32 	%fd89, %f10;
	cvt.ftz.f64.f32 	%fd90, %f9;
	fma.rn.f64 	%fd103, %fd89, %fd90, %fd113;
	add.f64 	%fd116, %fd114, %fd90;

$L__BB0_39:
	add.s64 	%rd63, %rd63, 16;
	add.s64 	%rd62, %rd62, 16;
	add.s64 	%rd64, %rd31, 32;
	st.global.f64 	[%rd31+24], %fd103;
	add.s64 	%rd65, %rd32, 32;
	st.global.f64 	[%rd32+24], %fd116;
	add.s32 	%r44, %r44, 4;
	setp.lt.s32 	%p33, %r44, %r20;
	@%p33 bra 	$L__BB0_19;

$L__BB0_40:
	ret;

}
	// .globl	vwma_batch_f32
.visible .entry vwma_batch_f32(
	.param .u64 vwma_batch_f32_param_0,
	.param .u64 vwma_batch_f32_param_1,
	.param .u64 vwma_batch_f32_param_2,
	.param .u32 vwma_batch_f32_param_3,
	.param .u32 vwma_batch_f32_param_4,
	.param .u32 vwma_batch_f32_param_5,
	.param .u64 vwma_batch_f32_param_6
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd2, [vwma_batch_f32_param_0];
	ld.param.u64 	%rd3, [vwma_batch_f32_param_1];
	ld.param.u64 	%rd4, [vwma_batch_f32_param_2];
	ld.param.u32 	%r11, [vwma_batch_f32_param_3];
	ld.param.u32 	%r13, [vwma_batch_f32_param_4];
	ld.param.u32 	%r12, [vwma_batch_f32_param_5];
	ld.param.u64 	%rd5, [vwma_batch_f32_param_6];
	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p1, %r1, %r13;
	@%p1 bra 	$L__BB1_9;

	cvta.to.global.u64 	%rd6, %rd4;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.u32 	%r2, [%rd8];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r19, %r14, %r3, %r15;
	setp.ge.s32 	%p2, %r19, %r11;
	@%p2 bra 	$L__BB1_9;

	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r16, %nctaid.x;
	mul.lo.s32 	%r5, %r16, %r3;
	mul.lo.s32 	%r6, %r1, %r11;
	add.s32 	%r17, %r12, %r2;
	add.s32 	%r7, %r17, -1;

$L__BB1_3:
	setp.lt.s32 	%p3, %r19, %r7;
	mov.f32 	%f5, 0f7FFFFFFF;
	@%p3 bra 	$L__BB1_8;

	sub.s32 	%r9, %r19, %r2;
	mul.wide.s32 	%rd11, %r19, 8;
	add.s64 	%rd9, %rd2, %rd11;
	// begin inline asm
	ld.global.nc.f64 %fd12, [%rd9];
	// end inline asm
	add.s64 	%rd10, %rd3, %rd11;
	// begin inline asm
	ld.global.nc.f64 %fd13, [%rd10];
	// end inline asm
	setp.lt.s32 	%p4, %r9, 0;
	@%p4 bra 	$L__BB1_6;

	mul.wide.s32 	%rd14, %r9, 8;
	add.s64 	%rd12, %rd2, %rd14;
	// begin inline asm
	ld.global.nc.f64 %fd9, [%rd12];
	// end inline asm
	sub.f64 	%fd12, %fd12, %fd9;
	add.s64 	%rd13, %rd3, %rd14;
	// begin inline asm
	ld.global.nc.f64 %fd10, [%rd13];
	// end inline asm
	sub.f64 	%fd13, %fd13, %fd10;

$L__BB1_6:
	setp.eq.f64 	%p5, %fd13, 0d0000000000000000;
	@%p5 bra 	$L__BB1_8;

	div.rn.f64 	%fd11, %fd12, %fd13;
	cvt.rn.ftz.f32.f64 	%f5, %fd11;

$L__BB1_8:
	add.s32 	%r18, %r19, %r6;
	mul.wide.s32 	%rd15, %r18, 4;
	add.s64 	%rd16, %rd1, %rd15;
	st.global.f32 	[%rd16], %f5;
	add.s32 	%r19, %r19, %r5;
	setp.lt.s32 	%p6, %r19, %r11;
	@%p6 bra 	$L__BB1_3;

$L__BB1_9:
	ret;

}
	// .globl	vwma_multi_series_one_param_f32
.visible .entry vwma_multi_series_one_param_f32(
	.param .u64 vwma_multi_series_one_param_f32_param_0,
	.param .u64 vwma_multi_series_one_param_f32_param_1,
	.param .u32 vwma_multi_series_one_param_f32_param_2,
	.param .u32 vwma_multi_series_one_param_f32_param_3,
	.param .u32 vwma_multi_series_one_param_f32_param_4,
	.param .u64 vwma_multi_series_one_param_f32_param_5,
	.param .u64 vwma_multi_series_one_param_f32_param_6
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd3, [vwma_multi_series_one_param_f32_param_0];
	ld.param.u64 	%rd4, [vwma_multi_series_one_param_f32_param_1];
	ld.param.u32 	%r10, [vwma_multi_series_one_param_f32_param_2];
	ld.param.u32 	%r11, [vwma_multi_series_one_param_f32_param_3];
	ld.param.u32 	%r12, [vwma_multi_series_one_param_f32_param_4];
	ld.param.u64 	%rd5, [vwma_multi_series_one_param_f32_param_5];
	ld.param.u64 	%rd6, [vwma_multi_series_one_param_f32_param_6];
	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p1, %r1, %r11;
	@%p1 bra 	$L__BB2_11;

	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r20, %r13, %r2, %r14;
	setp.ge.s32 	%p2, %r20, %r12;
	@%p2 bra 	$L__BB2_11;

	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.u32 	%r15, [%rd9];
	cvta.to.global.u64 	%rd1, %rd6;
	mov.u32 	%r16, %nctaid.x;
	mul.lo.s32 	%r4, %r16, %r2;
	add.s32 	%r17, %r10, %r15;
	add.s32 	%r5, %r17, -1;

$L__BB2_3:
	mad.lo.s32 	%r7, %r20, %r11, %r1;
	mul.wide.s32 	%rd10, %r7, 4;
	add.s64 	%rd2, %rd1, %rd10;
	setp.lt.s32 	%p3, %r20, %r5;
	@%p3 bra 	$L__BB2_9;
	bra.uni 	$L__BB2_4;

$L__BB2_9:
	mov.u32 	%r19, 2147483647;
	st.global.u32 	[%rd2], %r19;
	bra.uni 	$L__BB2_10;

$L__BB2_4:
	sub.s32 	%r8, %r20, %r10;
	mul.wide.s32 	%rd13, %r7, 8;
	add.s64 	%rd11, %rd3, %rd13;
	// begin inline asm
	ld.global.nc.f64 %fd12, [%rd11];
	// end inline asm
	add.s64 	%rd12, %rd4, %rd13;
	// begin inline asm
	ld.global.nc.f64 %fd13, [%rd12];
	// end inline asm
	setp.lt.s32 	%p4, %r8, 0;
	@%p4 bra 	$L__BB2_6;

	mad.lo.s32 	%r18, %r8, %r11, %r1;
	mul.wide.s32 	%rd16, %r18, 8;
	add.s64 	%rd14, %rd3, %rd16;
	// begin inline asm
	ld.global.nc.f64 %fd9, [%rd14];
	// end inline asm
	sub.f64 	%fd12, %fd12, %fd9;
	add.s64 	%rd15, %rd4, %rd16;
	// begin inline asm
	ld.global.nc.f64 %fd10, [%rd15];
	// end inline asm
	sub.f64 	%fd13, %fd13, %fd10;

$L__BB2_6:
	setp.eq.f64 	%p5, %fd13, 0d0000000000000000;
	mov.f32 	%f4, 0f7FFFFFFF;
	@%p5 bra 	$L__BB2_8;

	div.rn.f64 	%fd11, %fd12, %fd13;
	cvt.rn.ftz.f32.f64 	%f4, %fd11;

$L__BB2_8:
	st.global.f32 	[%rd2], %f4;

$L__BB2_10:
	add.s32 	%r20, %r20, %r4;
	setp.lt.s32 	%p6, %r20, %r12;
	@%p6 bra 	$L__BB2_3;

$L__BB2_11:
	ret;

}
	// .globl	vwma_multi_series_one_param_tm_coalesced_f32
.visible .entry vwma_multi_series_one_param_tm_coalesced_f32(
	.param .u64 vwma_multi_series_one_param_tm_coalesced_f32_param_0,
	.param .u64 vwma_multi_series_one_param_tm_coalesced_f32_param_1,
	.param .u32 vwma_multi_series_one_param_tm_coalesced_f32_param_2,
	.param .u32 vwma_multi_series_one_param_tm_coalesced_f32_param_3,
	.param .u32 vwma_multi_series_one_param_tm_coalesced_f32_param_4,
	.param .u64 vwma_multi_series_one_param_tm_coalesced_f32_param_5,
	.param .u64 vwma_multi_series_one_param_tm_coalesced_f32_param_6
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<24>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd3, [vwma_multi_series_one_param_tm_coalesced_f32_param_0];
	ld.param.u64 	%rd4, [vwma_multi_series_one_param_tm_coalesced_f32_param_1];
	ld.param.u32 	%r10, [vwma_multi_series_one_param_tm_coalesced_f32_param_2];
	ld.param.u32 	%r11, [vwma_multi_series_one_param_tm_coalesced_f32_param_3];
	ld.param.u32 	%r12, [vwma_multi_series_one_param_tm_coalesced_f32_param_4];
	ld.param.u64 	%rd5, [vwma_multi_series_one_param_tm_coalesced_f32_param_5];
	ld.param.u64 	%rd6, [vwma_multi_series_one_param_tm_coalesced_f32_param_6];
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p1, %r1, %r11;
	@%p1 bra 	$L__BB3_11;

	mov.u32 	%r2, %ntid.y;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r23, %r16, %r2, %r17;
	setp.ge.s32 	%p2, %r23, %r12;
	@%p2 bra 	$L__BB3_11;

	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.s32 	%rd8, %r1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.u32 	%r18, [%rd9];
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r4, %r19, %r2;
	cvta.to.global.u64 	%rd1, %rd6;
	add.s32 	%r20, %r10, %r18;
	add.s32 	%r5, %r20, -1;

$L__BB3_3:
	mad.lo.s32 	%r7, %r23, %r11, %r1;
	mul.wide.s32 	%rd10, %r7, 4;
	add.s64 	%rd2, %rd1, %rd10;
	setp.lt.s32 	%p3, %r23, %r5;
	@%p3 bra 	$L__BB3_9;
	bra.uni 	$L__BB3_4;

$L__BB3_9:
	mov.u32 	%r22, 2147483647;
	st.global.u32 	[%rd2], %r22;
	bra.uni 	$L__BB3_10;

$L__BB3_4:
	sub.s32 	%r8, %r23, %r10;
	mul.wide.s32 	%rd13, %r7, 8;
	add.s64 	%rd11, %rd3, %rd13;
	// begin inline asm
	ld.global.nc.f64 %fd12, [%rd11];
	// end inline asm
	add.s64 	%rd12, %rd4, %rd13;
	// begin inline asm
	ld.global.nc.f64 %fd13, [%rd12];
	// end inline asm
	setp.lt.s32 	%p4, %r8, 0;
	@%p4 bra 	$L__BB3_6;

	mad.lo.s32 	%r21, %r8, %r11, %r1;
	mul.wide.s32 	%rd16, %r21, 8;
	add.s64 	%rd14, %rd3, %rd16;
	// begin inline asm
	ld.global.nc.f64 %fd9, [%rd14];
	// end inline asm
	sub.f64 	%fd12, %fd12, %fd9;
	add.s64 	%rd15, %rd4, %rd16;
	// begin inline asm
	ld.global.nc.f64 %fd10, [%rd15];
	// end inline asm
	sub.f64 	%fd13, %fd13, %fd10;

$L__BB3_6:
	setp.eq.f64 	%p5, %fd13, 0d0000000000000000;
	mov.f32 	%f4, 0f7FFFFFFF;
	@%p5 bra 	$L__BB3_8;

	div.rn.f64 	%fd11, %fd12, %fd13;
	cvt.rn.ftz.f32.f64 	%f4, %fd11;

$L__BB3_8:
	st.global.f32 	[%rd2], %f4;

$L__BB3_10:
	add.s32 	%r23, %r23, %r4;
	setp.lt.s32 	%p6, %r23, %r12;
	@%p6 bra 	$L__BB3_3;

$L__BB3_11:
	ret;

}

