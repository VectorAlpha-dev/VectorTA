







.version 9.0
.target sm_89
.address_size 64

	
.extern .shared .align 16 .b8 shraw[];
.extern .shared .align 16 .b8 shraw$1[];

.visible .entry rvi_segprefix_f32(
	.param .u64 rvi_segprefix_f32_param_0,
	.param .u32 rvi_segprefix_f32_param_1,
	.param .u64 rvi_segprefix_f32_param_2,
	.param .u64 rvi_segprefix_f32_param_3,
	.param .u64 rvi_segprefix_f32_param_4
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<163>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd15, [rvi_segprefix_f32_param_0];
	ld.param.u32 	%r17, [rvi_segprefix_f32_param_1];
	ld.param.u64 	%rd16, [rvi_segprefix_f32_param_2];
	ld.param.u64 	%rd17, [rvi_segprefix_f32_param_3];
	ld.param.u64 	%rd18, [rvi_segprefix_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd17;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd15;
	mov.u32 	%r18, %tid.x;
	mov.u32 	%r19, %ctaid.x;
	or.b32  	%r20, %r18, %r19;
	setp.ne.s32 	%p1, %r20, 0;
	setp.lt.s32 	%p2, %r17, 1;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_24;

	and.b32  	%r1, %r17, 1;
	setp.eq.s32 	%p4, %r17, 1;
	mov.f32 	%f132, 0f00000000;
	mov.u32 	%r32, 0;
	mov.f32 	%f131, 0f7FC00000;
	mov.u64 	%rd23, 0;
	mov.f32 	%f133, %f132;
	mov.f32 	%f134, %f132;
	mov.f32 	%f135, %f132;
	@%p4 bra 	$L__BB0_17;

	sub.s32 	%r33, %r17, %r1;
	mov.f32 	%f132, 0f00000000;
	mov.u32 	%r31, 0;
	mov.f32 	%f131, 0f7FC00000;
	mov.u32 	%r32, %r31;

$L__BB0_3:
	cvt.s64.s32 	%rd5, %r31;
	mul.wide.s32 	%rd20, %r31, 4;
	add.s64 	%rd6, %rd4, %rd20;
	ld.global.nc.f32 	%f144, [%rd6];
	abs.ftz.f32 	%f7, %f144;
	setp.geu.ftz.f32 	%p5, %f7, 0f7F800000;
	add.s64 	%rd7, %rd3, %rd20;
	add.s64 	%rd8, %rd2, %rd20;
	add.s64 	%rd9, %rd1, %rd20;
	@%p5 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_4;

$L__BB0_8:
	mov.f32 	%f140, 0f00000000;
	mov.u32 	%r35, 0;
	st.global.u32 	[%rd7], %r35;
	st.global.u32 	[%rd8], %r35;
	st.global.u32 	[%rd9], %r35;
	mov.f32 	%f144, 0f7FC00000;
	mov.f32 	%f141, %f140;
	mov.f32 	%f142, %f140;
	mov.f32 	%f143, %f140;
	bra.uni 	$L__BB0_9;

$L__BB0_4:
	abs.ftz.f32 	%f64, %f131;
	setp.geu.ftz.f32 	%p6, %f64, 0f7F800000;
	@%p6 bra 	$L__BB0_6;
	bra.uni 	$L__BB0_5;

$L__BB0_6:
	mul.ftz.f32 	%f141, %f144, %f144;
	mov.u32 	%r35, 1;
	mov.f32 	%f142, 0f00000000;
	mov.f32 	%f140, %f144;
	mov.f32 	%f143, %f142;
	bra.uni 	$L__BB0_7;

$L__BB0_5:
	add.ftz.f32 	%f140, %f135, %f144;
	abs.ftz.f32 	%f65, %f135;
	setp.ltu.ftz.f32 	%p7, %f65, %f7;
	sub.ftz.f32 	%f66, %f144, %f140;
	add.ftz.f32 	%f67, %f135, %f66;
	sub.ftz.f32 	%f68, %f135, %f140;
	add.ftz.f32 	%f69, %f144, %f68;
	selp.f32 	%f70, %f67, %f69, %p7;
	add.ftz.f32 	%f142, %f133, %f70;
	mul.ftz.f32 	%f71, %f144, %f144;
	add.ftz.f32 	%f141, %f134, %f71;
	abs.ftz.f32 	%f72, %f71;
	abs.ftz.f32 	%f73, %f134;
	setp.ltu.ftz.f32 	%p8, %f73, %f72;
	sub.ftz.f32 	%f74, %f71, %f141;
	add.ftz.f32 	%f75, %f134, %f74;
	sub.ftz.f32 	%f76, %f134, %f141;
	add.ftz.f32 	%f77, %f71, %f76;
	selp.f32 	%f78, %f75, %f77, %p8;
	add.ftz.f32 	%f143, %f132, %f78;
	add.s32 	%r35, %r32, 1;

$L__BB0_7:
	add.ftz.f32 	%f81, %f140, %f142;
	st.global.f32 	[%rd7], %f81;
	add.ftz.f32 	%f82, %f141, %f143;
	st.global.f32 	[%rd8], %f82;
	st.global.u32 	[%rd9], %r35;

$L__BB0_9:
	ld.global.nc.f32 	%f131, [%rd6+4];
	abs.ftz.f32 	%f23, %f131;
	setp.geu.ftz.f32 	%p9, %f23, 0f7F800000;
	@%p9 bra 	$L__BB0_14;
	bra.uni 	$L__BB0_10;

$L__BB0_14:
	mov.f32 	%f135, 0f00000000;
	mov.u32 	%r32, 0;
	st.global.u32 	[%rd7+4], %r32;
	st.global.u32 	[%rd8+4], %r32;
	st.global.u32 	[%rd9+4], %r32;
	mov.f32 	%f131, 0f7FC00000;
	mov.f32 	%f134, %f135;
	mov.f32 	%f133, %f135;
	mov.f32 	%f132, %f135;
	bra.uni 	$L__BB0_15;

$L__BB0_10:
	abs.ftz.f32 	%f88, %f144;
	setp.geu.ftz.f32 	%p10, %f88, 0f7F800000;
	@%p10 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_11;

$L__BB0_12:
	mul.ftz.f32 	%f134, %f131, %f131;
	mov.u32 	%r32, 1;
	mov.f32 	%f133, 0f00000000;
	mov.f32 	%f135, %f131;
	mov.f32 	%f132, %f133;
	bra.uni 	$L__BB0_13;

$L__BB0_11:
	add.ftz.f32 	%f135, %f140, %f131;
	abs.ftz.f32 	%f89, %f140;
	setp.ltu.ftz.f32 	%p11, %f89, %f23;
	sub.ftz.f32 	%f90, %f131, %f135;
	add.ftz.f32 	%f91, %f140, %f90;
	sub.ftz.f32 	%f92, %f140, %f135;
	add.ftz.f32 	%f93, %f131, %f92;
	selp.f32 	%f94, %f91, %f93, %p11;
	add.ftz.f32 	%f133, %f142, %f94;
	mul.ftz.f32 	%f95, %f131, %f131;
	add.ftz.f32 	%f134, %f141, %f95;
	abs.ftz.f32 	%f96, %f95;
	abs.ftz.f32 	%f97, %f141;
	setp.ltu.ftz.f32 	%p12, %f97, %f96;
	sub.ftz.f32 	%f98, %f95, %f134;
	add.ftz.f32 	%f99, %f141, %f98;
	sub.ftz.f32 	%f100, %f141, %f134;
	add.ftz.f32 	%f101, %f95, %f100;
	selp.f32 	%f102, %f99, %f101, %p12;
	add.ftz.f32 	%f132, %f143, %f102;
	add.s32 	%r32, %r35, 1;

$L__BB0_13:
	add.ftz.f32 	%f105, %f135, %f133;
	st.global.f32 	[%rd7+4], %f105;
	add.ftz.f32 	%f106, %f134, %f132;
	st.global.f32 	[%rd8+4], %f106;
	st.global.u32 	[%rd9+4], %r32;

$L__BB0_15:
	cvt.u32.u64 	%r28, %rd5;
	add.s32 	%r31, %r28, 2;
	add.s32 	%r33, %r33, -2;
	setp.ne.s32 	%p13, %r33, 0;
	@%p13 bra 	$L__BB0_3;

	cvt.s64.s32 	%rd23, %r31;

$L__BB0_17:
	setp.eq.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB0_24;

	shl.b64 	%rd21, %rd23, 2;
	add.s64 	%rd22, %rd4, %rd21;
	ld.global.nc.f32 	%f43, [%rd22];
	abs.ftz.f32 	%f44, %f43;
	setp.geu.ftz.f32 	%p15, %f44, 0f7F800000;
	add.s64 	%rd12, %rd3, %rd21;
	add.s64 	%rd13, %rd2, %rd21;
	add.s64 	%rd14, %rd1, %rd21;
	@%p15 bra 	$L__BB0_23;
	bra.uni 	$L__BB0_19;

$L__BB0_23:
	mov.u32 	%r30, 0;
	st.global.u32 	[%rd12], %r30;
	st.global.u32 	[%rd13], %r30;
	st.global.u32 	[%rd14], %r30;
	bra.uni 	$L__BB0_24;

$L__BB0_19:
	abs.ftz.f32 	%f112, %f131;
	setp.geu.ftz.f32 	%p16, %f112, 0f7F800000;
	@%p16 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_20;

$L__BB0_21:
	mul.ftz.f32 	%f160, %f43, %f43;
	mov.u32 	%r39, 1;
	mov.f32 	%f161, 0f00000000;
	mov.f32 	%f159, %f43;
	mov.f32 	%f162, %f161;
	bra.uni 	$L__BB0_22;

$L__BB0_20:
	add.ftz.f32 	%f159, %f135, %f43;
	abs.ftz.f32 	%f113, %f135;
	setp.ltu.ftz.f32 	%p17, %f113, %f44;
	sub.ftz.f32 	%f114, %f43, %f159;
	add.ftz.f32 	%f115, %f135, %f114;
	sub.ftz.f32 	%f116, %f135, %f159;
	add.ftz.f32 	%f117, %f43, %f116;
	selp.f32 	%f118, %f115, %f117, %p17;
	add.ftz.f32 	%f161, %f133, %f118;
	mul.ftz.f32 	%f119, %f43, %f43;
	add.ftz.f32 	%f160, %f134, %f119;
	abs.ftz.f32 	%f120, %f119;
	abs.ftz.f32 	%f121, %f134;
	setp.ltu.ftz.f32 	%p18, %f121, %f120;
	sub.ftz.f32 	%f122, %f119, %f160;
	add.ftz.f32 	%f123, %f134, %f122;
	sub.ftz.f32 	%f124, %f134, %f160;
	add.ftz.f32 	%f125, %f119, %f124;
	selp.f32 	%f126, %f123, %f125, %p18;
	add.ftz.f32 	%f162, %f132, %f126;
	add.s32 	%r39, %r32, 1;

$L__BB0_22:
	add.ftz.f32 	%f129, %f159, %f161;
	st.global.f32 	[%rd12], %f129;
	add.ftz.f32 	%f130, %f160, %f162;
	st.global.f32 	[%rd13], %f130;
	st.global.u32 	[%rd14], %r39;

$L__BB0_24:
	ret;

}
	
.visible .entry rvi_batch_stddev_from_prefix_f32(
	.param .u64 rvi_batch_stddev_from_prefix_f32_param_0,
	.param .u64 rvi_batch_stddev_from_prefix_f32_param_1,
	.param .u64 rvi_batch_stddev_from_prefix_f32_param_2,
	.param .u64 rvi_batch_stddev_from_prefix_f32_param_3,
	.param .u64 rvi_batch_stddev_from_prefix_f32_param_4,
	.param .u64 rvi_batch_stddev_from_prefix_f32_param_5,
	.param .u64 rvi_batch_stddev_from_prefix_f32_param_6,
	.param .u32 rvi_batch_stddev_from_prefix_f32_param_7,
	.param .u32 rvi_batch_stddev_from_prefix_f32_param_8,
	.param .u32 rvi_batch_stddev_from_prefix_f32_param_9,
	.param .u32 rvi_batch_stddev_from_prefix_f32_param_10,
	.param .u64 rvi_batch_stddev_from_prefix_f32_param_11,
	.param .u64 rvi_batch_stddev_from_prefix_f32_param_12
)
{
	.reg .pred 	%p<48>;
	.reg .b16 	%rs<23>;
	.reg .f32 	%f<80>;
	.reg .b32 	%r<124>;
	.reg .f64 	%fd<99>;
	.reg .b64 	%rd<56>;


	ld.param.u64 	%rd30, [rvi_batch_stddev_from_prefix_f32_param_0];
	ld.param.u64 	%rd23, [rvi_batch_stddev_from_prefix_f32_param_1];
	ld.param.u64 	%rd24, [rvi_batch_stddev_from_prefix_f32_param_2];
	ld.param.u64 	%rd25, [rvi_batch_stddev_from_prefix_f32_param_3];
	ld.param.u64 	%rd26, [rvi_batch_stddev_from_prefix_f32_param_4];
	ld.param.u64 	%rd27, [rvi_batch_stddev_from_prefix_f32_param_5];
	ld.param.u64 	%rd28, [rvi_batch_stddev_from_prefix_f32_param_6];
	ld.param.u32 	%r48, [rvi_batch_stddev_from_prefix_f32_param_7];
	ld.param.u32 	%r49, [rvi_batch_stddev_from_prefix_f32_param_8];
	ld.param.u32 	%r51, [rvi_batch_stddev_from_prefix_f32_param_9];
	ld.param.u64 	%rd29, [rvi_batch_stddev_from_prefix_f32_param_11];
	ld.param.u64 	%rd31, [rvi_batch_stddev_from_prefix_f32_param_12];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd52, %rd30;
	mov.u32 	%r102, %ctaid.x;
	setp.ge.s32 	%p1, %r102, %r51;
	mov.u32 	%r52, %tid.x;
	setp.ne.s32 	%p2, %r52, 0;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB1_57;

	cvta.to.global.u64 	%rd32, %rd26;
	cvt.s64.s32 	%rd3, %r102;
	mul.wide.s32 	%rd33, %r102, 4;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.u32 	%r53, [%rd34];
	cvt.s64.s32 	%rd4, %r53;
	cvta.to.global.u64 	%rd35, %rd27;
	add.s64 	%rd36, %rd35, %rd33;
	setp.lt.s32 	%p4, %r53, 1;
	ld.global.nc.u32 	%r2, [%rd36];
	setp.lt.s32 	%p5, %r2, 1;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB1_57;

	cvta.to.global.u64 	%rd37, %rd28;
	shl.b64 	%rd38, %rd3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	ld.global.nc.u32 	%r3, [%rd39];
	setp.eq.s64 	%p7, %rd29, 0;
	@%p7 bra 	$L__BB1_4;

	cvta.to.global.u64 	%rd40, %rd29;
	add.s64 	%rd42, %rd40, %rd38;
	ld.global.nc.u32 	%r102, [%rd42];

$L__BB1_4:
	mul.lo.s32 	%r6, %r102, %r48;
	cvt.u32.u64 	%r54, %rd4;
	add.s32 	%r55, %r49, %r54;
	add.s32 	%r56, %r55, %r2;
	add.s32 	%r7, %r56, -2;
	min.s32 	%r8, %r7, %r48;
	setp.lt.s32 	%p8, %r8, 1;
	@%p8 bra 	$L__BB1_11;

	mov.u32 	%r59, 1;
	sub.s32 	%r60, %r59, %r54;
	sub.s32 	%r61, %r60, %r2;
	sub.s32 	%r62, %r61, %r49;
	not.b32 	%r63, %r48;
	max.s32 	%r64, %r62, %r63;
	mov.u32 	%r65, -2;
	sub.s32 	%r66, %r65, %r64;
	and.b32  	%r106, %r8, 3;
	setp.lt.u32 	%p9, %r66, 3;
	mov.u32 	%r105, 0;
	@%p9 bra 	$L__BB1_8;

	sub.s32 	%r104, %r8, %r106;
	mov.u32 	%r105, 0;

$L__BB1_7:
	add.s32 	%r68, %r105, %r6;
	mul.wide.s32 	%rd43, %r68, 4;
	add.s64 	%rd44, %rd1, %rd43;
	mov.u32 	%r69, 2143289344;
	st.global.u32 	[%rd44], %r69;
	st.global.u32 	[%rd44+4], %r69;
	st.global.u32 	[%rd44+8], %r69;
	st.global.u32 	[%rd44+12], %r69;
	add.s32 	%r105, %r105, 4;
	add.s32 	%r104, %r104, -4;
	setp.ne.s32 	%p10, %r104, 0;
	@%p10 bra 	$L__BB1_7;

$L__BB1_8:
	setp.eq.s32 	%p11, %r106, 0;
	@%p11 bra 	$L__BB1_11;

	add.s32 	%r70, %r105, %r6;
	mul.wide.s32 	%rd45, %r70, 4;
	add.s64 	%rd50, %rd1, %rd45;

$L__BB1_10:
	.pragma "nounroll";
	mov.u32 	%r71, 2143289344;
	st.global.u32 	[%rd50], %r71;
	add.s64 	%rd50, %rd50, 4;
	add.s32 	%r106, %r106, -1;
	setp.ne.s32 	%p12, %r106, 0;
	@%p12 bra 	$L__BB1_10;

$L__BB1_11:
	setp.ge.s32 	%p13, %r7, %r48;
	@%p13 bra 	$L__BB1_57;

	cvt.rn.f64.s32 	%fd35, %r2;
	rcp.rn.f64 	%fd1, %fd35;
	mov.f64 	%fd36, 0d3FF0000000000000;
	add.f64 	%fd37, %fd35, 0d3FF0000000000000;
	mov.f64 	%fd38, 0d4000000000000000;
	div.rn.f64 	%fd2, %fd38, %fd37;
	sub.f64 	%fd3, %fd36, %fd2;
	setp.lt.s32 	%p14, %r48, 1;
	@%p14 bra 	$L__BB1_57;

	ld.global.nc.f32 	%f69, [%rd52];
	cvt.rn.f64.s32 	%fd45, %r54;
	rcp.rn.f64 	%fd4, %fd45;
	neg.s32 	%r107, %r54;
	mov.u32 	%r123, 0;
	shl.b64 	%rd46, %rd4, 2;
	neg.s64 	%rd8, %rd46;
	mul.wide.s32 	%rd47, %r6, 4;
	add.s64 	%rd51, %rd1, %rd47;
	cvta.to.global.u64 	%rd55, %rd24;
	cvta.to.global.u64 	%rd54, %rd23;
	cvta.to.global.u64 	%rd53, %rd25;
	mov.u16 	%rs22, 0;
	mov.f64 	%fd98, 0d0000000000000000;
	mov.u32 	%r108, 1;
	mov.f64 	%fd97, %fd98;
	mov.f64 	%fd96, %fd98;
	mov.f64 	%fd95, %fd98;
	mov.u32 	%r122, %r123;
	mov.u32 	%r121, %r123;
	mov.u32 	%r120, %r123;
	mov.u32 	%r119, %r123;
	mov.u32 	%r118, %r123;
	mov.f64 	%fd94, %fd98;
	mov.f64 	%fd93, %fd98;
	mov.u16 	%rs21, %rs22;

$L__BB1_14:
	mov.u16 	%rs2, %rs21;
	mov.u16 	%rs1, %rs22;
	mov.f64 	%fd10, %fd93;
	mov.f64 	%fd9, %fd94;
	mov.u32 	%r26, %r118;
	mov.u32 	%r25, %r119;
	mov.u32 	%r23, %r121;
	mov.u32 	%r21, %r123;
	mov.f64 	%fd7, %fd96;
	mov.f32 	%f2, %f69;
	mov.u32 	%r20, %r108;
	ld.global.nc.f32 	%f69, [%rd52];
	setp.eq.s32 	%p15, %r20, 1;
	mov.f64 	%fd89, 0d7FF8000000000000;
	@%p15 bra 	$L__BB1_18;

	abs.ftz.f32 	%f32, %f69;
	setp.geu.ftz.f32 	%p16, %f32, 0f7F800000;
	@%p16 bra 	$L__BB1_18;

	abs.ftz.f32 	%f33, %f2;
	setp.geu.ftz.f32 	%p17, %f33, 0f7F800000;
	@%p17 bra 	$L__BB1_18;

	cvt.ftz.f64.f32 	%fd49, %f69;
	cvt.ftz.f64.f32 	%fd50, %f2;
	sub.f64 	%fd89, %fd49, %fd50;

$L__BB1_18:
	setp.lt.s32 	%p18, %r20, %r54;
	mov.f32 	%f72, 0f7FC00000;
	@%p18 bra 	$L__BB1_25;

	mov.f32 	%f72, 0f7FC00000;
	ld.global.nc.u32 	%r82, [%rd53];
	setp.lt.s32 	%p19, %r82, %r54;
	@%p19 bra 	$L__BB1_25;

	ld.global.nc.f32 	%f4, [%rd54];
	setp.eq.s32 	%p20, %r107, -1;
	mov.f32 	%f71, 0f00000000;
	mov.f32 	%f70, %f71;
	@%p20 bra 	$L__BB1_22;

	add.s64 	%rd48, %rd54, %rd8;
	ld.global.nc.f32 	%f70, [%rd48];

$L__BB1_22:
	sub.ftz.f32 	%f7, %f4, %f70;
	ld.global.nc.f32 	%f8, [%rd55];
	@%p20 bra 	$L__BB1_24;

	add.s64 	%rd49, %rd55, %rd8;
	ld.global.nc.f32 	%f71, [%rd49];

$L__BB1_24:
	sub.ftz.f32 	%f38, %f8, %f71;
	cvt.ftz.f64.f32 	%fd51, %f38;
	cvt.ftz.f64.f32 	%fd52, %f7;
	mul.f64 	%fd53, %fd4, %fd52;
	neg.f64 	%fd54, %fd53;
	mul.f64 	%fd55, %fd4, %fd51;
	fma.rn.f64 	%fd56, %fd54, %fd53, %fd55;
	mov.f64 	%fd57, 0d0000000000000000;
	max.f64 	%fd58, %fd57, %fd56;
	sqrt.rn.f64 	%fd59, %fd58;
	cvt.rn.ftz.f32.f64 	%f72, %fd59;

$L__BB1_25:
	mov.f32 	%f73, 0f7FC00000;
	abs.f64 	%fd60, %fd89;
	setp.geu.f64 	%p22, %fd60, 0d7FF0000000000000;
	mov.f32 	%f74, %f73;
	@%p22 bra 	$L__BB1_28;

	mov.f32 	%f73, 0f7FC00000;
	abs.ftz.f32 	%f43, %f72;
	setp.geu.ftz.f32 	%p23, %f43, 0f7F800000;
	@%p23 bra 	$L__BB1_28;

	setp.gt.f64 	%p24, %fd89, 0d0000000000000000;
	selp.f32 	%f73, %f72, 0f00000000, %p24;
	setp.lt.f64 	%p25, %fd89, 0d0000000000000000;
	selp.f32 	%f74, %f72, 0f00000000, %p25;

$L__BB1_28:
	abs.ftz.f32 	%f17, %f73;
	setp.eq.s32 	%p26, %r3, 0;
	mov.f32 	%f77, 0f7FC00000;
	mov.f32 	%f65, 0f7FC00000;
	@%p26 bra 	$L__BB1_39;

	setp.geu.ftz.f32 	%p27, %f17, 0f7F800000;
	mov.u32 	%r119, 0;
	mov.f64 	%fd94, 0d0000000000000000;
	mov.u16 	%rs21, 0;
	mov.f64 	%fd93, %fd94;
	mov.u32 	%r118, %r119;
	@%p27 bra 	$L__BB1_34;

	and.b16  	%rs9, %rs2, 255;
	setp.eq.s16 	%p28, %rs9, 0;
	cvt.ftz.f64.f32 	%fd13, %f73;
	@%p28 bra 	$L__BB1_32;

	mul.f64 	%fd62, %fd2, %fd13;
	fma.rn.f64 	%fd97, %fd3, %fd97, %fd62;
	cvt.rn.ftz.f32.f64 	%f77, %fd97;
	mov.u16 	%rs21, %rs2;
	mov.f64 	%fd93, %fd10;
	mov.u32 	%r118, %r26;
	bra.uni 	$L__BB1_34;

$L__BB1_39:
	setp.geu.ftz.f32 	%p33, %f17, 0f7F800000;
	mov.f64 	%fd96, 0d0000000000000000;
	mov.u32 	%r121, 0;
	mov.u32 	%r116, %r121;
	mov.u32 	%r117, %r121;
	mov.f64 	%fd92, %fd96;
	mov.f32 	%f77, %f65;
	@%p33 bra 	$L__BB1_44;

	setp.lt.s32 	%p34, %r122, %r2;
	shl.b32 	%r87, %r120, 2;
	mov.u32 	%r88, shraw$1;
	add.s32 	%r30, %r88, %r87;
	@%p34 bra 	$L__BB1_42;
	bra.uni 	$L__BB1_41;

$L__BB1_42:
	st.shared.f32 	[%r30], %f73;
	cvt.ftz.f64.f32 	%fd70, %f73;
	add.f64 	%fd92, %fd95, %fd70;
	add.s32 	%r90, %r120, 1;
	setp.eq.s32 	%p36, %r90, %r2;
	selp.b32 	%r116, 0, %r90, %p36;
	add.s32 	%r117, %r122, 1;
	setp.ne.s32 	%p37, %r117, %r2;
	mov.f32 	%f77, %f65;
	@%p37 bra 	$L__BB1_44;

	mul.f64 	%fd71, %fd1, %fd92;
	cvt.rn.ftz.f32.f64 	%f77, %fd71;
	mov.u32 	%r117, %r2;
	bra.uni 	$L__BB1_44;

$L__BB1_32:
	add.f64 	%fd93, %fd10, %fd13;
	add.s32 	%r118, %r26, 1;
	setp.ne.s32 	%p29, %r118, %r2;
	@%p29 bra 	$L__BB1_34;

	mul.f64 	%fd97, %fd1, %fd93;
	cvt.rn.ftz.f32.f64 	%f77, %fd97;
	mov.u16 	%rs21, 1;
	mov.u32 	%r118, %r2;

$L__BB1_34:
	mov.u16 	%rs22, 0;
	abs.ftz.f32 	%f47, %f74;
	setp.geu.ftz.f32 	%p30, %f47, 0f7F800000;
	mov.f32 	%f78, 0f7FC00000;
	mov.u32 	%r121, %r23;
	mov.u32 	%r123, %r21;
	mov.f64 	%fd96, %fd7;
	@%p30 bra 	$L__BB1_49;

	and.b16  	%rs13, %rs1, 255;
	setp.eq.s16 	%p31, %rs13, 0;
	cvt.ftz.f64.f32 	%fd19, %f74;
	@%p31 bra 	$L__BB1_37;

	mul.f64 	%fd64, %fd2, %fd19;
	fma.rn.f64 	%fd98, %fd3, %fd98, %fd64;
	cvt.rn.ftz.f32.f64 	%f78, %fd98;
	mov.u16 	%rs22, %rs1;
	mov.f64 	%fd94, %fd9;
	mov.u32 	%r119, %r25;
	mov.u32 	%r121, %r23;
	mov.u32 	%r123, %r21;
	mov.f64 	%fd96, %fd7;
	bra.uni 	$L__BB1_49;

$L__BB1_37:
	mov.u16 	%rs22, 0;
	add.f64 	%fd94, %fd9, %fd19;
	add.s32 	%r119, %r25, 1;
	setp.ne.s32 	%p32, %r119, %r2;
	mov.u32 	%r121, %r23;
	mov.u32 	%r123, %r21;
	mov.f64 	%fd96, %fd7;
	@%p32 bra 	$L__BB1_49;

	mul.f64 	%fd98, %fd1, %fd94;
	cvt.rn.ftz.f32.f64 	%f78, %fd98;
	mov.u16 	%rs22, 1;
	mov.u32 	%r119, %r2;
	mov.u32 	%r121, %r23;
	mov.u32 	%r123, %r21;
	mov.f64 	%fd96, %fd7;
	bra.uni 	$L__BB1_49;

$L__BB1_41:
	ld.shared.f32 	%f50, [%r30];
	st.shared.f32 	[%r30], %f73;
	add.s32 	%r89, %r120, 1;
	setp.eq.s32 	%p35, %r89, %r2;
	selp.b32 	%r116, 0, %r89, %p35;
	cvt.ftz.f64.f32 	%fd66, %f50;
	cvt.ftz.f64.f32 	%fd67, %f73;
	sub.f64 	%fd68, %fd67, %fd66;
	add.f64 	%fd92, %fd95, %fd68;
	mul.f64 	%fd69, %fd1, %fd92;
	cvt.rn.ftz.f32.f64 	%f77, %fd69;
	mov.u32 	%r117, %r122;

$L__BB1_44:
	abs.ftz.f32 	%f53, %f74;
	setp.geu.ftz.f32 	%p38, %f53, 0f7F800000;
	mov.f32 	%f78, 0f7FC00000;
	mov.u16 	%rs21, %rs2;
	mov.u16 	%rs22, %rs1;
	mov.f64 	%fd93, %fd10;
	mov.f64 	%fd94, %fd9;
	mov.u32 	%r118, %r26;
	mov.u32 	%r119, %r25;
	mov.u32 	%r120, %r116;
	mov.u32 	%r122, %r117;
	mov.u32 	%r123, %r121;
	mov.f64 	%fd95, %fd92;
	@%p38 bra 	$L__BB1_49;

	ld.param.u32 	%r101, [rvi_batch_stddev_from_prefix_f32_param_10];
	setp.lt.s32 	%p39, %r21, %r2;
	add.s32 	%r93, %r23, %r101;
	shl.b32 	%r94, %r93, 2;
	mov.u32 	%r95, shraw$1;
	add.s32 	%r36, %r95, %r94;
	@%p39 bra 	$L__BB1_47;
	bra.uni 	$L__BB1_46;

$L__BB1_47:
	st.shared.f32 	[%r36], %f74;
	cvt.ftz.f64.f32 	%fd77, %f74;
	add.f64 	%fd96, %fd7, %fd77;
	add.s32 	%r97, %r23, 1;
	setp.eq.s32 	%p41, %r97, %r2;
	selp.b32 	%r121, 0, %r97, %p41;
	add.s32 	%r123, %r21, 1;
	setp.ne.s32 	%p42, %r123, %r2;
	mov.u16 	%rs21, %rs2;
	mov.u16 	%rs22, %rs1;
	mov.f64 	%fd93, %fd10;
	mov.f64 	%fd94, %fd9;
	mov.u32 	%r118, %r26;
	mov.u32 	%r119, %r25;
	mov.u32 	%r120, %r116;
	mov.u32 	%r122, %r117;
	mov.f64 	%fd95, %fd92;
	@%p42 bra 	$L__BB1_49;

	mul.f64 	%fd78, %fd1, %fd96;
	cvt.rn.ftz.f32.f64 	%f78, %fd78;
	mov.u16 	%rs21, %rs2;
	mov.u16 	%rs22, %rs1;
	mov.f64 	%fd93, %fd10;
	mov.f64 	%fd94, %fd9;
	mov.u32 	%r118, %r26;
	mov.u32 	%r119, %r25;
	mov.u32 	%r120, %r116;
	mov.u32 	%r122, %r117;
	mov.u32 	%r123, %r2;
	mov.f64 	%fd95, %fd92;
	bra.uni 	$L__BB1_49;

$L__BB1_46:
	ld.shared.f32 	%f54, [%r36];
	st.shared.f32 	[%r36], %f74;
	add.s32 	%r96, %r23, 1;
	setp.eq.s32 	%p40, %r96, %r2;
	selp.b32 	%r121, 0, %r96, %p40;
	cvt.ftz.f64.f32 	%fd73, %f54;
	cvt.ftz.f64.f32 	%fd74, %f74;
	sub.f64 	%fd75, %fd74, %fd73;
	add.f64 	%fd96, %fd7, %fd75;
	mul.f64 	%fd76, %fd1, %fd96;
	cvt.rn.ftz.f32.f64 	%f78, %fd76;
	mov.u16 	%rs21, %rs2;
	mov.u16 	%rs22, %rs1;
	mov.f64 	%fd93, %fd10;
	mov.f64 	%fd94, %fd9;
	mov.u32 	%r118, %r26;
	mov.u32 	%r119, %r25;
	mov.u32 	%r120, %r116;
	mov.u32 	%r122, %r117;
	mov.u32 	%r123, %r21;
	mov.f64 	%fd95, %fd92;

$L__BB1_49:
	add.s32 	%r98, %r20, -1;
	setp.lt.s32 	%p43, %r98, %r7;
	@%p43 bra 	$L__BB1_56;

	abs.ftz.f32 	%f56, %f77;
	setp.geu.ftz.f32 	%p44, %f56, 0f7F800000;
	@%p44 bra 	$L__BB1_55;

	abs.ftz.f32 	%f57, %f78;
	setp.geu.ftz.f32 	%p45, %f57, 0f7F800000;
	@%p45 bra 	$L__BB1_55;
	bra.uni 	$L__BB1_52;

$L__BB1_55:
	mov.u32 	%r99, 2143289344;
	st.global.u32 	[%rd51], %r99;
	bra.uni 	$L__BB1_56;

$L__BB1_52:
	cvt.ftz.f64.f32 	%fd79, %f77;
	cvt.ftz.f64.f32 	%fd80, %f78;
	add.f64 	%fd34, %fd79, %fd80;
	abs.f64 	%fd81, %fd34;
	setp.le.f64 	%p46, %fd81, 0d3CD203AF9EE75616;
	mov.f32 	%f79, 0f7FC00000;
	@%p46 bra 	$L__BB1_54;

	cvt.rn.ftz.f32.f64 	%f59, %fd34;
	div.approx.ftz.f32 	%f60, %f77, %f59;
	mul.ftz.f32 	%f79, %f60, 0f42C80000;

$L__BB1_54:
	st.global.f32 	[%rd51], %f79;

$L__BB1_56:
	ld.param.u32 	%r100, [rvi_batch_stddev_from_prefix_f32_param_7];
	add.s32 	%r108, %r20, 1;
	add.s32 	%r107, %r107, 1;
	add.s64 	%rd55, %rd55, 4;
	add.s64 	%rd54, %rd54, 4;
	add.s64 	%rd53, %rd53, 4;
	add.s64 	%rd52, %rd52, 4;
	add.s64 	%rd51, %rd51, 4;
	setp.lt.s32 	%p47, %r20, %r100;
	@%p47 bra 	$L__BB1_14;

$L__BB1_57:
	ret;

}
	
.visible .entry scatter_rows_f32(
	.param .u64 scatter_rows_f32_param_0,
	.param .u32 scatter_rows_f32_param_1,
	.param .u32 scatter_rows_f32_param_2,
	.param .u64 scatter_rows_f32_param_3,
	.param .u64 scatter_rows_f32_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd3, [scatter_rows_f32_param_0];
	ld.param.u32 	%r9, [scatter_rows_f32_param_1];
	ld.param.u32 	%r8, [scatter_rows_f32_param_2];
	ld.param.u64 	%rd4, [scatter_rows_f32_param_3];
	ld.param.u64 	%rd5, [scatter_rows_f32_param_4];
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r9;
	@%p1 bra 	$L__BB2_4;

	mov.u32 	%r13, %tid.x;
	setp.ge.s32 	%p2, %r13, %r8;
	@%p2 bra 	$L__BB2_4;

	cvta.to.global.u64 	%rd6, %rd4;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.u32 	%r10, [%rd8];
	mov.u32 	%r3, %ntid.x;
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd3;
	mul.lo.s32 	%r4, %r10, %r8;
	mul.lo.s32 	%r5, %r1, %r8;

$L__BB2_3:
	add.s32 	%r11, %r13, %r5;
	mul.wide.s32 	%rd9, %r11, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.f32 	%f1, [%rd10];
	add.s32 	%r12, %r13, %r4;
	mul.wide.s32 	%rd11, %r12, 4;
	add.s64 	%rd12, %rd1, %rd11;
	st.global.f32 	[%rd12], %f1;
	add.s32 	%r13, %r13, %r3;
	setp.lt.s32 	%p3, %r13, %r8;
	@%p3 bra 	$L__BB2_3;

$L__BB2_4:
	ret;

}
	
.visible .entry rvi_batch_f32(
	.param .u64 rvi_batch_f32_param_0,
	.param .u64 rvi_batch_f32_param_1,
	.param .u64 rvi_batch_f32_param_2,
	.param .u64 rvi_batch_f32_param_3,
	.param .u64 rvi_batch_f32_param_4,
	.param .u32 rvi_batch_f32_param_5,
	.param .u32 rvi_batch_f32_param_6,
	.param .u32 rvi_batch_f32_param_7,
	.param .u32 rvi_batch_f32_param_8,
	.param .u32 rvi_batch_f32_param_9,
	.param .u64 rvi_batch_f32_param_10
)
{
	.reg .pred 	%p<68>;
	.reg .b16 	%rs<21>;
	.reg .f32 	%f<110>;
	.reg .b32 	%r<180>;
	.reg .f64 	%fd<270>;
	.reg .b64 	%rd<39>;


	ld.param.u64 	%rd11, [rvi_batch_f32_param_0];
	ld.param.u64 	%rd7, [rvi_batch_f32_param_1];
	ld.param.u64 	%rd8, [rvi_batch_f32_param_2];
	ld.param.u64 	%rd9, [rvi_batch_f32_param_3];
	ld.param.u64 	%rd10, [rvi_batch_f32_param_4];
	ld.param.u32 	%r72, [rvi_batch_f32_param_5];
	ld.param.u32 	%r73, [rvi_batch_f32_param_6];
	ld.param.u32 	%r75, [rvi_batch_f32_param_7];
	ld.param.u32 	%r74, [rvi_batch_f32_param_9];
	ld.param.u64 	%rd12, [rvi_batch_f32_param_10];
	cvta.to.global.u64 	%rd1, %rd12;
	cvta.to.global.u64 	%rd2, %rd11;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r75;
	@%p1 bra 	$L__BB3_80;

	cvta.to.global.u64 	%rd13, %rd7;
	cvt.s64.s32 	%rd3, %r1;
	mul.wide.s32 	%rd14, %r1, 4;
	add.s64 	%rd15, %rd13, %rd14;
	cvta.to.global.u64 	%rd16, %rd8;
	add.s64 	%rd17, %rd16, %rd14;
	ld.global.nc.u32 	%r2, [%rd15];
	setp.lt.s32 	%p2, %r2, 1;
	ld.global.nc.u32 	%r3, [%rd17];
	setp.lt.s32 	%p3, %r3, 1;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB3_80;

	cvta.to.global.u64 	%rd18, %rd9;
	shl.b64 	%rd19, %rd3, 2;
	add.s64 	%rd20, %rd18, %rd19;
	ld.global.nc.u32 	%r4, [%rd20];
	cvta.to.global.u64 	%rd21, %rd10;
	add.s64 	%rd22, %rd21, %rd19;
	ld.global.nc.u32 	%r5, [%rd22];
	mul.lo.s32 	%r6, %r1, %r72;
	add.s32 	%r7, %r2, -1;
	add.s32 	%r76, %r73, %r7;
	add.s32 	%r77, %r76, %r3;
	add.s32 	%r8, %r77, -1;
	min.s32 	%r9, %r8, %r72;
	mov.u32 	%r10, %tid.x;
	setp.ge.s32 	%p5, %r10, %r9;
	@%p5 bra 	$L__BB3_5;

	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r144, %r10;

$L__BB3_4:
	add.s32 	%r78, %r144, %r6;
	mul.wide.s32 	%rd23, %r78, 4;
	add.s64 	%rd24, %rd1, %rd23;
	mov.u32 	%r79, 2143289344;
	st.global.u32 	[%rd24], %r79;
	add.s32 	%r144, %r144, %r11;
	setp.lt.s32 	%p6, %r144, %r9;
	@%p6 bra 	$L__BB3_4;

$L__BB3_5:
	bar.sync 	0;
	setp.ne.s32 	%p7, %r10, 0;
	@%p7 bra 	$L__BB3_80;

	setp.ge.s32 	%p8, %r73, %r72;
	setp.lt.s32 	%p9, %r72, 1;
	or.pred  	%p10, %p9, %p8;
	@%p10 bra 	$L__BB3_80;

	shl.b32 	%r14, %r74, 1;
	cvt.rn.f64.s32 	%fd93, %r3;
	rcp.rn.f64 	%fd1, %fd93;
	mov.f64 	%fd94, 0d3FF0000000000000;
	add.f64 	%fd95, %fd93, 0d3FF0000000000000;
	mov.f64 	%fd96, 0d4000000000000000;
	div.rn.f64 	%fd2, %fd96, %fd95;
	sub.f64 	%fd3, %fd94, %fd2;
	ld.global.nc.f32 	%f100, [%rd2];
	cvt.rn.f64.s32 	%fd4, %r2;
	and.b32  	%r15, %r2, 3;
	sub.s32 	%r16, %r2, %r15;
	cvt.s64.s32 	%rd4, %r6;
	mov.u16 	%rs20, 0;
	mov.f64 	%fd251, 0d0000000000000000;
	mov.u32 	%r163, 0;
	mov.f64 	%fd250, %fd251;
	mov.u32 	%r162, %r163;
	mov.f64 	%fd249, %fd251;
	mov.u32 	%r147, %r163;
	mov.f64 	%fd256, %fd251;
	mov.f64 	%fd264, %fd251;
	mov.f64 	%fd10, %fd251;
	mov.f64 	%fd11, %fd251;
	mov.u32 	%r20, %r163;
	mov.u32 	%r21, %r163;
	mov.u32 	%r22, %r163;
	mov.u32 	%r23, %r163;
	mov.u32 	%r171, %r163;
	mov.u32 	%r170, %r163;
	mov.f64 	%fd261, %fd251;
	mov.f64 	%fd260, %fd251;
	mov.u16 	%rs19, %rs20;

$L__BB3_8:
	mov.f32 	%f2, %f100;
	mov.f64 	%fd7, %fd249;
	mov.u32 	%r18, %r162;
	mov.u32 	%r17, %r163;
	mul.wide.s32 	%rd25, %r147, 4;
	add.s64 	%rd26, %rd2, %rd25;
	ld.global.nc.f32 	%f100, [%rd26];
	setp.eq.s32 	%p11, %r147, 0;
	mov.f32 	%f101, 0f7FC00000;
	@%p11 bra 	$L__BB3_11;

	mov.f32 	%f101, 0f7FC00000;
	abs.ftz.f32 	%f33, %f100;
	setp.geu.ftz.f32 	%p12, %f33, 0f7F800000;
	@%p12 bra 	$L__BB3_11;

	abs.ftz.f32 	%f34, %f2;
	setp.geu.ftz.f32 	%p13, %f34, 0f7F800000;
	sub.ftz.f32 	%f35, %f100, %f2;
	selp.f32 	%f101, 0f7FC00000, %f35, %p13;

$L__BB3_11:
	cvt.s64.s32 	%rd37, %r147;
	mov.f32 	%f102, 0f7FC00000;
	cvt.u32.u64 	%r89, %rd37;
	add.s32 	%r26, %r89, 1;
	setp.lt.s32 	%p14, %r26, %r2;
	mov.f64 	%fd249, %fd7;
	mov.u32 	%r162, %r18;
	mov.u32 	%r163, %r17;
	@%p14 bra 	$L__BB3_48;

	setp.eq.s32 	%p15, %r5, 0;
	@%p15 bra 	$L__BB3_34;

	mov.f32 	%f102, 0f7FC00000;
	abs.ftz.f32 	%f38, %f100;
	setp.geu.ftz.f32 	%p16, %f38, 0f7F800000;
	mov.u32 	%r162, 0;
	mov.f64 	%fd249, 0d0000000000000000;
	mov.u32 	%r163, %r162;
	@%p16 bra 	$L__BB3_48;

	setp.lt.s32 	%p17, %r18, %r2;
	add.s32 	%r92, %r17, %r14;
	shl.b32 	%r93, %r92, 2;
	mov.u32 	%r94, shraw;
	add.s32 	%r27, %r94, %r93;
	@%p17 bra 	$L__BB3_24;
	bra.uni 	$L__BB3_15;

$L__BB3_24:
	mov.f32 	%f102, 0f7FC00000;
	st.shared.f32 	[%r27], %f100;
	cvt.ftz.f64.f32 	%fd130, %f100;
	add.f64 	%fd249, %fd7, %fd130;
	add.s32 	%r105, %r17, 1;
	setp.eq.s32 	%p25, %r105, %r2;
	selp.b32 	%r163, 0, %r105, %p25;
	add.s32 	%r162, %r18, 1;
	setp.ne.s32 	%p26, %r162, %r2;
	@%p26 bra 	$L__BB3_48;

	div.rn.f64 	%fd25, %fd249, %fd4;
	mov.f64 	%fd240, 0d0000000000000000;
	@%p2 bra 	$L__BB3_33;

	setp.lt.u32 	%p28, %r7, 3;
	mov.f64 	%fd240, 0d0000000000000000;
	mov.u32 	%r159, 0;
	@%p28 bra 	$L__BB3_29;

	mov.f64 	%fd240, 0d0000000000000000;
	mov.u32 	%r159, 0;
	mov.u32 	%r158, %r16;

$L__BB3_28:
	mov.u32 	%r140, shraw;
	add.s32 	%r108, %r159, %r14;
	shl.b32 	%r109, %r108, 2;
	add.s32 	%r111, %r140, %r109;
	ld.shared.v2.f32 	{%f52, %f53}, [%r111];
	cvt.ftz.f64.f32 	%fd135, %f52;
	sub.f64 	%fd136, %fd135, %fd25;
	abs.f64 	%fd137, %fd136;
	add.f64 	%fd138, %fd240, %fd137;
	cvt.ftz.f64.f32 	%fd139, %f53;
	sub.f64 	%fd140, %fd139, %fd25;
	abs.f64 	%fd141, %fd140;
	add.f64 	%fd142, %fd138, %fd141;
	ld.shared.v2.f32 	{%f56, %f57}, [%r111+8];
	cvt.ftz.f64.f32 	%fd143, %f56;
	sub.f64 	%fd144, %fd143, %fd25;
	abs.f64 	%fd145, %fd144;
	add.f64 	%fd146, %fd142, %fd145;
	cvt.ftz.f64.f32 	%fd147, %f57;
	sub.f64 	%fd148, %fd147, %fd25;
	abs.f64 	%fd149, %fd148;
	add.f64 	%fd240, %fd146, %fd149;
	add.s32 	%r159, %r159, 4;
	add.s32 	%r158, %r158, -4;
	setp.ne.s32 	%p29, %r158, 0;
	@%p29 bra 	$L__BB3_28;

$L__BB3_29:
	setp.eq.s32 	%p30, %r15, 0;
	@%p30 bra 	$L__BB3_33;

	mov.u32 	%r141, shraw;
	setp.eq.s32 	%p31, %r15, 1;
	add.s32 	%r112, %r159, %r14;
	shl.b32 	%r113, %r112, 2;
	add.s32 	%r42, %r141, %r113;
	ld.shared.f32 	%f60, [%r42];
	cvt.ftz.f64.f32 	%fd150, %f60;
	sub.f64 	%fd151, %fd150, %fd25;
	abs.f64 	%fd152, %fd151;
	add.f64 	%fd240, %fd240, %fd152;
	@%p31 bra 	$L__BB3_33;

	setp.eq.s32 	%p32, %r15, 2;
	ld.shared.f32 	%f61, [%r42+4];
	cvt.ftz.f64.f32 	%fd153, %f61;
	sub.f64 	%fd154, %fd153, %fd25;
	abs.f64 	%fd155, %fd154;
	add.f64 	%fd240, %fd240, %fd155;
	@%p32 bra 	$L__BB3_33;

	ld.shared.f32 	%f62, [%r42+8];
	cvt.ftz.f64.f32 	%fd156, %f62;
	sub.f64 	%fd157, %fd156, %fd25;
	abs.f64 	%fd158, %fd157;
	add.f64 	%fd240, %fd240, %fd158;

$L__BB3_33:
	div.rn.f64 	%fd159, %fd240, %fd4;
	cvt.rn.ftz.f32.f64 	%f102, %fd159;
	mov.u32 	%r162, %r2;
	bra.uni 	$L__BB3_48;

$L__BB3_34:
	setp.eq.s32 	%p33, %r89, %r7;
	@%p33 bra 	$L__BB3_43;
	bra.uni 	$L__BB3_35;

$L__BB3_43:
	mov.f64 	%fd186, 0d0000000000000000;
	mov.f64 	%fd250, %fd186;
	mov.f64 	%fd251, %fd186;
	@%p2 bra 	$L__BB3_47;

	mov.f64 	%fd251, 0d0000000000000000;
	mov.u32 	%r161, 0;
	mov.f64 	%fd250, %fd251;

$L__BB3_45:
	mov.f32 	%f102, 0f7FC00000;
	mul.wide.s32 	%rd31, %r161, 4;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.nc.f32 	%f12, [%rd32];
	abs.ftz.f32 	%f68, %f12;
	setp.geu.ftz.f32 	%p40, %f68, 0f7F800000;
	mov.f64 	%fd249, %fd7;
	mov.u32 	%r162, %r18;
	mov.u32 	%r163, %r17;
	@%p40 bra 	$L__BB3_48;

	cvt.ftz.f64.f32 	%fd189, %f12;
	add.f64 	%fd251, %fd251, %fd189;
	fma.rn.f64 	%fd250, %fd189, %fd189, %fd250;
	add.s32 	%r161, %r161, 1;
	setp.lt.s32 	%p41, %r161, %r2;
	@%p41 bra 	$L__BB3_45;

$L__BB3_47:
	div.rn.f64 	%fd190, %fd251, %fd4;
	mul.f64 	%fd191, %fd190, %fd190;
	div.rn.f64 	%fd192, %fd250, %fd4;
	sub.f64 	%fd193, %fd192, %fd191;
	max.f64 	%fd195, %fd186, %fd193;
	sqrt.rn.f64 	%fd196, %fd195;
	cvt.rn.ftz.f32.f64 	%f102, %fd196;
	mov.f64 	%fd249, %fd7;
	mov.u32 	%r162, %r18;
	mov.u32 	%r163, %r17;
	bra.uni 	$L__BB3_48;

$L__BB3_35:
	sub.s32 	%r43, %r89, %r2;
	mul.wide.s32 	%rd27, %r43, 4;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.f32 	%f8, [%rd28];
	abs.ftz.f32 	%f63, %f8;
	setp.geu.ftz.f32 	%p34, %f63, 0f7F800000;
	@%p34 bra 	$L__BB3_38;

	abs.ftz.f32 	%f64, %f100;
	setp.geu.ftz.f32 	%p35, %f64, 0f7F800000;
	@%p35 bra 	$L__BB3_38;
	bra.uni 	$L__BB3_37;

$L__BB3_38:
	mov.f64 	%fd174, 0d0000000000000000;
	mov.f64 	%fd250, %fd174;
	mov.f64 	%fd251, %fd174;
	@%p2 bra 	$L__BB3_42;

	cvt.s64.s32 	%rd38, %r147;
	cvt.u32.u64 	%r143, %rd38;
	sub.s32 	%r160, %r143, %r2;
	mov.f64 	%fd251, 0d0000000000000000;
	mov.f64 	%fd250, %fd251;

$L__BB3_40:
	mov.f32 	%f102, 0f7FC00000;
	add.s32 	%r160, %r160, 1;
	mul.wide.s32 	%rd29, %r160, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f10, [%rd30];
	abs.ftz.f32 	%f66, %f10;
	setp.geu.ftz.f32 	%p37, %f66, 0f7F800000;
	mov.f64 	%fd249, %fd7;
	mov.u32 	%r162, %r18;
	mov.u32 	%r163, %r17;
	@%p37 bra 	$L__BB3_48;

	cvt.ftz.f64.f32 	%fd177, %f10;
	add.f64 	%fd251, %fd251, %fd177;
	fma.rn.f64 	%fd250, %fd177, %fd177, %fd250;
	setp.lt.s32 	%p38, %r160, %r89;
	@%p38 bra 	$L__BB3_40;

$L__BB3_42:
	div.rn.f64 	%fd178, %fd251, %fd4;
	mul.f64 	%fd179, %fd178, %fd178;
	div.rn.f64 	%fd180, %fd250, %fd4;
	sub.f64 	%fd181, %fd180, %fd179;
	max.f64 	%fd183, %fd174, %fd181;
	sqrt.rn.f64 	%fd184, %fd183;
	cvt.rn.ftz.f32.f64 	%f102, %fd184;
	mov.f64 	%fd249, %fd7;
	mov.u32 	%r162, %r18;
	mov.u32 	%r163, %r17;
	bra.uni 	$L__BB3_48;

$L__BB3_15:
	ld.shared.f32 	%f39, [%r27];
	st.shared.f32 	[%r27], %f100;
	cvt.ftz.f64.f32 	%fd99, %f39;
	cvt.ftz.f64.f32 	%fd100, %f100;
	sub.f64 	%fd101, %fd100, %fd99;
	add.f64 	%fd249, %fd7, %fd101;
	div.rn.f64 	%fd15, %fd249, %fd4;
	mov.f64 	%fd236, 0d0000000000000000;
	@%p2 bra 	$L__BB3_23;

	setp.lt.u32 	%p19, %r7, 3;
	mov.f64 	%fd236, 0d0000000000000000;
	mov.u32 	%r156, 0;
	@%p19 bra 	$L__BB3_19;

	mov.f64 	%fd236, 0d0000000000000000;
	mov.u32 	%r156, 0;
	mov.u32 	%r155, %r16;

$L__BB3_18:
	mov.u32 	%r138, shraw;
	add.s32 	%r97, %r156, %r14;
	shl.b32 	%r98, %r97, 2;
	add.s32 	%r100, %r138, %r98;
	ld.shared.v2.f32 	{%f40, %f41}, [%r100];
	cvt.ftz.f64.f32 	%fd105, %f40;
	sub.f64 	%fd106, %fd105, %fd15;
	abs.f64 	%fd107, %fd106;
	add.f64 	%fd108, %fd236, %fd107;
	cvt.ftz.f64.f32 	%fd109, %f41;
	sub.f64 	%fd110, %fd109, %fd15;
	abs.f64 	%fd111, %fd110;
	add.f64 	%fd112, %fd108, %fd111;
	ld.shared.v2.f32 	{%f44, %f45}, [%r100+8];
	cvt.ftz.f64.f32 	%fd113, %f44;
	sub.f64 	%fd114, %fd113, %fd15;
	abs.f64 	%fd115, %fd114;
	add.f64 	%fd116, %fd112, %fd115;
	cvt.ftz.f64.f32 	%fd117, %f45;
	sub.f64 	%fd118, %fd117, %fd15;
	abs.f64 	%fd119, %fd118;
	add.f64 	%fd236, %fd116, %fd119;
	add.s32 	%r156, %r156, 4;
	add.s32 	%r155, %r155, -4;
	setp.ne.s32 	%p20, %r155, 0;
	@%p20 bra 	$L__BB3_18;

$L__BB3_19:
	setp.eq.s32 	%p21, %r15, 0;
	@%p21 bra 	$L__BB3_23;

	mov.u32 	%r139, shraw;
	setp.eq.s32 	%p22, %r15, 1;
	add.s32 	%r101, %r156, %r14;
	shl.b32 	%r102, %r101, 2;
	add.s32 	%r33, %r139, %r102;
	ld.shared.f32 	%f48, [%r33];
	cvt.ftz.f64.f32 	%fd120, %f48;
	sub.f64 	%fd121, %fd120, %fd15;
	abs.f64 	%fd122, %fd121;
	add.f64 	%fd236, %fd236, %fd122;
	@%p22 bra 	$L__BB3_23;

	setp.eq.s32 	%p23, %r15, 2;
	ld.shared.f32 	%f49, [%r33+4];
	cvt.ftz.f64.f32 	%fd123, %f49;
	sub.f64 	%fd124, %fd123, %fd15;
	abs.f64 	%fd125, %fd124;
	add.f64 	%fd236, %fd236, %fd125;
	@%p23 bra 	$L__BB3_23;

	ld.shared.f32 	%f50, [%r33+8];
	cvt.ftz.f64.f32 	%fd126, %f50;
	sub.f64 	%fd127, %fd126, %fd15;
	abs.f64 	%fd128, %fd127;
	add.f64 	%fd236, %fd236, %fd128;

$L__BB3_23:
	add.s32 	%r104, %r17, 1;
	div.rn.f64 	%fd129, %fd236, %fd4;
	cvt.rn.ftz.f32.f64 	%f102, %fd129;
	setp.eq.s32 	%p24, %r104, %r2;
	selp.b32 	%r163, 0, %r104, %p24;
	mov.u32 	%r162, %r18;
	bra.uni 	$L__BB3_48;

$L__BB3_37:
	cvt.ftz.f64.f32 	%fd160, %f100;
	cvt.ftz.f64.f32 	%fd161, %f8;
	sub.f64 	%fd162, %fd160, %fd161;
	add.f64 	%fd251, %fd251, %fd162;
	mul.f64 	%fd163, %fd160, %fd160;
	mul.f64 	%fd164, %fd161, %fd161;
	sub.f64 	%fd165, %fd163, %fd164;
	add.f64 	%fd250, %fd250, %fd165;
	div.rn.f64 	%fd166, %fd251, %fd4;
	div.rn.f64 	%fd167, %fd250, %fd4;
	mul.f64 	%fd168, %fd166, %fd166;
	sub.f64 	%fd169, %fd167, %fd168;
	mov.f64 	%fd170, 0d0000000000000000;
	max.f64 	%fd171, %fd170, %fd169;
	sqrt.rn.f64 	%fd172, %fd171;
	cvt.rn.ftz.f32.f64 	%f102, %fd172;
	mov.f64 	%fd249, %fd7;
	mov.u32 	%r162, %r18;
	mov.u32 	%r163, %r17;

$L__BB3_48:
	abs.ftz.f32 	%f71, %f101;
	setp.geu.ftz.f32 	%p42, %f71, 0f7F800000;
	mov.f32 	%f17, 0f7FC00000;
	mov.f32 	%f104, 0f7FC00000;
	@%p42 bra 	$L__BB3_51;

	mov.f32 	%f104, 0f7FC00000;
	abs.ftz.f32 	%f74, %f102;
	setp.geu.ftz.f32 	%p43, %f74, 0f7F800000;
	@%p43 bra 	$L__BB3_51;

	setp.gt.ftz.f32 	%p44, %f101, 0f00000000;
	selp.f32 	%f17, %f102, 0f00000000, %p44;
	setp.lt.ftz.f32 	%p45, %f101, 0f00000000;
	selp.f32 	%f104, %f102, 0f00000000, %p45;

$L__BB3_51:
	setp.eq.s32 	%p46, %r4, 0;
	@%p46 bra 	$L__BB3_61;

	cvt.ftz.f64.f32 	%fd51, %f17;
	abs.f64 	%fd199, %fd51;
	setp.geu.f64 	%p47, %fd199, 0d7FF0000000000000;
	mov.f64 	%fd257, 0d7FF8000000000000;
	mov.u32 	%r165, 0;
	mov.f64 	%fd255, 0d0000000000000000;
	mov.u16 	%rs18, 0;
	mov.u16 	%rs17, %rs18;
	mov.f64 	%fd252, %fd255;
	mov.u32 	%r164, %r165;
	mov.f64 	%fd254, %fd257;
	@%p47 bra 	$L__BB3_56;

	and.b16  	%rs12, %rs19, 255;
	setp.eq.s16 	%p48, %rs12, 0;
	@%p48 bra 	$L__BB3_55;

	mul.f64 	%fd200, %fd2, %fd51;
	fma.rn.f64 	%fd264, %fd3, %fd264, %fd200;
	mov.u16 	%rs17, %rs19;
	mov.f64 	%fd252, %fd260;
	mov.u32 	%r164, %r170;
	mov.f64 	%fd254, %fd264;
	bra.uni 	$L__BB3_56;

$L__BB3_61:
	mov.f32 	%f105, 0f7FC00000;
	abs.ftz.f32 	%f76, %f17;
	setp.geu.ftz.f32 	%p53, %f76, 0f7F800000;
	mov.f64 	%fd259, 0d0000000000000000;
	mov.u32 	%r168, 0;
	mov.u32 	%r166, %r168;
	mov.u32 	%r167, %r168;
	mov.f64 	%fd258, %fd259;
	@%p53 bra 	$L__BB3_66;

	setp.lt.s32 	%p54, %r21, %r3;
	shl.b32 	%r123, %r23, 2;
	mov.u32 	%r124, shraw;
	add.s32 	%r54, %r124, %r123;
	@%p54 bra 	$L__BB3_64;
	bra.uni 	$L__BB3_63;

$L__BB3_64:
	mov.f32 	%f105, 0f7FC00000;
	st.shared.f32 	[%r54], %f17;
	cvt.ftz.f64.f32 	%fd212, %f17;
	add.f64 	%fd258, %fd11, %fd212;
	add.s32 	%r126, %r23, 1;
	setp.eq.s32 	%p56, %r126, %r3;
	selp.b32 	%r166, 0, %r126, %p56;
	add.s32 	%r167, %r21, 1;
	setp.ne.s32 	%p57, %r167, %r3;
	@%p57 bra 	$L__BB3_66;

	mul.f64 	%fd213, %fd1, %fd258;
	cvt.rn.ftz.f32.f64 	%f105, %fd213;
	mov.u32 	%r167, %r3;
	bra.uni 	$L__BB3_66;

$L__BB3_55:
	add.f64 	%fd252, %fd260, %fd51;
	add.s32 	%r164, %r170, 1;
	setp.eq.s32 	%p49, %r164, %r3;
	mul.f64 	%fd201, %fd1, %fd252;
	selp.u16 	%rs17, 1, 0, %p49;
	selp.f64 	%fd264, %fd201, %fd264, %p49;
	selp.f64 	%fd254, %fd201, 0d7FF8000000000000, %p49;

$L__BB3_56:
	cvt.ftz.f64.f32 	%fd59, %f104;
	abs.f64 	%fd204, %fd59;
	setp.geu.f64 	%p50, %fd204, 0d7FF0000000000000;
	@%p50 bra 	$L__BB3_60;

	and.b16  	%rs14, %rs20, 255;
	setp.eq.s16 	%p51, %rs14, 0;
	@%p51 bra 	$L__BB3_59;

	mul.f64 	%fd205, %fd2, %fd59;
	fma.rn.f64 	%fd256, %fd3, %fd256, %fd205;
	mov.u16 	%rs18, %rs20;
	mov.f64 	%fd255, %fd261;
	mov.u32 	%r165, %r171;
	mov.f64 	%fd257, %fd256;
	bra.uni 	$L__BB3_60;

$L__BB3_59:
	add.f64 	%fd255, %fd261, %fd59;
	add.s32 	%r165, %r171, 1;
	setp.eq.s32 	%p52, %r165, %r3;
	mul.f64 	%fd206, %fd1, %fd255;
	selp.u16 	%rs18, 1, 0, %p52;
	selp.f64 	%fd256, %fd206, %fd256, %p52;
	selp.f64 	%fd257, %fd206, 0d7FF8000000000000, %p52;

$L__BB3_60:
	cvt.rn.ftz.f32.f64 	%f105, %fd254;
	cvt.rn.ftz.f32.f64 	%f106, %fd257;
	mov.u16 	%rs19, %rs17;
	mov.u16 	%rs20, %rs18;
	mov.f64 	%fd260, %fd252;
	mov.f64 	%fd261, %fd255;
	mov.u32 	%r170, %r164;
	mov.u32 	%r171, %r165;
	bra.uni 	$L__BB3_72;

$L__BB3_63:
	ld.shared.f32 	%f77, [%r54];
	st.shared.f32 	[%r54], %f17;
	add.s32 	%r125, %r23, 1;
	setp.eq.s32 	%p55, %r125, %r3;
	selp.b32 	%r166, 0, %r125, %p55;
	cvt.ftz.f64.f32 	%fd208, %f77;
	cvt.ftz.f64.f32 	%fd209, %f17;
	sub.f64 	%fd210, %fd209, %fd208;
	add.f64 	%fd258, %fd11, %fd210;
	mul.f64 	%fd211, %fd1, %fd258;
	cvt.rn.ftz.f32.f64 	%f105, %fd211;
	mov.u32 	%r167, %r21;

$L__BB3_66:
	abs.ftz.f32 	%f80, %f104;
	setp.geu.ftz.f32 	%p58, %f80, 0f7F800000;
	mov.f32 	%f106, 0f7FC00000;
	mov.u32 	%r169, %r168;
	@%p58 bra 	$L__BB3_71;

	setp.lt.s32 	%p59, %r20, %r3;
	add.s32 	%r129, %r22, %r74;
	shl.b32 	%r130, %r129, 2;
	mov.u32 	%r131, shraw;
	add.s32 	%r60, %r131, %r130;
	@%p59 bra 	$L__BB3_69;
	bra.uni 	$L__BB3_68;

$L__BB3_69:
	mov.f32 	%f106, 0f7FC00000;
	st.shared.f32 	[%r60], %f104;
	cvt.ftz.f64.f32 	%fd219, %f104;
	add.f64 	%fd259, %fd10, %fd219;
	add.s32 	%r133, %r22, 1;
	setp.eq.s32 	%p61, %r133, %r3;
	selp.b32 	%r168, 0, %r133, %p61;
	add.s32 	%r169, %r20, 1;
	setp.ne.s32 	%p62, %r169, %r3;
	@%p62 bra 	$L__BB3_71;

	mul.f64 	%fd220, %fd1, %fd259;
	cvt.rn.ftz.f32.f64 	%f106, %fd220;
	mov.u32 	%r169, %r3;
	bra.uni 	$L__BB3_71;

$L__BB3_68:
	ld.shared.f32 	%f81, [%r60];
	st.shared.f32 	[%r60], %f104;
	add.s32 	%r132, %r22, 1;
	setp.eq.s32 	%p60, %r132, %r3;
	selp.b32 	%r168, 0, %r132, %p60;
	cvt.ftz.f64.f32 	%fd215, %f81;
	cvt.ftz.f64.f32 	%fd216, %f104;
	sub.f64 	%fd217, %fd216, %fd215;
	add.f64 	%fd259, %fd10, %fd217;
	mul.f64 	%fd218, %fd1, %fd259;
	cvt.rn.ftz.f32.f64 	%f106, %fd218;
	mov.u32 	%r169, %r20;

$L__BB3_71:
	cvt.ftz.f64.f32 	%fd254, %f105;
	cvt.ftz.f64.f32 	%fd257, %f106;
	mov.f64 	%fd10, %fd259;
	mov.f64 	%fd11, %fd258;
	mov.u32 	%r20, %r169;
	mov.u32 	%r21, %r167;
	mov.u32 	%r22, %r168;
	mov.u32 	%r23, %r166;

$L__BB3_72:
	setp.lt.s32 	%p63, %r89, %r8;
	@%p63 bra 	$L__BB3_79;

	cvt.s64.s32 	%rd35, %r147;
	abs.ftz.f32 	%f83, %f105;
	setp.geu.ftz.f32 	%p64, %f83, 0f7F800000;
	add.s64 	%rd33, %rd35, %rd4;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd6, %rd1, %rd34;
	@%p64 bra 	$L__BB3_78;

	abs.ftz.f32 	%f84, %f106;
	setp.geu.ftz.f32 	%p65, %f84, 0f7F800000;
	@%p65 bra 	$L__BB3_78;
	bra.uni 	$L__BB3_75;

$L__BB3_78:
	mov.u32 	%r135, 2143289344;
	st.global.u32 	[%rd6], %r135;
	bra.uni 	$L__BB3_79;

$L__BB3_75:
	add.f64 	%fd83, %fd254, %fd257;
	abs.f64 	%fd221, %fd83;
	setp.le.f64 	%p66, %fd221, 0d3CD203AF9EE75616;
	mov.f32 	%f109, 0f7FC00000;
	@%p66 bra 	$L__BB3_77;

	div.rn.f64 	%fd222, %fd254, %fd83;
	mul.f64 	%fd223, %fd222, 0d4059000000000000;
	cvt.rn.ftz.f32.f64 	%f109, %fd223;

$L__BB3_77:
	st.global.f32 	[%rd6], %f109;

$L__BB3_79:
	cvt.s64.s32 	%rd36, %r147;
	cvt.u32.u64 	%r137, %rd36;
	add.s32 	%r147, %r137, 1;
	setp.lt.s32 	%p67, %r147, %r72;
	@%p67 bra 	$L__BB3_8;

$L__BB3_80:
	ret;

}
	
.visible .entry rvi_many_series_one_param_f32(
	.param .u64 rvi_many_series_one_param_f32_param_0,
	.param .u64 rvi_many_series_one_param_f32_param_1,
	.param .u32 rvi_many_series_one_param_f32_param_2,
	.param .u32 rvi_many_series_one_param_f32_param_3,
	.param .u32 rvi_many_series_one_param_f32_param_4,
	.param .u32 rvi_many_series_one_param_f32_param_5,
	.param .u32 rvi_many_series_one_param_f32_param_6,
	.param .u32 rvi_many_series_one_param_f32_param_7,
	.param .u64 rvi_many_series_one_param_f32_param_8
)
{
	.local .align 16 .b8 	__local_depot4[16384];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<86>;
	.reg .b16 	%rs<24>;
	.reg .f32 	%f<135>;
	.reg .b32 	%r<209>;
	.reg .f64 	%fd<316>;
	.reg .b64 	%rd<78>;


	mov.u64 	%SPL, __local_depot4;
	ld.param.u64 	%rd22, [rvi_many_series_one_param_f32_param_0];
	ld.param.u64 	%rd23, [rvi_many_series_one_param_f32_param_1];
	ld.param.u32 	%r85, [rvi_many_series_one_param_f32_param_2];
	ld.param.u32 	%r86, [rvi_many_series_one_param_f32_param_3];
	ld.param.u32 	%r87, [rvi_many_series_one_param_f32_param_4];
	ld.param.u32 	%r88, [rvi_many_series_one_param_f32_param_5];
	ld.param.u32 	%r89, [rvi_many_series_one_param_f32_param_6];
	ld.param.u64 	%rd24, [rvi_many_series_one_param_f32_param_8];
	cvta.to.global.u64 	%rd1, %rd24;
	add.u64 	%rd2, %SPL, 8192;
	mov.u32 	%r91, %ntid.x;
	mov.u32 	%r92, %ctaid.x;
	mov.u32 	%r93, %tid.x;
	mad.lo.s32 	%r1, %r92, %r91, %r93;
	setp.ge.s32 	%p2, %r1, %r85;
	@%p2 bra 	$L__BB4_103;

	setp.lt.s32 	%p3, %r87, 1;
	setp.lt.s32 	%p4, %r88, 1;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB4_103;

	cvt.s64.s32 	%rd3, %r1;
	cvta.to.global.u64 	%rd26, %rd23;
	mul.wide.s32 	%rd27, %r1, 4;
	add.s64 	%rd28, %rd26, %rd27;
	add.s32 	%r2, %r87, -1;
	add.s32 	%r94, %r88, %r2;
	ld.global.nc.u32 	%r3, [%rd28];
	add.s32 	%r95, %r94, %r3;
	add.s32 	%r4, %r95, -1;
	min.s32 	%r5, %r4, %r86;
	setp.lt.s32 	%p6, %r5, 1;
	@%p6 bra 	$L__BB4_8;

	mov.u32 	%r97, 1;
	sub.s32 	%r98, %r97, %r3;
	sub.s32 	%r99, %r98, %r88;
	sub.s32 	%r100, %r99, %r87;
	not.b32 	%r101, %r86;
	max.s32 	%r102, %r100, %r101;
	mov.u32 	%r103, -2;
	sub.s32 	%r104, %r103, %r102;
	and.b32  	%r176, %r5, 3;
	setp.lt.u32 	%p7, %r104, 3;
	mov.u32 	%r174, 0;
	@%p7 bra 	$L__BB4_6;

	sub.s32 	%r173, %r5, %r176;
	mul.wide.s32 	%rd4, %r85, 4;
	mov.u32 	%r174, 0;

$L__BB4_5:
	mad.lo.s32 	%r106, %r174, %r85, %r1;
	mul.wide.s32 	%rd29, %r106, 4;
	add.s64 	%rd30, %rd1, %rd29;
	mov.u32 	%r107, 2143289344;
	st.global.u32 	[%rd30], %r107;
	add.s64 	%rd31, %rd30, %rd4;
	st.global.u32 	[%rd31], %r107;
	add.s64 	%rd32, %rd31, %rd4;
	st.global.u32 	[%rd32], %r107;
	add.s64 	%rd33, %rd32, %rd4;
	st.global.u32 	[%rd33], %r107;
	add.s32 	%r174, %r174, 4;
	add.s32 	%r173, %r173, -4;
	setp.ne.s32 	%p8, %r173, 0;
	@%p8 bra 	$L__BB4_5;

$L__BB4_6:
	setp.eq.s32 	%p9, %r176, 0;
	@%p9 bra 	$L__BB4_8;

$L__BB4_7:
	.pragma "nounroll";
	mad.lo.s32 	%r108, %r174, %r85, %r1;
	mul.wide.s32 	%rd34, %r108, 4;
	add.s64 	%rd35, %rd1, %rd34;
	mov.u32 	%r109, 2143289344;
	st.global.u32 	[%rd35], %r109;
	add.s32 	%r174, %r174, 1;
	add.s32 	%r176, %r176, -1;
	setp.ne.s32 	%p10, %r176, 0;
	@%p10 bra 	$L__BB4_7;

$L__BB4_8:
	setp.ge.s32 	%p11, %r4, %r86;
	@%p11 bra 	$L__BB4_103;

	cvt.rn.f64.s32 	%fd87, %r88;
	rcp.rn.f64 	%fd1, %fd87;
	mov.f64 	%fd88, 0d3FF0000000000000;
	add.f64 	%fd89, %fd87, 0d3FF0000000000000;
	mov.f64 	%fd90, 0d4000000000000000;
	div.rn.f64 	%fd2, %fd90, %fd89;
	sub.f64 	%fd3, %fd88, %fd2;
	setp.lt.s32 	%p12, %r86, 1;
	@%p12 bra 	$L__BB4_103;

	cvta.to.global.u64 	%rd36, %rd22;
	shl.b64 	%rd37, %rd3, 2;
	add.s64 	%rd5, %rd36, %rd37;
	ld.global.nc.f32 	%f124, [%rd5];
	cvt.rn.f64.s32 	%fd4, %r87;
	and.b32  	%r18, %r87, 3;
	sub.s32 	%r19, %r87, %r18;
	mul.wide.s32 	%rd6, %r85, 4;
	setp.lt.s32 	%p13, %r88, 1025;
	setp.eq.s32 	%p14, %r89, 0;
	mov.u32 	%r199, 0;
	and.pred  	%p1, %p13, %p14;
	mov.u16 	%rs23, 0;
	mov.f64 	%fd306, 0d0000000000000000;
	mov.f64 	%fd305, %fd306;
	mov.u32 	%r198, %r199;
	mov.f64 	%fd304, %fd306;
	mov.u32 	%r179, %r199;
	mov.f64 	%fd315, %fd306;
	mov.f64 	%fd314, %fd306;
	mov.f64 	%fd313, %fd306;
	mov.f64 	%fd312, %fd306;
	mov.u32 	%r208, %r199;
	mov.u32 	%r207, %r199;
	mov.u32 	%r206, %r199;
	mov.u32 	%r205, %r199;
	mov.u32 	%r204, %r199;
	mov.u32 	%r203, %r199;
	mov.f64 	%fd311, %fd306;
	mov.f64 	%fd310, %fd306;
	mov.u16 	%rs22, %rs23;

$L__BB4_11:
	mov.u16 	%rs2, %rs22;
	mov.u16 	%rs1, %rs23;
	mov.f64 	%fd13, %fd310;
	mov.f64 	%fd12, %fd311;
	mov.u32 	%r28, %r203;
	mov.u32 	%r27, %r204;
	mov.u32 	%r25, %r206;
	mov.u32 	%r23, %r208;
	mov.f64 	%fd10, %fd313;
	mov.f32 	%f2, %f124;
	mov.f64 	%fd7, %fd304;
	mov.u32 	%r21, %r198;
	mov.u32 	%r20, %r199;
	mov.u32 	%r161, 1;
	sub.s32 	%r160, %r161, %r87;
	add.s32 	%r120, %r160, %r179;
	max.s32 	%r121, %r179, %r120;
	sub.s32 	%r122, %r87, %r179;
	add.s32 	%r29, %r121, %r122;
	mul.lo.s32 	%r30, %r179, %r85;
	mul.wide.s32 	%rd40, %r30, 4;
	add.s64 	%rd41, %rd5, %rd40;
	ld.global.nc.f32 	%f124, [%rd41];
	setp.eq.s32 	%p15, %r179, 0;
	mov.f32 	%f125, 0f7FC00000;
	@%p15 bra 	$L__BB4_14;

	mov.f32 	%f125, 0f7FC00000;
	abs.ftz.f32 	%f39, %f124;
	setp.geu.ftz.f32 	%p16, %f39, 0f7F800000;
	@%p16 bra 	$L__BB4_14;

	abs.ftz.f32 	%f40, %f2;
	setp.geu.ftz.f32 	%p17, %f40, 0f7F800000;
	sub.ftz.f32 	%f41, %f124, %f2;
	selp.f32 	%f125, 0f7FC00000, %f41, %p17;

$L__BB4_14:
	mov.f32 	%f127, 0f7FC00000;
	add.s32 	%r31, %r179, 1;
	setp.lt.s32 	%p18, %r31, %r87;
	mov.f64 	%fd304, %fd7;
	mov.u32 	%r198, %r21;
	mov.u32 	%r199, %r20;
	@%p18 bra 	$L__BB4_71;

	ld.param.u32 	%r162, [rvi_many_series_one_param_f32_param_7];
	setp.eq.s32 	%p19, %r162, 0;
	sub.s32 	%r32, %r179, %r87;
	mul.lo.s32 	%r123, %r32, %r85;
	mul.wide.s32 	%rd42, %r123, 4;
	add.s64 	%rd9, %rd5, %rd42;
	@%p19 bra 	$L__BB4_57;

	mov.f32 	%f127, 0f7FC00000;
	abs.ftz.f32 	%f44, %f124;
	setp.geu.ftz.f32 	%p20, %f44, 0f7F800000;
	mov.u32 	%r198, 0;
	mov.f64 	%fd304, 0d0000000000000000;
	mov.u32 	%r199, %r198;
	@%p20 bra 	$L__BB4_71;

	setp.lt.s32 	%p21, %r21, %r87;
	sub.s32 	%r33, %r179, %r87;
	add.s32 	%r126, %r33, 1;
	mul.lo.s32 	%r127, %r126, %r85;
	mul.wide.s32 	%rd44, %r127, 4;
	add.s64 	%rd11, %rd5, %rd44;
	add.s32 	%r128, %r127, %r85;
	mul.wide.s32 	%rd45, %r128, 4;
	add.s64 	%rd12, %rd5, %rd45;
	add.s32 	%r129, %r128, %r85;
	mul.wide.s32 	%rd46, %r129, 4;
	add.s64 	%rd13, %rd5, %rd46;
	@%p21 bra 	$L__BB4_38;
	bra.uni 	$L__BB4_18;

$L__BB4_38:
	setp.gt.s32 	%p36, %r87, 2048;
	@%p36 bra 	$L__BB4_40;

	mul.wide.s32 	%rd76, %r20, 4;
	add.s64 	%rd75, %rd2, %rd76;
	st.local.f32 	[%rd75], %f124;

$L__BB4_40:
	mov.f32 	%f127, 0f7FC00000;
	cvt.ftz.f64.f32 	%fd159, %f124;
	add.f64 	%fd304, %fd7, %fd159;
	add.s32 	%r136, %r20, 1;
	setp.eq.s32 	%p37, %r136, %r87;
	selp.b32 	%r199, 0, %r136, %p37;
	add.s32 	%r198, %r21, 1;
	setp.ne.s32 	%p38, %r198, %r87;
	@%p38 bra 	$L__BB4_71;

	setp.lt.s32 	%p39, %r87, 2049;
	div.rn.f64 	%fd32, %fd304, %fd4;
	@%p39 bra 	$L__BB4_48;
	bra.uni 	$L__BB4_42;

$L__BB4_48:
	mov.f64 	%fd295, 0d0000000000000000;
	@%p3 bra 	$L__BB4_56;

	setp.lt.u32 	%p46, %r2, 3;
	mov.f64 	%fd295, 0d0000000000000000;
	mov.u32 	%r195, 0;
	@%p46 bra 	$L__BB4_52;

	mov.f64 	%fd295, 0d0000000000000000;
	mov.u32 	%r195, 0;
	mov.u32 	%r194, %r19;

$L__BB4_51:
	mul.wide.s32 	%rd59, %r195, 4;
	add.s64 	%rd60, %rd2, %rd59;
	ld.local.v4.f32 	{%f71, %f72, %f73, %f74}, [%rd60];
	cvt.ftz.f64.f32 	%fd190, %f71;
	sub.f64 	%fd191, %fd190, %fd32;
	abs.f64 	%fd192, %fd191;
	add.f64 	%fd193, %fd295, %fd192;
	cvt.ftz.f64.f32 	%fd194, %f72;
	sub.f64 	%fd195, %fd194, %fd32;
	abs.f64 	%fd196, %fd195;
	add.f64 	%fd197, %fd193, %fd196;
	cvt.ftz.f64.f32 	%fd198, %f73;
	sub.f64 	%fd199, %fd198, %fd32;
	abs.f64 	%fd200, %fd199;
	add.f64 	%fd201, %fd197, %fd200;
	cvt.ftz.f64.f32 	%fd202, %f74;
	sub.f64 	%fd203, %fd202, %fd32;
	abs.f64 	%fd204, %fd203;
	add.f64 	%fd295, %fd201, %fd204;
	add.s32 	%r195, %r195, 4;
	add.s32 	%r194, %r194, -4;
	setp.ne.s32 	%p47, %r194, 0;
	@%p47 bra 	$L__BB4_51;

$L__BB4_52:
	setp.eq.s32 	%p48, %r18, 0;
	@%p48 bra 	$L__BB4_56;

	setp.eq.s32 	%p49, %r18, 1;
	mul.wide.s32 	%rd61, %r195, 4;
	add.s64 	%rd18, %rd2, %rd61;
	ld.local.f32 	%f79, [%rd18];
	cvt.ftz.f64.f32 	%fd205, %f79;
	sub.f64 	%fd206, %fd205, %fd32;
	abs.f64 	%fd207, %fd206;
	add.f64 	%fd295, %fd295, %fd207;
	@%p49 bra 	$L__BB4_56;

	setp.eq.s32 	%p50, %r18, 2;
	ld.local.f32 	%f80, [%rd18+4];
	cvt.ftz.f64.f32 	%fd208, %f80;
	sub.f64 	%fd209, %fd208, %fd32;
	abs.f64 	%fd210, %fd209;
	add.f64 	%fd295, %fd295, %fd210;
	@%p50 bra 	$L__BB4_56;

	ld.local.f32 	%f81, [%rd18+8];
	cvt.ftz.f64.f32 	%fd211, %f81;
	sub.f64 	%fd212, %fd211, %fd32;
	abs.f64 	%fd213, %fd212;
	add.f64 	%fd295, %fd295, %fd213;
	bra.uni 	$L__BB4_56;

$L__BB4_57:
	setp.eq.s32 	%p51, %r2, %r179;
	@%p51 bra 	$L__BB4_66;
	bra.uni 	$L__BB4_58;

$L__BB4_66:
	mov.f64 	%fd241, 0d0000000000000000;
	mov.f64 	%fd305, %fd241;
	mov.f64 	%fd306, %fd241;
	@%p3 bra 	$L__BB4_70;

	mov.f64 	%fd306, 0d0000000000000000;
	mov.u32 	%r197, 0;
	mov.f64 	%fd305, %fd306;

$L__BB4_68:
	mov.f32 	%f127, 0f7FC00000;
	mul.lo.s32 	%r144, %r197, %r85;
	mul.wide.s32 	%rd64, %r144, 4;
	add.s64 	%rd65, %rd5, %rd64;
	ld.global.nc.f32 	%f15, [%rd65];
	abs.ftz.f32 	%f87, %f15;
	setp.geu.ftz.f32 	%p58, %f87, 0f7F800000;
	mov.f64 	%fd304, %fd7;
	mov.u32 	%r198, %r21;
	mov.u32 	%r199, %r20;
	@%p58 bra 	$L__BB4_71;

	cvt.ftz.f64.f32 	%fd244, %f15;
	add.f64 	%fd306, %fd306, %fd244;
	fma.rn.f64 	%fd305, %fd244, %fd244, %fd305;
	add.s32 	%r197, %r197, 1;
	setp.lt.s32 	%p59, %r197, %r87;
	@%p59 bra 	$L__BB4_68;

$L__BB4_70:
	div.rn.f64 	%fd245, %fd306, %fd4;
	mul.f64 	%fd246, %fd245, %fd245;
	div.rn.f64 	%fd247, %fd305, %fd4;
	sub.f64 	%fd248, %fd247, %fd246;
	max.f64 	%fd250, %fd241, %fd248;
	sqrt.rn.f64 	%fd251, %fd250;
	cvt.rn.ftz.f32.f64 	%f127, %fd251;
	mov.f64 	%fd304, %fd7;
	mov.u32 	%r198, %r21;
	mov.u32 	%r199, %r20;
	bra.uni 	$L__BB4_71;

$L__BB4_58:
	ld.global.nc.f32 	%f11, [%rd9];
	abs.ftz.f32 	%f82, %f11;
	setp.geu.ftz.f32 	%p52, %f82, 0f7F800000;
	@%p52 bra 	$L__BB4_61;

	abs.ftz.f32 	%f83, %f124;
	setp.geu.ftz.f32 	%p53, %f83, 0f7F800000;
	@%p53 bra 	$L__BB4_61;
	bra.uni 	$L__BB4_60;

$L__BB4_61:
	mov.f64 	%fd229, 0d0000000000000000;
	mov.f64 	%fd305, %fd229;
	mov.f64 	%fd306, %fd229;
	@%p3 bra 	$L__BB4_65;

	sub.s32 	%r196, %r179, %r87;
	mov.f64 	%fd306, 0d0000000000000000;
	mov.f64 	%fd305, %fd306;

$L__BB4_63:
	mov.f32 	%f127, 0f7FC00000;
	add.s32 	%r196, %r196, 1;
	mul.lo.s32 	%r142, %r196, %r85;
	mul.wide.s32 	%rd62, %r142, 4;
	add.s64 	%rd63, %rd5, %rd62;
	ld.global.nc.f32 	%f13, [%rd63];
	abs.ftz.f32 	%f85, %f13;
	setp.geu.ftz.f32 	%p55, %f85, 0f7F800000;
	mov.f64 	%fd304, %fd7;
	mov.u32 	%r198, %r21;
	mov.u32 	%r199, %r20;
	@%p55 bra 	$L__BB4_71;

	cvt.ftz.f64.f32 	%fd232, %f13;
	add.f64 	%fd306, %fd306, %fd232;
	fma.rn.f64 	%fd305, %fd232, %fd232, %fd305;
	setp.lt.s32 	%p56, %r196, %r179;
	@%p56 bra 	$L__BB4_63;

$L__BB4_65:
	div.rn.f64 	%fd233, %fd306, %fd4;
	mul.f64 	%fd234, %fd233, %fd233;
	div.rn.f64 	%fd235, %fd305, %fd4;
	sub.f64 	%fd236, %fd235, %fd234;
	max.f64 	%fd238, %fd229, %fd236;
	sqrt.rn.f64 	%fd239, %fd238;
	cvt.rn.ftz.f32.f64 	%f127, %fd239;
	mov.f64 	%fd304, %fd7;
	mov.u32 	%r198, %r21;
	mov.u32 	%r199, %r20;
	bra.uni 	$L__BB4_71;

$L__BB4_18:
	setp.lt.s32 	%p22, %r87, 2049;
	@%p22 bra 	$L__BB4_20;
	bra.uni 	$L__BB4_19;

$L__BB4_20:
	mul.wide.s32 	%rd74, %r20, 4;
	add.s64 	%rd73, %rd2, %rd74;
	ld.local.f32 	%f126, [%rd73];
	st.local.f32 	[%rd73], %f124;
	bra.uni 	$L__BB4_21;

$L__BB4_19:
	ld.global.nc.f32 	%f126, [%rd9];

$L__BB4_21:
	setp.lt.s32 	%p85, %r87, 2049;
	cvt.ftz.f64.f32 	%fd101, %f126;
	cvt.ftz.f64.f32 	%fd102, %f124;
	sub.f64 	%fd103, %fd102, %fd101;
	add.f64 	%fd304, %fd7, %fd103;
	div.rn.f64 	%fd15, %fd304, %fd4;
	@%p85 bra 	$L__BB4_29;
	bra.uni 	$L__BB4_22;

$L__BB4_29:
	mov.f64 	%fd288, 0d0000000000000000;
	@%p3 bra 	$L__BB4_37;

	setp.lt.u32 	%p30, %r2, 3;
	mov.f64 	%fd288, 0d0000000000000000;
	mov.u32 	%r190, 0;
	@%p30 bra 	$L__BB4_33;

	mov.f64 	%fd288, 0d0000000000000000;
	mov.u32 	%r190, 0;
	mov.u32 	%r189, %r19;

$L__BB4_32:
	mul.wide.s32 	%rd51, %r190, 4;
	add.s64 	%rd52, %rd2, %rd51;
	ld.local.v4.f32 	{%f52, %f53, %f54, %f55}, [%rd52];
	cvt.ftz.f64.f32 	%fd134, %f52;
	sub.f64 	%fd135, %fd134, %fd15;
	abs.f64 	%fd136, %fd135;
	add.f64 	%fd137, %fd288, %fd136;
	cvt.ftz.f64.f32 	%fd138, %f53;
	sub.f64 	%fd139, %fd138, %fd15;
	abs.f64 	%fd140, %fd139;
	add.f64 	%fd141, %fd137, %fd140;
	cvt.ftz.f64.f32 	%fd142, %f54;
	sub.f64 	%fd143, %fd142, %fd15;
	abs.f64 	%fd144, %fd143;
	add.f64 	%fd145, %fd141, %fd144;
	cvt.ftz.f64.f32 	%fd146, %f55;
	sub.f64 	%fd147, %fd146, %fd15;
	abs.f64 	%fd148, %fd147;
	add.f64 	%fd288, %fd145, %fd148;
	add.s32 	%r190, %r190, 4;
	add.s32 	%r189, %r189, -4;
	setp.ne.s32 	%p31, %r189, 0;
	@%p31 bra 	$L__BB4_32;

$L__BB4_33:
	setp.eq.s32 	%p32, %r18, 0;
	@%p32 bra 	$L__BB4_37;

	setp.eq.s32 	%p33, %r18, 1;
	mul.wide.s32 	%rd53, %r190, 4;
	add.s64 	%rd17, %rd2, %rd53;
	ld.local.f32 	%f60, [%rd17];
	cvt.ftz.f64.f32 	%fd149, %f60;
	sub.f64 	%fd150, %fd149, %fd15;
	abs.f64 	%fd151, %fd150;
	add.f64 	%fd288, %fd288, %fd151;
	@%p33 bra 	$L__BB4_37;

	setp.eq.s32 	%p34, %r18, 2;
	ld.local.f32 	%f61, [%rd17+4];
	cvt.ftz.f64.f32 	%fd152, %f61;
	sub.f64 	%fd153, %fd152, %fd15;
	abs.f64 	%fd154, %fd153;
	add.f64 	%fd288, %fd288, %fd154;
	@%p34 bra 	$L__BB4_37;

	ld.local.f32 	%f62, [%rd17+8];
	cvt.ftz.f64.f32 	%fd155, %f62;
	sub.f64 	%fd156, %fd155, %fd15;
	abs.f64 	%fd157, %fd156;
	add.f64 	%fd288, %fd288, %fd157;
	bra.uni 	$L__BB4_37;

$L__BB4_22:
	sub.s32 	%r186, %r179, %r87;
	and.b32  	%r35, %r29, 3;
	setp.eq.s32 	%p24, %r35, 0;
	mov.f64 	%fd288, 0d0000000000000000;
	@%p24 bra 	$L__BB4_26;

	sub.s32 	%r169, %r179, %r87;
	add.s32 	%r186, %r169, 1;
	ld.global.nc.f32 	%f45, [%rd11];
	cvt.ftz.f64.f32 	%fd106, %f45;
	sub.f64 	%fd107, %fd106, %fd15;
	abs.f64 	%fd108, %fd107;
	add.f64 	%fd288, %fd108, 0d0000000000000000;
	setp.eq.s32 	%p25, %r35, 1;
	@%p25 bra 	$L__BB4_26;

	sub.s32 	%r170, %r179, %r87;
	add.s32 	%r186, %r170, 2;
	ld.global.nc.f32 	%f46, [%rd12];
	cvt.ftz.f64.f32 	%fd109, %f46;
	sub.f64 	%fd110, %fd109, %fd15;
	abs.f64 	%fd111, %fd110;
	add.f64 	%fd288, %fd288, %fd111;
	setp.eq.s32 	%p26, %r35, 2;
	@%p26 bra 	$L__BB4_26;

	sub.s32 	%r171, %r179, %r87;
	add.s32 	%r186, %r171, 3;
	ld.global.nc.f32 	%f47, [%rd13];
	cvt.ftz.f64.f32 	%fd112, %f47;
	sub.f64 	%fd113, %fd112, %fd15;
	abs.f64 	%fd114, %fd113;
	add.f64 	%fd288, %fd288, %fd114;

$L__BB4_26:
	add.s32 	%r130, %r29, -1;
	setp.lt.u32 	%p27, %r130, 3;
	@%p27 bra 	$L__BB4_37;

	add.s32 	%r131, %r186, 1;
	mul.lo.s32 	%r132, %r85, %r131;
	mul.wide.s32 	%rd47, %r132, 4;
	add.s64 	%rd77, %rd5, %rd47;

$L__BB4_28:
	ld.global.nc.f32 	%f48, [%rd77];
	cvt.ftz.f64.f32 	%fd115, %f48;
	sub.f64 	%fd116, %fd115, %fd15;
	abs.f64 	%fd117, %fd116;
	add.f64 	%fd118, %fd288, %fd117;
	add.s64 	%rd48, %rd77, %rd6;
	ld.global.nc.f32 	%f49, [%rd48];
	cvt.ftz.f64.f32 	%fd119, %f49;
	sub.f64 	%fd120, %fd119, %fd15;
	abs.f64 	%fd121, %fd120;
	add.f64 	%fd122, %fd118, %fd121;
	add.s64 	%rd49, %rd48, %rd6;
	ld.global.nc.f32 	%f50, [%rd49];
	cvt.ftz.f64.f32 	%fd123, %f50;
	sub.f64 	%fd124, %fd123, %fd15;
	abs.f64 	%fd125, %fd124;
	add.f64 	%fd126, %fd122, %fd125;
	add.s64 	%rd50, %rd49, %rd6;
	add.s64 	%rd77, %rd50, %rd6;
	ld.global.nc.f32 	%f51, [%rd50];
	cvt.ftz.f64.f32 	%fd127, %f51;
	sub.f64 	%fd128, %fd127, %fd15;
	abs.f64 	%fd129, %fd128;
	add.f64 	%fd288, %fd126, %fd129;
	add.s32 	%r186, %r186, 4;
	setp.lt.s32 	%p28, %r186, %r179;
	@%p28 bra 	$L__BB4_28;

$L__BB4_37:
	add.s32 	%r135, %r20, 1;
	div.rn.f64 	%fd158, %fd288, %fd4;
	cvt.rn.ftz.f32.f64 	%f127, %fd158;
	setp.eq.s32 	%p35, %r135, %r87;
	selp.b32 	%r199, 0, %r135, %p35;
	mov.u32 	%r198, %r21;
	bra.uni 	$L__BB4_71;

$L__BB4_60:
	cvt.ftz.f64.f32 	%fd215, %f124;
	cvt.ftz.f64.f32 	%fd216, %f11;
	sub.f64 	%fd217, %fd215, %fd216;
	add.f64 	%fd306, %fd306, %fd217;
	mul.f64 	%fd218, %fd215, %fd215;
	mul.f64 	%fd219, %fd216, %fd216;
	sub.f64 	%fd220, %fd218, %fd219;
	add.f64 	%fd305, %fd305, %fd220;
	div.rn.f64 	%fd221, %fd306, %fd4;
	div.rn.f64 	%fd222, %fd305, %fd4;
	mul.f64 	%fd223, %fd221, %fd221;
	sub.f64 	%fd224, %fd222, %fd223;
	mov.f64 	%fd225, 0d0000000000000000;
	max.f64 	%fd226, %fd225, %fd224;
	sqrt.rn.f64 	%fd227, %fd226;
	cvt.rn.ftz.f32.f64 	%f127, %fd227;
	mov.f64 	%fd304, %fd7;
	mov.u32 	%r198, %r21;
	mov.u32 	%r199, %r20;
	bra.uni 	$L__BB4_71;

$L__BB4_42:
	sub.s32 	%r191, %r179, %r87;
	and.b32  	%r50, %r29, 3;
	setp.eq.s32 	%p40, %r50, 0;
	mov.f64 	%fd295, 0d0000000000000000;
	@%p40 bra 	$L__BB4_46;

	sub.s32 	%r165, %r179, %r87;
	add.s32 	%r191, %r165, 1;
	ld.global.nc.f32 	%f64, [%rd11];
	cvt.ftz.f64.f32 	%fd162, %f64;
	sub.f64 	%fd163, %fd162, %fd32;
	abs.f64 	%fd164, %fd163;
	add.f64 	%fd295, %fd164, 0d0000000000000000;
	setp.eq.s32 	%p41, %r50, 1;
	@%p41 bra 	$L__BB4_46;

	sub.s32 	%r166, %r179, %r87;
	add.s32 	%r191, %r166, 2;
	ld.global.nc.f32 	%f65, [%rd12];
	cvt.ftz.f64.f32 	%fd165, %f65;
	sub.f64 	%fd166, %fd165, %fd32;
	abs.f64 	%fd167, %fd166;
	add.f64 	%fd295, %fd295, %fd167;
	setp.eq.s32 	%p42, %r50, 2;
	@%p42 bra 	$L__BB4_46;

	sub.s32 	%r167, %r179, %r87;
	add.s32 	%r191, %r167, 3;
	ld.global.nc.f32 	%f66, [%rd13];
	cvt.ftz.f64.f32 	%fd168, %f66;
	sub.f64 	%fd169, %fd168, %fd32;
	abs.f64 	%fd170, %fd169;
	add.f64 	%fd295, %fd295, %fd170;

$L__BB4_46:
	add.s32 	%r137, %r29, -1;
	setp.lt.u32 	%p43, %r137, 3;
	@%p43 bra 	$L__BB4_56;

$L__BB4_47:
	add.s32 	%r138, %r191, 1;
	mul.lo.s32 	%r139, %r138, %r85;
	mul.wide.s32 	%rd54, %r139, 4;
	add.s64 	%rd55, %rd5, %rd54;
	ld.global.nc.f32 	%f67, [%rd55];
	cvt.ftz.f64.f32 	%fd171, %f67;
	sub.f64 	%fd172, %fd171, %fd32;
	abs.f64 	%fd173, %fd172;
	add.f64 	%fd174, %fd295, %fd173;
	add.s64 	%rd56, %rd55, %rd6;
	ld.global.nc.f32 	%f68, [%rd56];
	cvt.ftz.f64.f32 	%fd175, %f68;
	sub.f64 	%fd176, %fd175, %fd32;
	abs.f64 	%fd177, %fd176;
	add.f64 	%fd178, %fd174, %fd177;
	add.s64 	%rd57, %rd56, %rd6;
	ld.global.nc.f32 	%f69, [%rd57];
	cvt.ftz.f64.f32 	%fd179, %f69;
	sub.f64 	%fd180, %fd179, %fd32;
	abs.f64 	%fd181, %fd180;
	add.f64 	%fd182, %fd178, %fd181;
	add.s64 	%rd58, %rd57, %rd6;
	ld.global.nc.f32 	%f70, [%rd58];
	cvt.ftz.f64.f32 	%fd183, %f70;
	sub.f64 	%fd184, %fd183, %fd32;
	abs.f64 	%fd185, %fd184;
	add.f64 	%fd295, %fd182, %fd185;
	add.s32 	%r191, %r191, 4;
	setp.lt.s32 	%p44, %r191, %r179;
	@%p44 bra 	$L__BB4_47;

$L__BB4_56:
	div.rn.f64 	%fd214, %fd295, %fd4;
	cvt.rn.ftz.f32.f64 	%f127, %fd214;
	mov.u32 	%r198, %r87;

$L__BB4_71:
	abs.ftz.f32 	%f90, %f125;
	setp.geu.ftz.f32 	%p60, %f90, 0f7F800000;
	mov.f32 	%f20, 0f7FC00000;
	mov.f32 	%f132, 0f7FC00000;
	mov.f32 	%f129, %f132;
	@%p60 bra 	$L__BB4_74;

	abs.ftz.f32 	%f93, %f127;
	setp.geu.ftz.f32 	%p61, %f93, 0f7F800000;
	@%p61 bra 	$L__BB4_74;

	setp.gt.ftz.f32 	%p62, %f125, 0f00000000;
	selp.f32 	%f20, %f127, 0f00000000, %p62;
	setp.lt.ftz.f32 	%p63, %f125, 0f00000000;
	selp.f32 	%f129, %f127, 0f00000000, %p63;

$L__BB4_74:
	abs.ftz.f32 	%f22, %f20;
	@%p1 bra 	$L__BB4_85;
	bra.uni 	$L__BB4_75;

$L__BB4_85:
	setp.geu.ftz.f32 	%p70, %f22, 0f7F800000;
	mov.f64 	%fd313, 0d0000000000000000;
	mov.u32 	%r206, 0;
	mov.u32 	%r201, %r206;
	mov.u32 	%r202, %r206;
	mov.f64 	%fd309, %fd313;
	@%p70 bra 	$L__BB4_90;

	add.u64 	%rd69, %SPL, 0;
	setp.lt.s32 	%p71, %r207, %r88;
	mul.wide.s32 	%rd66, %r205, 4;
	add.s64 	%rd19, %rd69, %rd66;
	@%p71 bra 	$L__BB4_88;
	bra.uni 	$L__BB4_87;

$L__BB4_88:
	mov.f32 	%f132, 0f7FC00000;
	st.local.f32 	[%rd19], %f20;
	cvt.ftz.f64.f32 	%fd261, %f20;
	add.f64 	%fd309, %fd312, %fd261;
	add.s32 	%r150, %r205, 1;
	setp.eq.s32 	%p73, %r150, %r88;
	selp.b32 	%r201, 0, %r150, %p73;
	add.s32 	%r202, %r207, 1;
	setp.ne.s32 	%p74, %r202, %r88;
	@%p74 bra 	$L__BB4_90;

	mul.f64 	%fd262, %fd1, %fd309;
	cvt.rn.ftz.f32.f64 	%f132, %fd262;
	mov.u32 	%r202, %r88;
	bra.uni 	$L__BB4_90;

$L__BB4_75:
	setp.geu.ftz.f32 	%p64, %f22, 0f7F800000;
	mov.u32 	%r204, 0;
	mov.f64 	%fd311, 0d0000000000000000;
	mov.u16 	%rs22, 0;
	mov.f64 	%fd310, %fd311;
	mov.u32 	%r203, %r204;
	@%p64 bra 	$L__BB4_80;

	and.b16  	%rs9, %rs2, 255;
	setp.eq.s16 	%p65, %rs9, 0;
	cvt.ftz.f64.f32 	%fd65, %f20;
	@%p65 bra 	$L__BB4_78;

	mul.f64 	%fd253, %fd2, %fd65;
	fma.rn.f64 	%fd314, %fd3, %fd314, %fd253;
	cvt.rn.ftz.f32.f64 	%f132, %fd314;
	mov.u16 	%rs22, %rs2;
	mov.f64 	%fd310, %fd13;
	mov.u32 	%r203, %r28;
	bra.uni 	$L__BB4_80;

$L__BB4_87:
	ld.local.f32 	%f100, [%rd19];
	st.local.f32 	[%rd19], %f20;
	add.s32 	%r149, %r205, 1;
	setp.eq.s32 	%p72, %r149, %r88;
	selp.b32 	%r201, 0, %r149, %p72;
	cvt.ftz.f64.f32 	%fd257, %f100;
	cvt.ftz.f64.f32 	%fd258, %f20;
	sub.f64 	%fd259, %fd258, %fd257;
	add.f64 	%fd309, %fd312, %fd259;
	mul.f64 	%fd260, %fd1, %fd309;
	cvt.rn.ftz.f32.f64 	%f132, %fd260;
	mov.u32 	%r202, %r207;

$L__BB4_90:
	abs.ftz.f32 	%f103, %f129;
	setp.geu.ftz.f32 	%p75, %f103, 0f7F800000;
	mov.f32 	%f133, 0f7FC00000;
	mov.u16 	%rs22, %rs2;
	mov.u16 	%rs23, %rs1;
	mov.f64 	%fd310, %fd13;
	mov.f64 	%fd311, %fd12;
	mov.u32 	%r203, %r28;
	mov.u32 	%r204, %r27;
	mov.u32 	%r205, %r201;
	mov.u32 	%r207, %r202;
	mov.u32 	%r208, %r206;
	mov.f64 	%fd312, %fd309;
	@%p75 bra 	$L__BB4_95;

	add.u64 	%rd71, %SPL, 4096;
	setp.lt.s32 	%p76, %r23, %r88;
	mul.wide.s32 	%rd67, %r25, 4;
	add.s64 	%rd20, %rd71, %rd67;
	@%p76 bra 	$L__BB4_93;
	bra.uni 	$L__BB4_92;

$L__BB4_93:
	st.local.f32 	[%rd20], %f129;
	cvt.ftz.f64.f32 	%fd268, %f129;
	add.f64 	%fd313, %fd10, %fd268;
	add.s32 	%r154, %r25, 1;
	setp.eq.s32 	%p78, %r154, %r88;
	selp.b32 	%r206, 0, %r154, %p78;
	add.s32 	%r208, %r23, 1;
	setp.ne.s32 	%p79, %r208, %r88;
	mov.u16 	%rs22, %rs2;
	mov.u16 	%rs23, %rs1;
	mov.f64 	%fd310, %fd13;
	mov.f64 	%fd311, %fd12;
	mov.u32 	%r203, %r28;
	mov.u32 	%r204, %r27;
	mov.u32 	%r205, %r201;
	mov.u32 	%r207, %r202;
	mov.f64 	%fd312, %fd309;
	@%p79 bra 	$L__BB4_95;

	mul.f64 	%fd269, %fd1, %fd313;
	cvt.rn.ftz.f32.f64 	%f133, %fd269;
	mov.u16 	%rs22, %rs2;
	mov.u16 	%rs23, %rs1;
	mov.f64 	%fd310, %fd13;
	mov.f64 	%fd311, %fd12;
	mov.u32 	%r203, %r28;
	mov.u32 	%r204, %r27;
	mov.u32 	%r205, %r201;
	mov.u32 	%r207, %r202;
	mov.u32 	%r208, %r88;
	mov.f64 	%fd312, %fd309;
	bra.uni 	$L__BB4_95;

$L__BB4_92:
	ld.local.f32 	%f104, [%rd20];
	st.local.f32 	[%rd20], %f129;
	add.s32 	%r153, %r25, 1;
	setp.eq.s32 	%p77, %r153, %r88;
	selp.b32 	%r206, 0, %r153, %p77;
	cvt.ftz.f64.f32 	%fd264, %f104;
	cvt.ftz.f64.f32 	%fd265, %f129;
	sub.f64 	%fd266, %fd265, %fd264;
	add.f64 	%fd313, %fd10, %fd266;
	mul.f64 	%fd267, %fd1, %fd313;
	cvt.rn.ftz.f32.f64 	%f133, %fd267;
	mov.u16 	%rs22, %rs2;
	mov.u16 	%rs23, %rs1;
	mov.f64 	%fd310, %fd13;
	mov.f64 	%fd311, %fd12;
	mov.u32 	%r203, %r28;
	mov.u32 	%r204, %r27;
	mov.u32 	%r205, %r201;
	mov.u32 	%r207, %r202;
	mov.u32 	%r208, %r23;
	mov.f64 	%fd312, %fd309;
	bra.uni 	$L__BB4_95;

$L__BB4_78:
	mov.u16 	%rs22, 0;
	mov.f32 	%f132, 0f7FC00000;
	add.f64 	%fd310, %fd13, %fd65;
	add.s32 	%r203, %r28, 1;
	setp.ne.s32 	%p66, %r203, %r88;
	@%p66 bra 	$L__BB4_80;

	mul.f64 	%fd314, %fd1, %fd310;
	cvt.rn.ftz.f32.f64 	%f132, %fd314;
	mov.u16 	%rs22, 1;
	mov.u32 	%r203, %r88;

$L__BB4_80:
	mov.u16 	%rs23, 0;
	abs.ftz.f32 	%f97, %f129;
	setp.geu.ftz.f32 	%p67, %f97, 0f7F800000;
	mov.f32 	%f133, 0f7FC00000;
	mov.u32 	%r206, %r25;
	mov.u32 	%r208, %r23;
	mov.f64 	%fd313, %fd10;
	@%p67 bra 	$L__BB4_95;

	and.b16  	%rs13, %rs1, 255;
	setp.eq.s16 	%p68, %rs13, 0;
	cvt.ftz.f64.f32 	%fd71, %f129;
	@%p68 bra 	$L__BB4_83;

	mul.f64 	%fd255, %fd2, %fd71;
	fma.rn.f64 	%fd315, %fd3, %fd315, %fd255;
	cvt.rn.ftz.f32.f64 	%f133, %fd315;
	mov.u16 	%rs23, %rs1;
	mov.f64 	%fd311, %fd12;
	mov.u32 	%r204, %r27;
	mov.u32 	%r206, %r25;
	mov.u32 	%r208, %r23;
	mov.f64 	%fd313, %fd10;
	bra.uni 	$L__BB4_95;

$L__BB4_83:
	mov.u16 	%rs23, 0;
	add.f64 	%fd311, %fd12, %fd71;
	add.s32 	%r204, %r27, 1;
	setp.ne.s32 	%p69, %r204, %r88;
	mov.u32 	%r206, %r25;
	mov.u32 	%r208, %r23;
	mov.f64 	%fd313, %fd10;
	@%p69 bra 	$L__BB4_95;

	mul.f64 	%fd315, %fd1, %fd311;
	cvt.rn.ftz.f32.f64 	%f133, %fd315;
	mov.u16 	%rs23, 1;
	mov.u32 	%r204, %r88;
	mov.u32 	%r206, %r25;
	mov.u32 	%r208, %r23;
	mov.f64 	%fd313, %fd10;

$L__BB4_95:
	setp.lt.s32 	%p80, %r179, %r5;
	@%p80 bra 	$L__BB4_102;

	mul.lo.s32 	%r157, %r179, %r85;
	abs.ftz.f32 	%f106, %f132;
	setp.geu.ftz.f32 	%p81, %f106, 0f7F800000;
	add.s32 	%r155, %r157, %r1;
	mul.wide.s32 	%rd68, %r155, 4;
	add.s64 	%rd21, %rd1, %rd68;
	@%p81 bra 	$L__BB4_101;

	abs.ftz.f32 	%f107, %f133;
	setp.geu.ftz.f32 	%p82, %f107, 0f7F800000;
	@%p82 bra 	$L__BB4_101;
	bra.uni 	$L__BB4_98;

$L__BB4_101:
	mov.u32 	%r156, 2143289344;
	st.global.u32 	[%rd21], %r156;
	bra.uni 	$L__BB4_102;

$L__BB4_98:
	cvt.ftz.f64.f32 	%fd270, %f132;
	cvt.ftz.f64.f32 	%fd271, %f133;
	add.f64 	%fd86, %fd270, %fd271;
	abs.f64 	%fd272, %fd86;
	setp.le.f64 	%p83, %fd272, 0d3CD203AF9EE75616;
	mov.f32 	%f134, 0f7FC00000;
	@%p83 bra 	$L__BB4_100;

	cvt.rn.ftz.f32.f64 	%f109, %fd86;
	div.approx.ftz.f32 	%f110, %f132, %f109;
	mul.ftz.f32 	%f134, %f110, 0f42C80000;

$L__BB4_100:
	st.global.f32 	[%rd21], %f134;

$L__BB4_102:
	add.s32 	%r179, %r179, 1;
	ld.param.u32 	%r158, [rvi_many_series_one_param_f32_param_3];
	setp.lt.s32 	%p84, %r179, %r158;
	@%p84 bra 	$L__BB4_11;

$L__BB4_103:
	ret;

}

