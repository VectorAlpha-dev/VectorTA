//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_89
.address_size 64

	// .globl	cwma_batch_f32
// _ZZN32CwmaBatchTiledPrecomputed2xAsyncILi128ELi2EE3runEPKfS2_PKiS2_iiiiPfE3pss has been demoted
// _ZZN32CwmaBatchTiledPrecomputed2xAsyncILi256ELi2EE3runEPKfS2_PKiS2_iiiiPfE3pss has been demoted
.extern .shared .align 16 .b8 shraw[];
.extern .shared .align 16 .b8 shared_weights[];

.visible .entry cwma_batch_f32(
	.param .u64 cwma_batch_f32_param_0,
	.param .u64 cwma_batch_f32_param_1,
	.param .u64 cwma_batch_f32_param_2,
	.param .u64 cwma_batch_f32_param_3,
	.param .u32 cwma_batch_f32_param_4,
	.param .u32 cwma_batch_f32_param_5,
	.param .u32 cwma_batch_f32_param_6,
	.param .u32 cwma_batch_f32_param_7,
	.param .u64 cwma_batch_f32_param_8
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<86>;
	.reg .b64 	%rd<36>;


	ld.param.u64 	%rd17, [cwma_batch_f32_param_0];
	ld.param.u64 	%rd14, [cwma_batch_f32_param_1];
	ld.param.u64 	%rd15, [cwma_batch_f32_param_2];
	ld.param.u64 	%rd16, [cwma_batch_f32_param_3];
	ld.param.u32 	%r37, [cwma_batch_f32_param_4];
	ld.param.u32 	%r38, [cwma_batch_f32_param_5];
	ld.param.u32 	%r40, [cwma_batch_f32_param_6];
	ld.param.u32 	%r39, [cwma_batch_f32_param_7];
	ld.param.u64 	%rd18, [cwma_batch_f32_param_8];
	cvta.to.global.u64 	%rd1, %rd17;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p1, %r1, %r40;
	@%p1 bra 	$L__BB0_38;

	cvta.to.global.u64 	%rd19, %rd15;
	mul.wide.s32 	%rd20, %r1, 4;
	add.s64 	%rd21, %rd19, %rd20;
	ld.global.nc.u32 	%r41, [%rd21];
	setp.gt.s32 	%p2, %r41, 0;
	add.s32 	%r42, %r41, -1;
	selp.b32 	%r2, %r42, 0, %p2;
	cvta.to.global.u64 	%rd22, %rd16;
	add.s64 	%rd23, %rd22, %rd20;
	ld.global.nc.f32 	%f1, [%rd23];
	mov.u32 	%r3, %tid.x;
	setp.lt.s32 	%p3, %r3, %r2;
	@%p3 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_4;

$L__BB0_2:
	mul.lo.s32 	%r4, %r1, %r37;
	mov.u32 	%r5, %ntid.x;
	cvta.to.global.u64 	%rd3, %rd14;
	mov.u32 	%r77, %r3;

$L__BB0_3:
	add.s32 	%r43, %r77, %r4;
	mul.wide.s32 	%rd24, %r43, 4;
	add.s64 	%rd25, %rd3, %rd24;
	ld.global.nc.f32 	%f15, [%rd25];
	shl.b32 	%r44, %r77, 2;
	mov.u32 	%r45, shared_weights;
	add.s32 	%r46, %r45, %r44;
	st.shared.f32 	[%r46], %f15;
	add.s32 	%r77, %r77, %r5;
	setp.lt.s32 	%p4, %r77, %r2;
	@%p4 bra 	$L__BB0_3;

$L__BB0_4:
	mov.u32 	%r47, %ntid.x;
	bar.sync 	0;
	add.s32 	%r8, %r2, %r39;
	mul.lo.s32 	%r9, %r1, %r38;
	mov.u32 	%r48, %ctaid.x;
	mad.lo.s32 	%r80, %r48, %r47, %r3;
	mov.u32 	%r49, %nctaid.x;
	mul.lo.s32 	%r11, %r49, %r47;
	setp.ge.s32 	%p5, %r80, %r38;
	@%p5 bra 	$L__BB0_38;

	setp.gt.s32 	%p6, %r2, 0;
	@%p6 bra 	$L__BB0_26;
	bra.uni 	$L__BB0_6;

$L__BB0_26:
	add.s32 	%r24, %r2, -1;
	and.b32  	%r25, %r2, 3;
	sub.s32 	%r26, %r2, %r25;
	mov.u32 	%r65, 1;
	sub.s32 	%r27, %r65, %r2;

$L__BB0_27:
	add.s32 	%r66, %r80, %r9;
	mul.wide.s32 	%rd31, %r66, 4;
	add.s64 	%rd12, %rd2, %rd31;
	setp.lt.s32 	%p16, %r80, %r8;
	@%p16 bra 	$L__BB0_36;
	bra.uni 	$L__BB0_28;

$L__BB0_36:
	mov.u32 	%r76, 2143289344;
	st.global.u32 	[%rd12], %r76;
	bra.uni 	$L__BB0_37;

$L__BB0_28:
	setp.lt.u32 	%p17, %r24, 3;
	add.s32 	%r29, %r27, %r80;
	mov.f32 	%f83, 0f00000000;
	mov.u32 	%r85, 0;
	mov.f32 	%f85, %f83;
	@%p17 bra 	$L__BB0_31;

	mov.f32 	%f30, 0f00000000;
	mov.u32 	%r85, 0;
	mov.f32 	%f83, %f30;
	mov.f32 	%f85, %f30;
	mov.u32 	%r84, %r26;

$L__BB0_30:
	.pragma "nounroll";
	add.s32 	%r69, %r29, %r85;
	mul.wide.s32 	%rd32, %r69, 4;
	add.s64 	%rd33, %rd1, %rd32;
	shl.b32 	%r70, %r85, 2;
	mov.u32 	%r71, shared_weights;
	add.s32 	%r72, %r71, %r70;
	ld.shared.v4.f32 	{%f31, %f32, %f33, %f34}, [%r72];
	ld.global.nc.f32 	%f39, [%rd33];
	fma.rn.ftz.f32 	%f41, %f39, %f31, %f30;
	sub.ftz.f32 	%f42, %f41, %f83;
	add.ftz.f32 	%f43, %f85, %f42;
	sub.ftz.f32 	%f44, %f43, %f85;
	sub.ftz.f32 	%f45, %f44, %f42;
	ld.global.nc.f32 	%f46, [%rd33+4];
	fma.rn.ftz.f32 	%f47, %f46, %f32, %f30;
	sub.ftz.f32 	%f48, %f47, %f45;
	add.ftz.f32 	%f49, %f43, %f48;
	sub.ftz.f32 	%f50, %f49, %f43;
	sub.ftz.f32 	%f51, %f50, %f48;
	ld.global.nc.f32 	%f52, [%rd33+8];
	fma.rn.ftz.f32 	%f53, %f52, %f33, %f30;
	sub.ftz.f32 	%f54, %f53, %f51;
	add.ftz.f32 	%f55, %f49, %f54;
	sub.ftz.f32 	%f56, %f55, %f49;
	sub.ftz.f32 	%f57, %f56, %f54;
	ld.global.nc.f32 	%f58, [%rd33+12];
	fma.rn.ftz.f32 	%f59, %f58, %f34, %f30;
	sub.ftz.f32 	%f60, %f59, %f57;
	add.ftz.f32 	%f85, %f55, %f60;
	sub.ftz.f32 	%f61, %f85, %f55;
	sub.ftz.f32 	%f83, %f61, %f60;
	add.s32 	%r85, %r85, 4;
	add.s32 	%r84, %r84, -4;
	setp.ne.s32 	%p18, %r84, 0;
	@%p18 bra 	$L__BB0_30;

$L__BB0_31:
	setp.eq.s32 	%p19, %r25, 0;
	@%p19 bra 	$L__BB0_35;

	setp.eq.s32 	%p20, %r25, 1;
	add.s32 	%r73, %r29, %r85;
	mul.wide.s32 	%rd34, %r73, 4;
	add.s64 	%rd13, %rd1, %rd34;
	shl.b32 	%r74, %r85, 2;
	mov.u32 	%r75, shared_weights;
	add.s32 	%r35, %r75, %r74;
	ld.shared.f32 	%f62, [%r35];
	ld.global.nc.f32 	%f63, [%rd13];
	mov.f32 	%f64, 0f00000000;
	fma.rn.ftz.f32 	%f65, %f63, %f62, %f64;
	sub.ftz.f32 	%f66, %f65, %f83;
	add.ftz.f32 	%f9, %f85, %f66;
	sub.ftz.f32 	%f67, %f9, %f85;
	sub.ftz.f32 	%f10, %f67, %f66;
	mov.f32 	%f85, %f9;
	@%p20 bra 	$L__BB0_35;

	setp.eq.s32 	%p21, %r25, 2;
	ld.shared.f32 	%f68, [%r35+4];
	ld.global.nc.f32 	%f69, [%rd13+4];
	fma.rn.ftz.f32 	%f71, %f69, %f68, %f64;
	sub.ftz.f32 	%f72, %f71, %f10;
	add.ftz.f32 	%f85, %f9, %f72;
	sub.ftz.f32 	%f73, %f85, %f9;
	sub.ftz.f32 	%f12, %f73, %f72;
	@%p21 bra 	$L__BB0_35;

	ld.shared.f32 	%f74, [%r35+8];
	ld.global.nc.f32 	%f75, [%rd13+8];
	mov.f32 	%f76, 0f00000000;
	fma.rn.ftz.f32 	%f77, %f75, %f74, %f76;
	sub.ftz.f32 	%f78, %f77, %f12;
	add.ftz.f32 	%f85, %f85, %f78;

$L__BB0_35:
	mul.rn.ftz.f32 	%f79, %f85, %f1;
	st.global.f32 	[%rd12], %f79;

$L__BB0_37:
	add.s32 	%r80, %r80, %r11;
	setp.lt.s32 	%p22, %r80, %r38;
	@%p22 bra 	$L__BB0_27;
	bra.uni 	$L__BB0_38;

$L__BB0_6:
	add.s32 	%r50, %r11, %r38;
	add.s32 	%r51, %r80, %r11;
	not.b32 	%r52, %r51;
	add.s32 	%r53, %r50, %r52;
	div.u32 	%r12, %r53, %r11;
	add.s32 	%r54, %r12, 1;
	and.b32  	%r79, %r54, 3;
	setp.eq.s32 	%p7, %r79, 0;
	@%p7 bra 	$L__BB0_12;

	add.s32 	%r55, %r80, %r9;
	mul.wide.s32 	%rd26, %r55, 4;
	add.s64 	%rd35, %rd2, %rd26;
	mul.wide.s32 	%rd5, %r11, 4;

$L__BB0_8:
	.pragma "nounroll";
	setp.lt.s32 	%p8, %r80, %r8;
	@%p8 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	mov.u32 	%r56, 2143289344;
	st.global.u32 	[%rd35], %r56;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	mov.f32 	%f16, 0f00000000;
	mul.rn.ftz.f32 	%f17, %f16, %f1;
	st.global.f32 	[%rd35], %f17;

$L__BB0_11:
	add.s32 	%r80, %r80, %r11;
	add.s64 	%rd35, %rd35, %rd5;
	add.s32 	%r79, %r79, -1;
	setp.ne.s32 	%p9, %r79, 0;
	@%p9 bra 	$L__BB0_8;

$L__BB0_12:
	setp.lt.u32 	%p10, %r12, 3;
	@%p10 bra 	$L__BB0_38;

$L__BB0_13:
	add.s32 	%r57, %r80, %r9;
	mul.wide.s32 	%rd27, %r57, 4;
	add.s64 	%rd8, %rd2, %rd27;
	setp.lt.s32 	%p11, %r80, %r8;
	@%p11 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_14;

$L__BB0_15:
	mov.u32 	%r58, 2143289344;
	st.global.u32 	[%rd8], %r58;
	bra.uni 	$L__BB0_16;

$L__BB0_14:
	mov.f32 	%f18, 0f00000000;
	mul.rn.ftz.f32 	%f19, %f18, %f1;
	st.global.f32 	[%rd8], %f19;

$L__BB0_16:
	add.s32 	%r20, %r80, %r11;
	add.s32 	%r59, %r20, %r9;
	setp.lt.s32 	%p12, %r20, %r8;
	mul.wide.s32 	%rd28, %r59, 4;
	add.s64 	%rd9, %rd2, %rd28;
	@%p12 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_17;

$L__BB0_18:
	mov.u32 	%r60, 2143289344;
	st.global.u32 	[%rd9], %r60;
	bra.uni 	$L__BB0_19;

$L__BB0_17:
	mov.f32 	%f20, 0f00000000;
	mul.rn.ftz.f32 	%f21, %f20, %f1;
	st.global.f32 	[%rd9], %f21;

$L__BB0_19:
	add.s32 	%r21, %r20, %r11;
	add.s32 	%r61, %r21, %r9;
	setp.lt.s32 	%p13, %r21, %r8;
	mul.wide.s32 	%rd29, %r61, 4;
	add.s64 	%rd10, %rd2, %rd29;
	@%p13 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_20;

$L__BB0_21:
	mov.u32 	%r62, 2143289344;
	st.global.u32 	[%rd10], %r62;
	bra.uni 	$L__BB0_22;

$L__BB0_20:
	mov.f32 	%f22, 0f00000000;
	mul.rn.ftz.f32 	%f23, %f22, %f1;
	st.global.f32 	[%rd10], %f23;

$L__BB0_22:
	add.s32 	%r22, %r21, %r11;
	add.s32 	%r63, %r22, %r9;
	setp.lt.s32 	%p14, %r22, %r8;
	mul.wide.s32 	%rd30, %r63, 4;
	add.s64 	%rd11, %rd2, %rd30;
	@%p14 bra 	$L__BB0_24;
	bra.uni 	$L__BB0_23;

$L__BB0_24:
	mov.u32 	%r64, 2143289344;
	st.global.u32 	[%rd11], %r64;
	bra.uni 	$L__BB0_25;

$L__BB0_23:
	mov.f32 	%f24, 0f00000000;
	mul.rn.ftz.f32 	%f25, %f24, %f1;
	st.global.f32 	[%rd11], %f25;

$L__BB0_25:
	add.s32 	%r80, %r22, %r11;
	setp.lt.s32 	%p15, %r80, %r38;
	@%p15 bra 	$L__BB0_13;

$L__BB0_38:
	ret;

}
	// .globl	cwma_batch_tiled_f32_tile128
.visible .entry cwma_batch_tiled_f32_tile128(
	.param .u64 cwma_batch_tiled_f32_tile128_param_0,
	.param .u64 cwma_batch_tiled_f32_tile128_param_1,
	.param .u64 cwma_batch_tiled_f32_tile128_param_2,
	.param .u64 cwma_batch_tiled_f32_tile128_param_3,
	.param .u32 cwma_batch_tiled_f32_tile128_param_4,
	.param .u32 cwma_batch_tiled_f32_tile128_param_5,
	.param .u32 cwma_batch_tiled_f32_tile128_param_6,
	.param .u32 cwma_batch_tiled_f32_tile128_param_7,
	.param .u64 cwma_batch_tiled_f32_tile128_param_8
)
{
	.reg .pred 	%p<37>;
	.reg .f32 	%f<91>;
	.reg .b32 	%r<139>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd21, [cwma_batch_tiled_f32_tile128_param_0];
	ld.param.u64 	%rd22, [cwma_batch_tiled_f32_tile128_param_1];
	ld.param.u64 	%rd18, [cwma_batch_tiled_f32_tile128_param_2];
	ld.param.u64 	%rd19, [cwma_batch_tiled_f32_tile128_param_3];
	ld.param.u32 	%r63, [cwma_batch_tiled_f32_tile128_param_4];
	ld.param.u32 	%r64, [cwma_batch_tiled_f32_tile128_param_5];
	ld.param.u32 	%r65, [cwma_batch_tiled_f32_tile128_param_6];
	ld.param.u32 	%r66, [cwma_batch_tiled_f32_tile128_param_7];
	ld.param.u64 	%rd20, [cwma_batch_tiled_f32_tile128_param_8];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd21;
	mov.u32 	%r67, %ntid.x;
	setp.ne.s32 	%p1, %r67, 128;
	@%p1 bra 	$L__BB1_38;

	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p2, %r1, %r65;
	@%p2 bra 	$L__BB1_38;

	cvt.s64.s32 	%rd3, %r1;
	cvta.to.global.u64 	%rd23, %rd18;
	mul.wide.s32 	%rd24, %r1, 4;
	add.s64 	%rd25, %rd23, %rd24;
	ld.global.nc.u32 	%r2, [%rd25];
	add.s32 	%r68, %r2, -1;
	setp.gt.s32 	%p3, %r2, 1;
	selp.b32 	%r3, %r68, 0, %p3;
	mov.u32 	%r69, %ctaid.x;
	shl.b32 	%r4, %r69, 7;
	setp.ge.s32 	%p4, %r4, %r64;
	@%p4 bra 	$L__BB1_38;

	add.s32 	%r5, %r3, 127;
	shl.b32 	%r70, %r3, 2;
	add.s32 	%r71, %r70, 15;
	and.b32  	%r6, %r71, -16;
	mul.lo.s32 	%r72, %r1, %r63;
	cvt.s64.s32 	%rd4, %r72;
	mov.u32 	%r7, %tid.x;
	setp.ge.s32 	%p5, %r7, %r3;
	@%p5 bra 	$L__BB1_10;

	not.b32 	%r73, %r7;
	add.s32 	%r8, %r3, %r73;
	shr.u32 	%r74, %r8, 7;
	add.s32 	%r75, %r74, 1;
	and.b32  	%r121, %r75, 3;
	setp.eq.s32 	%p6, %r121, 0;
	mov.u32 	%r122, %r7;
	@%p6 bra 	$L__BB1_7;

	shl.b32 	%r76, %r7, 2;
	mov.u32 	%r77, shraw;
	add.s32 	%r119, %r77, %r76;
	cvt.s64.s32 	%rd26, %r7;
	add.s64 	%rd27, %rd26, %rd4;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd39, %rd1, %rd28;
	mov.u32 	%r122, %r7;

$L__BB1_6:
	.pragma "nounroll";
	ld.global.nc.f32 	%f23, [%rd39];
	st.shared.f32 	[%r119], %f23;
	add.s32 	%r122, %r122, 128;
	add.s32 	%r119, %r119, 512;
	add.s64 	%rd39, %rd39, 512;
	add.s32 	%r121, %r121, -1;
	setp.ne.s32 	%p7, %r121, 0;
	@%p7 bra 	$L__BB1_6;

$L__BB1_7:
	setp.lt.u32 	%p8, %r8, 384;
	@%p8 bra 	$L__BB1_10;

	cvt.s64.s32 	%rd29, %r122;
	add.s64 	%rd30, %rd29, %rd4;
	shl.b64 	%rd31, %rd30, 2;
	add.s64 	%rd40, %rd1, %rd31;
	shl.b32 	%r78, %r122, 2;
	mov.u32 	%r79, shraw;
	add.s32 	%r80, %r79, %r78;
	add.s32 	%r123, %r80, 1024;

$L__BB1_9:
	ld.global.nc.f32 	%f24, [%rd40];
	st.shared.f32 	[%r123+-1024], %f24;
	ld.global.nc.f32 	%f25, [%rd40+512];
	st.shared.f32 	[%r123+-512], %f25;
	ld.global.nc.f32 	%f26, [%rd40+1024];
	st.shared.f32 	[%r123], %f26;
	ld.global.nc.f32 	%f27, [%rd40+1536];
	st.shared.f32 	[%r123+512], %f27;
	add.s64 	%rd40, %rd40, 2048;
	add.s32 	%r123, %r123, 2048;
	add.s32 	%r122, %r122, 512;
	setp.lt.s32 	%p9, %r122, %r3;
	@%p9 bra 	$L__BB1_9;

$L__BB1_10:
	bar.sync 	0;
	setp.ge.s32 	%p10, %r7, %r5;
	@%p10 bra 	$L__BB1_27;

	add.s32 	%r81, %r3, 126;
	sub.s32 	%r23, %r81, %r7;
	shr.u32 	%r82, %r23, 7;
	add.s32 	%r83, %r82, 1;
	and.b32  	%r128, %r83, 3;
	setp.eq.s32 	%p11, %r128, 0;
	mov.u32 	%r129, %r7;
	@%p11 bra 	$L__BB1_16;

	shl.b32 	%r84, %r7, 2;
	add.s32 	%r85, %r6, %r84;
	mov.u32 	%r86, shraw;
	add.s32 	%r126, %r86, %r85;
	add.s32 	%r87, %r7, %r4;
	not.b32 	%r88, %r3;
	add.s32 	%r89, %r87, %r88;
	add.s32 	%r125, %r89, 2;
	mul.wide.s32 	%rd32, %r125, 4;
	add.s64 	%rd41, %rd2, %rd32;
	mov.u32 	%r129, %r7;

$L__BB1_13:
	.pragma "nounroll";
	setp.ge.s32 	%p12, %r125, %r64;
	setp.lt.s32 	%p13, %r125, 0;
	mov.f32 	%f78, 0f00000000;
	or.pred  	%p14, %p13, %p12;
	@%p14 bra 	$L__BB1_15;

	ld.global.nc.f32 	%f78, [%rd41];

$L__BB1_15:
	st.shared.f32 	[%r126], %f78;
	add.s32 	%r129, %r129, 128;
	add.s32 	%r126, %r126, 512;
	add.s64 	%rd41, %rd41, 512;
	add.s32 	%r125, %r125, 128;
	add.s32 	%r128, %r128, -1;
	setp.ne.s32 	%p15, %r128, 0;
	@%p15 bra 	$L__BB1_13;

$L__BB1_16:
	setp.lt.u32 	%p16, %r23, 384;
	@%p16 bra 	$L__BB1_27;

	add.s32 	%r90, %r129, %r4;
	add.s32 	%r91, %r90, 386;
	add.s32 	%r92, %r3, 1;
	sub.s32 	%r130, %r91, %r92;
	add.s32 	%r93, %r90, 2;
	sub.s32 	%r94, %r93, %r92;
	mul.wide.s32 	%rd33, %r94, 4;
	add.s64 	%rd42, %rd2, %rd33;
	shl.b32 	%r95, %r129, 2;
	add.s32 	%r96, %r6, %r95;
	mov.u32 	%r97, shraw;
	add.s32 	%r132, %r97, %r96;

$L__BB1_18:
	add.s32 	%r98, %r130, -384;
	setp.lt.s32 	%p17, %r98, 0;
	setp.ge.s32 	%p18, %r98, %r64;
	mov.f32 	%f80, 0f00000000;
	or.pred  	%p19, %p17, %p18;
	mov.f32 	%f79, %f80;
	@%p19 bra 	$L__BB1_20;

	ld.global.nc.f32 	%f79, [%rd42];

$L__BB1_20:
	st.shared.f32 	[%r132], %f79;
	add.s32 	%r99, %r130, -256;
	setp.lt.s32 	%p20, %r99, 0;
	setp.ge.s32 	%p21, %r99, %r64;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	$L__BB1_22;

	ld.global.nc.f32 	%f80, [%rd42+512];

$L__BB1_22:
	st.shared.f32 	[%r132+512], %f80;
	add.s32 	%r100, %r130, -128;
	setp.lt.s32 	%p23, %r100, 0;
	setp.ge.s32 	%p24, %r100, %r64;
	mov.f32 	%f82, 0f00000000;
	or.pred  	%p25, %p23, %p24;
	mov.f32 	%f81, %f82;
	@%p25 bra 	$L__BB1_24;

	ld.global.nc.f32 	%f81, [%rd42+1024];

$L__BB1_24:
	st.shared.f32 	[%r132+1024], %f81;
	setp.ge.s32 	%p26, %r130, %r64;
	setp.lt.s32 	%p27, %r130, 0;
	or.pred  	%p28, %p27, %p26;
	@%p28 bra 	$L__BB1_26;

	ld.global.nc.f32 	%f82, [%rd42+1536];

$L__BB1_26:
	add.s64 	%rd42, %rd42, 2048;
	add.s32 	%r42, %r132, 2048;
	st.shared.f32 	[%r132+1536], %f82;
	add.s32 	%r130, %r130, 512;
	add.s32 	%r129, %r129, 512;
	setp.lt.s32 	%p29, %r129, %r5;
	mov.u32 	%r132, %r42;
	@%p29 bra 	$L__BB1_18;

$L__BB1_27:
	bar.sync 	0;
	add.s32 	%r45, %r4, %r7;
	setp.ge.s32 	%p30, %r45, %r64;
	@%p30 bra 	$L__BB1_38;

	mad.lo.s32 	%r101, %r1, %r64, %r45;
	cvta.to.global.u64 	%rd34, %rd20;
	mul.wide.s32 	%rd35, %r101, 4;
	add.s64 	%rd17, %rd34, %rd35;
	add.s32 	%r102, %r3, %r66;
	setp.lt.s32 	%p31, %r45, %r102;
	@%p31 bra 	$L__BB1_37;
	bra.uni 	$L__BB1_29;

$L__BB1_37:
	mov.u32 	%r118, 2143289344;
	st.global.u32 	[%rd17], %r118;
	bra.uni 	$L__BB1_38;

$L__BB1_29:
	setp.lt.s32 	%p32, %r2, 2;
	mov.f32 	%f85, 0f00000000;
	@%p32 bra 	$L__BB1_36;

	max.s32 	%r46, %r3, 1;
	add.s32 	%r104, %r46, -1;
	and.b32  	%r138, %r46, 3;
	setp.lt.u32 	%p33, %r104, 3;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r135, 0;
	mov.f32 	%f85, %f86;
	@%p33 bra 	$L__BB1_33;

	sub.s32 	%r134, %r46, %r138;
	mov.u32 	%r106, shraw;
	add.s32 	%r49, %r106, %r6;
	mov.f32 	%f38, 0f00000000;
	mov.u32 	%r135, 0;
	mov.f32 	%f86, %f38;
	mov.f32 	%f85, %f38;

$L__BB1_32:
	.pragma "nounroll";
	add.s32 	%r107, %r135, %r7;
	shl.b32 	%r108, %r107, 2;
	add.s32 	%r109, %r49, %r108;
	shl.b32 	%r110, %r135, 2;
	add.s32 	%r112, %r106, %r110;
	ld.shared.v4.f32 	{%f39, %f40, %f41, %f42}, [%r112];
	ld.shared.f32 	%f47, [%r109];
	fma.rn.ftz.f32 	%f49, %f47, %f39, %f38;
	sub.ftz.f32 	%f50, %f49, %f86;
	add.ftz.f32 	%f51, %f85, %f50;
	sub.ftz.f32 	%f52, %f51, %f85;
	sub.ftz.f32 	%f53, %f52, %f50;
	ld.shared.f32 	%f54, [%r109+4];
	fma.rn.ftz.f32 	%f55, %f54, %f40, %f38;
	sub.ftz.f32 	%f56, %f55, %f53;
	add.ftz.f32 	%f57, %f51, %f56;
	sub.ftz.f32 	%f58, %f57, %f51;
	sub.ftz.f32 	%f59, %f58, %f56;
	ld.shared.f32 	%f60, [%r109+8];
	fma.rn.ftz.f32 	%f61, %f60, %f41, %f38;
	sub.ftz.f32 	%f62, %f61, %f59;
	add.ftz.f32 	%f63, %f57, %f62;
	sub.ftz.f32 	%f64, %f63, %f57;
	sub.ftz.f32 	%f65, %f64, %f62;
	ld.shared.f32 	%f66, [%r109+12];
	fma.rn.ftz.f32 	%f67, %f66, %f42, %f38;
	sub.ftz.f32 	%f68, %f67, %f65;
	add.ftz.f32 	%f85, %f63, %f68;
	sub.ftz.f32 	%f69, %f85, %f63;
	sub.ftz.f32 	%f86, %f69, %f68;
	add.s32 	%r135, %r135, 4;
	add.s32 	%r134, %r134, -4;
	setp.ne.s32 	%p34, %r134, 0;
	@%p34 bra 	$L__BB1_32;

$L__BB1_33:
	setp.eq.s32 	%p35, %r138, 0;
	@%p35 bra 	$L__BB1_36;

	shl.b32 	%r113, %r135, 2;
	mov.u32 	%r114, shraw;
	add.s32 	%r137, %r114, %r113;
	add.s32 	%r115, %r7, %r135;
	shl.b32 	%r116, %r115, 2;
	add.s32 	%r117, %r6, %r116;
	add.s32 	%r136, %r114, %r117;
	mov.f32 	%f89, %f85;

$L__BB1_35:
	.pragma "nounroll";
	ld.shared.f32 	%f70, [%r137];
	ld.shared.f32 	%f71, [%r136];
	mov.f32 	%f72, 0f00000000;
	fma.rn.ftz.f32 	%f73, %f71, %f70, %f72;
	sub.ftz.f32 	%f74, %f73, %f86;
	add.ftz.f32 	%f85, %f89, %f74;
	sub.ftz.f32 	%f75, %f85, %f89;
	sub.ftz.f32 	%f86, %f75, %f74;
	add.s32 	%r137, %r137, 4;
	add.s32 	%r136, %r136, 4;
	add.s32 	%r138, %r138, -1;
	setp.ne.s32 	%p36, %r138, 0;
	mov.f32 	%f89, %f85;
	@%p36 bra 	$L__BB1_35;

$L__BB1_36:
	cvta.to.global.u64 	%rd36, %rd19;
	shl.b64 	%rd37, %rd3, 2;
	add.s64 	%rd38, %rd36, %rd37;
	ld.global.nc.f32 	%f76, [%rd38];
	mul.rn.ftz.f32 	%f77, %f85, %f76;
	st.global.f32 	[%rd17], %f77;

$L__BB1_38:
	ret;

}
	// .globl	cwma_batch_tiled_f32_tile256
.visible .entry cwma_batch_tiled_f32_tile256(
	.param .u64 cwma_batch_tiled_f32_tile256_param_0,
	.param .u64 cwma_batch_tiled_f32_tile256_param_1,
	.param .u64 cwma_batch_tiled_f32_tile256_param_2,
	.param .u64 cwma_batch_tiled_f32_tile256_param_3,
	.param .u32 cwma_batch_tiled_f32_tile256_param_4,
	.param .u32 cwma_batch_tiled_f32_tile256_param_5,
	.param .u32 cwma_batch_tiled_f32_tile256_param_6,
	.param .u32 cwma_batch_tiled_f32_tile256_param_7,
	.param .u64 cwma_batch_tiled_f32_tile256_param_8
)
{
	.reg .pred 	%p<37>;
	.reg .f32 	%f<91>;
	.reg .b32 	%r<139>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd21, [cwma_batch_tiled_f32_tile256_param_0];
	ld.param.u64 	%rd22, [cwma_batch_tiled_f32_tile256_param_1];
	ld.param.u64 	%rd18, [cwma_batch_tiled_f32_tile256_param_2];
	ld.param.u64 	%rd19, [cwma_batch_tiled_f32_tile256_param_3];
	ld.param.u32 	%r63, [cwma_batch_tiled_f32_tile256_param_4];
	ld.param.u32 	%r64, [cwma_batch_tiled_f32_tile256_param_5];
	ld.param.u32 	%r65, [cwma_batch_tiled_f32_tile256_param_6];
	ld.param.u32 	%r66, [cwma_batch_tiled_f32_tile256_param_7];
	ld.param.u64 	%rd20, [cwma_batch_tiled_f32_tile256_param_8];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd21;
	mov.u32 	%r67, %ntid.x;
	setp.ne.s32 	%p1, %r67, 256;
	@%p1 bra 	$L__BB2_38;

	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p2, %r1, %r65;
	@%p2 bra 	$L__BB2_38;

	cvt.s64.s32 	%rd3, %r1;
	cvta.to.global.u64 	%rd23, %rd18;
	mul.wide.s32 	%rd24, %r1, 4;
	add.s64 	%rd25, %rd23, %rd24;
	ld.global.nc.u32 	%r2, [%rd25];
	add.s32 	%r68, %r2, -1;
	setp.gt.s32 	%p3, %r2, 1;
	selp.b32 	%r3, %r68, 0, %p3;
	mov.u32 	%r69, %ctaid.x;
	shl.b32 	%r4, %r69, 8;
	setp.ge.s32 	%p4, %r4, %r64;
	@%p4 bra 	$L__BB2_38;

	add.s32 	%r5, %r3, 255;
	shl.b32 	%r70, %r3, 2;
	add.s32 	%r71, %r70, 15;
	and.b32  	%r6, %r71, -16;
	mul.lo.s32 	%r72, %r1, %r63;
	cvt.s64.s32 	%rd4, %r72;
	mov.u32 	%r7, %tid.x;
	setp.ge.s32 	%p5, %r7, %r3;
	@%p5 bra 	$L__BB2_10;

	not.b32 	%r73, %r7;
	add.s32 	%r8, %r3, %r73;
	shr.u32 	%r74, %r8, 8;
	add.s32 	%r75, %r74, 1;
	and.b32  	%r121, %r75, 3;
	setp.eq.s32 	%p6, %r121, 0;
	mov.u32 	%r122, %r7;
	@%p6 bra 	$L__BB2_7;

	shl.b32 	%r76, %r7, 2;
	mov.u32 	%r77, shraw;
	add.s32 	%r119, %r77, %r76;
	cvt.s64.s32 	%rd26, %r7;
	add.s64 	%rd27, %rd26, %rd4;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd39, %rd1, %rd28;
	mov.u32 	%r122, %r7;

$L__BB2_6:
	.pragma "nounroll";
	ld.global.nc.f32 	%f23, [%rd39];
	st.shared.f32 	[%r119], %f23;
	add.s32 	%r122, %r122, 256;
	add.s32 	%r119, %r119, 1024;
	add.s64 	%rd39, %rd39, 1024;
	add.s32 	%r121, %r121, -1;
	setp.ne.s32 	%p7, %r121, 0;
	@%p7 bra 	$L__BB2_6;

$L__BB2_7:
	setp.lt.u32 	%p8, %r8, 768;
	@%p8 bra 	$L__BB2_10;

	cvt.s64.s32 	%rd29, %r122;
	add.s64 	%rd30, %rd29, %rd4;
	shl.b64 	%rd31, %rd30, 2;
	add.s64 	%rd40, %rd1, %rd31;
	shl.b32 	%r78, %r122, 2;
	mov.u32 	%r79, shraw;
	add.s32 	%r80, %r79, %r78;
	add.s32 	%r123, %r80, 2048;

$L__BB2_9:
	ld.global.nc.f32 	%f24, [%rd40];
	st.shared.f32 	[%r123+-2048], %f24;
	ld.global.nc.f32 	%f25, [%rd40+1024];
	st.shared.f32 	[%r123+-1024], %f25;
	ld.global.nc.f32 	%f26, [%rd40+2048];
	st.shared.f32 	[%r123], %f26;
	ld.global.nc.f32 	%f27, [%rd40+3072];
	st.shared.f32 	[%r123+1024], %f27;
	add.s64 	%rd40, %rd40, 4096;
	add.s32 	%r123, %r123, 4096;
	add.s32 	%r122, %r122, 1024;
	setp.lt.s32 	%p9, %r122, %r3;
	@%p9 bra 	$L__BB2_9;

$L__BB2_10:
	bar.sync 	0;
	setp.ge.s32 	%p10, %r7, %r5;
	@%p10 bra 	$L__BB2_27;

	add.s32 	%r81, %r3, 254;
	sub.s32 	%r23, %r81, %r7;
	shr.u32 	%r82, %r23, 8;
	add.s32 	%r83, %r82, 1;
	and.b32  	%r128, %r83, 3;
	setp.eq.s32 	%p11, %r128, 0;
	mov.u32 	%r129, %r7;
	@%p11 bra 	$L__BB2_16;

	shl.b32 	%r84, %r7, 2;
	add.s32 	%r85, %r6, %r84;
	mov.u32 	%r86, shraw;
	add.s32 	%r126, %r86, %r85;
	add.s32 	%r87, %r7, %r4;
	not.b32 	%r88, %r3;
	add.s32 	%r89, %r87, %r88;
	add.s32 	%r125, %r89, 2;
	mul.wide.s32 	%rd32, %r125, 4;
	add.s64 	%rd41, %rd2, %rd32;
	mov.u32 	%r129, %r7;

$L__BB2_13:
	.pragma "nounroll";
	setp.ge.s32 	%p12, %r125, %r64;
	setp.lt.s32 	%p13, %r125, 0;
	mov.f32 	%f78, 0f00000000;
	or.pred  	%p14, %p13, %p12;
	@%p14 bra 	$L__BB2_15;

	ld.global.nc.f32 	%f78, [%rd41];

$L__BB2_15:
	st.shared.f32 	[%r126], %f78;
	add.s32 	%r129, %r129, 256;
	add.s32 	%r126, %r126, 1024;
	add.s64 	%rd41, %rd41, 1024;
	add.s32 	%r125, %r125, 256;
	add.s32 	%r128, %r128, -1;
	setp.ne.s32 	%p15, %r128, 0;
	@%p15 bra 	$L__BB2_13;

$L__BB2_16:
	setp.lt.u32 	%p16, %r23, 768;
	@%p16 bra 	$L__BB2_27;

	add.s32 	%r90, %r129, %r4;
	add.s32 	%r91, %r90, 770;
	add.s32 	%r92, %r3, 1;
	sub.s32 	%r130, %r91, %r92;
	add.s32 	%r93, %r90, 2;
	sub.s32 	%r94, %r93, %r92;
	mul.wide.s32 	%rd33, %r94, 4;
	add.s64 	%rd42, %rd2, %rd33;
	shl.b32 	%r95, %r129, 2;
	add.s32 	%r96, %r6, %r95;
	mov.u32 	%r97, shraw;
	add.s32 	%r132, %r97, %r96;

$L__BB2_18:
	add.s32 	%r98, %r130, -768;
	setp.lt.s32 	%p17, %r98, 0;
	setp.ge.s32 	%p18, %r98, %r64;
	mov.f32 	%f80, 0f00000000;
	or.pred  	%p19, %p17, %p18;
	mov.f32 	%f79, %f80;
	@%p19 bra 	$L__BB2_20;

	ld.global.nc.f32 	%f79, [%rd42];

$L__BB2_20:
	st.shared.f32 	[%r132], %f79;
	add.s32 	%r99, %r130, -512;
	setp.lt.s32 	%p20, %r99, 0;
	setp.ge.s32 	%p21, %r99, %r64;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	$L__BB2_22;

	ld.global.nc.f32 	%f80, [%rd42+1024];

$L__BB2_22:
	st.shared.f32 	[%r132+1024], %f80;
	add.s32 	%r100, %r130, -256;
	setp.lt.s32 	%p23, %r100, 0;
	setp.ge.s32 	%p24, %r100, %r64;
	mov.f32 	%f82, 0f00000000;
	or.pred  	%p25, %p23, %p24;
	mov.f32 	%f81, %f82;
	@%p25 bra 	$L__BB2_24;

	ld.global.nc.f32 	%f81, [%rd42+2048];

$L__BB2_24:
	st.shared.f32 	[%r132+2048], %f81;
	setp.ge.s32 	%p26, %r130, %r64;
	setp.lt.s32 	%p27, %r130, 0;
	or.pred  	%p28, %p27, %p26;
	@%p28 bra 	$L__BB2_26;

	ld.global.nc.f32 	%f82, [%rd42+3072];

$L__BB2_26:
	add.s64 	%rd42, %rd42, 4096;
	add.s32 	%r42, %r132, 4096;
	st.shared.f32 	[%r132+3072], %f82;
	add.s32 	%r130, %r130, 1024;
	add.s32 	%r129, %r129, 1024;
	setp.lt.s32 	%p29, %r129, %r5;
	mov.u32 	%r132, %r42;
	@%p29 bra 	$L__BB2_18;

$L__BB2_27:
	bar.sync 	0;
	add.s32 	%r45, %r4, %r7;
	setp.ge.s32 	%p30, %r45, %r64;
	@%p30 bra 	$L__BB2_38;

	mad.lo.s32 	%r101, %r1, %r64, %r45;
	cvta.to.global.u64 	%rd34, %rd20;
	mul.wide.s32 	%rd35, %r101, 4;
	add.s64 	%rd17, %rd34, %rd35;
	add.s32 	%r102, %r3, %r66;
	setp.lt.s32 	%p31, %r45, %r102;
	@%p31 bra 	$L__BB2_37;
	bra.uni 	$L__BB2_29;

$L__BB2_37:
	mov.u32 	%r118, 2143289344;
	st.global.u32 	[%rd17], %r118;
	bra.uni 	$L__BB2_38;

$L__BB2_29:
	setp.lt.s32 	%p32, %r2, 2;
	mov.f32 	%f85, 0f00000000;
	@%p32 bra 	$L__BB2_36;

	max.s32 	%r46, %r3, 1;
	add.s32 	%r104, %r46, -1;
	and.b32  	%r138, %r46, 3;
	setp.lt.u32 	%p33, %r104, 3;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r135, 0;
	mov.f32 	%f85, %f86;
	@%p33 bra 	$L__BB2_33;

	sub.s32 	%r134, %r46, %r138;
	mov.u32 	%r106, shraw;
	add.s32 	%r49, %r106, %r6;
	mov.f32 	%f38, 0f00000000;
	mov.u32 	%r135, 0;
	mov.f32 	%f86, %f38;
	mov.f32 	%f85, %f38;

$L__BB2_32:
	.pragma "nounroll";
	add.s32 	%r107, %r135, %r7;
	shl.b32 	%r108, %r107, 2;
	add.s32 	%r109, %r49, %r108;
	shl.b32 	%r110, %r135, 2;
	add.s32 	%r112, %r106, %r110;
	ld.shared.v4.f32 	{%f39, %f40, %f41, %f42}, [%r112];
	ld.shared.f32 	%f47, [%r109];
	fma.rn.ftz.f32 	%f49, %f47, %f39, %f38;
	sub.ftz.f32 	%f50, %f49, %f86;
	add.ftz.f32 	%f51, %f85, %f50;
	sub.ftz.f32 	%f52, %f51, %f85;
	sub.ftz.f32 	%f53, %f52, %f50;
	ld.shared.f32 	%f54, [%r109+4];
	fma.rn.ftz.f32 	%f55, %f54, %f40, %f38;
	sub.ftz.f32 	%f56, %f55, %f53;
	add.ftz.f32 	%f57, %f51, %f56;
	sub.ftz.f32 	%f58, %f57, %f51;
	sub.ftz.f32 	%f59, %f58, %f56;
	ld.shared.f32 	%f60, [%r109+8];
	fma.rn.ftz.f32 	%f61, %f60, %f41, %f38;
	sub.ftz.f32 	%f62, %f61, %f59;
	add.ftz.f32 	%f63, %f57, %f62;
	sub.ftz.f32 	%f64, %f63, %f57;
	sub.ftz.f32 	%f65, %f64, %f62;
	ld.shared.f32 	%f66, [%r109+12];
	fma.rn.ftz.f32 	%f67, %f66, %f42, %f38;
	sub.ftz.f32 	%f68, %f67, %f65;
	add.ftz.f32 	%f85, %f63, %f68;
	sub.ftz.f32 	%f69, %f85, %f63;
	sub.ftz.f32 	%f86, %f69, %f68;
	add.s32 	%r135, %r135, 4;
	add.s32 	%r134, %r134, -4;
	setp.ne.s32 	%p34, %r134, 0;
	@%p34 bra 	$L__BB2_32;

$L__BB2_33:
	setp.eq.s32 	%p35, %r138, 0;
	@%p35 bra 	$L__BB2_36;

	shl.b32 	%r113, %r135, 2;
	mov.u32 	%r114, shraw;
	add.s32 	%r137, %r114, %r113;
	add.s32 	%r115, %r7, %r135;
	shl.b32 	%r116, %r115, 2;
	add.s32 	%r117, %r6, %r116;
	add.s32 	%r136, %r114, %r117;
	mov.f32 	%f89, %f85;

$L__BB2_35:
	.pragma "nounroll";
	ld.shared.f32 	%f70, [%r137];
	ld.shared.f32 	%f71, [%r136];
	mov.f32 	%f72, 0f00000000;
	fma.rn.ftz.f32 	%f73, %f71, %f70, %f72;
	sub.ftz.f32 	%f74, %f73, %f86;
	add.ftz.f32 	%f85, %f89, %f74;
	sub.ftz.f32 	%f75, %f85, %f89;
	sub.ftz.f32 	%f86, %f75, %f74;
	add.s32 	%r137, %r137, 4;
	add.s32 	%r136, %r136, 4;
	add.s32 	%r138, %r138, -1;
	setp.ne.s32 	%p36, %r138, 0;
	mov.f32 	%f89, %f85;
	@%p36 bra 	$L__BB2_35;

$L__BB2_36:
	cvta.to.global.u64 	%rd36, %rd19;
	shl.b64 	%rd37, %rd3, 2;
	add.s64 	%rd38, %rd36, %rd37;
	ld.global.nc.f32 	%f76, [%rd38];
	mul.rn.ftz.f32 	%f77, %f85, %f76;
	st.global.f32 	[%rd17], %f77;

$L__BB2_38:
	ret;

}
	// .globl	cwma_batch_tiled_f32_2x_tile128
.visible .entry cwma_batch_tiled_f32_2x_tile128(
	.param .u64 cwma_batch_tiled_f32_2x_tile128_param_0,
	.param .u64 cwma_batch_tiled_f32_2x_tile128_param_1,
	.param .u64 cwma_batch_tiled_f32_2x_tile128_param_2,
	.param .u64 cwma_batch_tiled_f32_2x_tile128_param_3,
	.param .u32 cwma_batch_tiled_f32_2x_tile128_param_4,
	.param .u32 cwma_batch_tiled_f32_2x_tile128_param_5,
	.param .u32 cwma_batch_tiled_f32_2x_tile128_param_6,
	.param .u32 cwma_batch_tiled_f32_2x_tile128_param_7,
	.param .u64 cwma_batch_tiled_f32_2x_tile128_param_8
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<134>;
	.reg .b32 	%r<77>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd7, [cwma_batch_tiled_f32_2x_tile128_param_0];
	ld.param.u64 	%rd8, [cwma_batch_tiled_f32_2x_tile128_param_1];
	ld.param.u64 	%rd9, [cwma_batch_tiled_f32_2x_tile128_param_2];
	ld.param.u64 	%rd10, [cwma_batch_tiled_f32_2x_tile128_param_3];
	ld.param.u32 	%r37, [cwma_batch_tiled_f32_2x_tile128_param_4];
	ld.param.u32 	%r38, [cwma_batch_tiled_f32_2x_tile128_param_5];
	ld.param.u32 	%r39, [cwma_batch_tiled_f32_2x_tile128_param_6];
	ld.param.u32 	%r40, [cwma_batch_tiled_f32_2x_tile128_param_7];
	ld.param.u64 	%rd11, [cwma_batch_tiled_f32_2x_tile128_param_8];
	mov.u32 	%r1, %ntid.x;
	and.b32  	%r41, %r1, 2147483647;
	setp.ne.s32 	%p1, %r41, 64;
	@%p1 bra 	$L__BB3_25;

	mov.u32 	%r2, %ctaid.y;
	setp.ge.s32 	%p2, %r2, %r39;
	@%p2 bra 	$L__BB3_25;

	cvt.s64.s32 	%rd1, %r2;
	cvta.to.global.u64 	%rd12, %rd9;
	mul.wide.s32 	%rd13, %r2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.nc.u32 	%r3, [%rd14];
	add.s32 	%r42, %r3, -1;
	setp.gt.s32 	%p3, %r3, 1;
	selp.b32 	%r4, %r42, 0, %p3;
	mov.u32 	%r43, %ctaid.x;
	shl.b32 	%r5, %r43, 7;
	setp.ge.s32 	%p4, %r5, %r38;
	@%p4 bra 	$L__BB3_25;

	add.s32 	%r6, %r4, 127;
	shl.b32 	%r44, %r4, 2;
	add.s32 	%r45, %r44, 15;
	and.b32  	%r7, %r45, -16;
	mov.u32 	%r8, %tid.x;
	setp.ge.s32 	%p5, %r8, %r4;
	@%p5 bra 	$L__BB3_6;

	cvta.to.global.u64 	%rd2, %rd8;
	mul.lo.s32 	%r46, %r2, %r37;
	cvt.s64.s32 	%rd3, %r46;
	mov.u32 	%r69, %r8;

$L__BB3_5:
	cvt.s64.s32 	%rd15, %r69;
	add.s64 	%rd16, %rd15, %rd3;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.nc.f32 	%f31, [%rd18];
	shl.b32 	%r47, %r69, 2;
	mov.u32 	%r48, shraw;
	add.s32 	%r49, %r48, %r47;
	st.shared.f32 	[%r49], %f31;
	add.s32 	%r69, %r69, %r1;
	setp.lt.s32 	%p6, %r69, %r4;
	@%p6 bra 	$L__BB3_5;

$L__BB3_6:
	mov.u32 	%r50, shraw;
	add.s32 	%r11, %r50, %r7;
	bar.sync 	0;
	add.s32 	%r12, %r4, %r40;
	setp.ge.s32 	%p7, %r8, %r6;
	@%p7 bra 	$L__BB3_11;

	add.s32 	%r51, %r5, 1;
	sub.s32 	%r13, %r51, %r4;
	cvta.to.global.u64 	%rd4, %rd7;
	mov.u32 	%r70, %r8;

$L__BB3_8:
	add.s32 	%r15, %r13, %r70;
	setp.lt.s32 	%p8, %r15, 0;
	setp.ge.s32 	%p9, %r15, %r38;
	mov.f32 	%f115, 0f00000000;
	or.pred  	%p10, %p8, %p9;
	@%p10 bra 	$L__BB3_10;

	mul.wide.s32 	%rd19, %r15, 4;
	add.s64 	%rd20, %rd4, %rd19;
	ld.global.nc.f32 	%f115, [%rd20];

$L__BB3_10:
	shl.b32 	%r52, %r70, 2;
	add.s32 	%r53, %r11, %r52;
	st.shared.f32 	[%r53], %f115;
	add.s32 	%r70, %r70, %r1;
	setp.lt.s32 	%p11, %r70, %r6;
	@%p11 bra 	$L__BB3_8;

$L__BB3_11:
	bar.sync 	0;
	shl.b32 	%r17, %r8, 1;
	add.s32 	%r18, %r5, %r17;
	add.s32 	%r19, %r18, 1;
	setp.ge.s32 	%p12, %r18, %r38;
	@%p12 bra 	$L__BB3_25;

	setp.lt.s32 	%p13, %r3, 2;
	mov.f32 	%f120, 0f00000000;
	mov.f32 	%f121, %f120;
	@%p13 bra 	$L__BB3_19;

	max.s32 	%r20, %r4, 1;
	add.s32 	%r55, %r20, -1;
	and.b32  	%r76, %r20, 3;
	setp.lt.u32 	%p14, %r55, 3;
	mov.f32 	%f122, 0f00000000;
	mov.u32 	%r73, 0;
	mov.f32 	%f121, %f122;
	mov.f32 	%f124, %f122;
	mov.f32 	%f120, %f122;
	@%p14 bra 	$L__BB3_16;

	sub.s32 	%r72, %r20, %r76;
	mov.f32 	%f43, 0f00000000;
	mov.u32 	%r73, 0;
	mov.f32 	%f122, %f43;
	mov.f32 	%f121, %f43;
	mov.f32 	%f124, %f43;
	mov.f32 	%f120, %f43;

$L__BB3_15:
	.pragma "nounroll";
	shl.b32 	%r57, %r73, 2;
	add.s32 	%r59, %r50, %r57;
	ld.shared.v4.f32 	{%f44, %f45, %f46, %f47}, [%r59];
	add.s32 	%r60, %r73, %r17;
	shl.b32 	%r61, %r60, 2;
	add.s32 	%r62, %r11, %r61;
	ld.shared.v2.f32 	{%f52, %f53}, [%r62];
	fma.rn.ftz.f32 	%f57, %f52, %f44, %f43;
	sub.ftz.f32 	%f58, %f57, %f124;
	add.ftz.f32 	%f59, %f120, %f58;
	sub.ftz.f32 	%f60, %f59, %f120;
	sub.ftz.f32 	%f61, %f60, %f58;
	fma.rn.ftz.f32 	%f62, %f53, %f44, %f43;
	sub.ftz.f32 	%f63, %f62, %f122;
	add.ftz.f32 	%f64, %f121, %f63;
	sub.ftz.f32 	%f65, %f64, %f121;
	sub.ftz.f32 	%f66, %f65, %f63;
	ld.shared.f32 	%f67, [%r62+4];
	fma.rn.ftz.f32 	%f68, %f67, %f45, %f43;
	sub.ftz.f32 	%f69, %f68, %f61;
	add.ftz.f32 	%f70, %f59, %f69;
	sub.ftz.f32 	%f71, %f70, %f59;
	sub.ftz.f32 	%f72, %f71, %f69;
	ld.shared.v2.f32 	{%f73, %f74}, [%r62+8];
	fma.rn.ftz.f32 	%f77, %f73, %f45, %f43;
	sub.ftz.f32 	%f78, %f77, %f66;
	add.ftz.f32 	%f79, %f64, %f78;
	sub.ftz.f32 	%f80, %f79, %f64;
	sub.ftz.f32 	%f81, %f80, %f78;
	ld.shared.f32 	%f82, [%r62+8];
	fma.rn.ftz.f32 	%f83, %f82, %f46, %f43;
	sub.ftz.f32 	%f84, %f83, %f72;
	add.ftz.f32 	%f85, %f70, %f84;
	sub.ftz.f32 	%f86, %f85, %f70;
	sub.ftz.f32 	%f87, %f86, %f84;
	fma.rn.ftz.f32 	%f88, %f74, %f46, %f43;
	sub.ftz.f32 	%f89, %f88, %f81;
	add.ftz.f32 	%f90, %f79, %f89;
	sub.ftz.f32 	%f91, %f90, %f79;
	sub.ftz.f32 	%f92, %f91, %f89;
	ld.shared.f32 	%f93, [%r62+12];
	fma.rn.ftz.f32 	%f94, %f93, %f47, %f43;
	sub.ftz.f32 	%f95, %f94, %f87;
	add.ftz.f32 	%f120, %f85, %f95;
	sub.ftz.f32 	%f96, %f120, %f85;
	sub.ftz.f32 	%f124, %f96, %f95;
	ld.shared.f32 	%f97, [%r62+16];
	fma.rn.ftz.f32 	%f98, %f97, %f47, %f43;
	sub.ftz.f32 	%f99, %f98, %f92;
	add.ftz.f32 	%f121, %f90, %f99;
	sub.ftz.f32 	%f100, %f121, %f90;
	sub.ftz.f32 	%f122, %f100, %f99;
	add.s32 	%r73, %r73, 4;
	add.s32 	%r72, %r72, -4;
	setp.ne.s32 	%p15, %r72, 0;
	@%p15 bra 	$L__BB3_15;

$L__BB3_16:
	setp.eq.s32 	%p16, %r76, 0;
	@%p16 bra 	$L__BB3_19;

	shl.b32 	%r63, %r8, 3;
	add.s32 	%r64, %r7, %r63;
	shl.b32 	%r65, %r73, 2;
	add.s32 	%r66, %r64, %r65;
	add.s32 	%r68, %r50, %r66;
	add.s32 	%r75, %r68, 4;
	add.s32 	%r74, %r50, %r65;
	mov.f32 	%f127, %f121;
	mov.f32 	%f129, %f120;

$L__BB3_18:
	.pragma "nounroll";
	ld.shared.f32 	%f101, [%r75+-4];
	ld.shared.f32 	%f102, [%r74];
	mov.f32 	%f103, 0f00000000;
	fma.rn.ftz.f32 	%f104, %f101, %f102, %f103;
	sub.ftz.f32 	%f105, %f104, %f124;
	add.ftz.f32 	%f120, %f129, %f105;
	sub.ftz.f32 	%f106, %f120, %f129;
	sub.ftz.f32 	%f124, %f106, %f105;
	ld.shared.f32 	%f107, [%r75];
	fma.rn.ftz.f32 	%f108, %f107, %f102, %f103;
	sub.ftz.f32 	%f109, %f108, %f122;
	add.ftz.f32 	%f121, %f127, %f109;
	sub.ftz.f32 	%f110, %f121, %f127;
	sub.ftz.f32 	%f122, %f110, %f109;
	add.s32 	%r75, %r75, 4;
	add.s32 	%r74, %r74, 4;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p17, %r76, 0;
	mov.f32 	%f127, %f121;
	mov.f32 	%f129, %f120;
	@%p17 bra 	$L__BB3_18;

$L__BB3_19:
	cvta.to.global.u64 	%rd21, %rd10;
	shl.b64 	%rd22, %rd1, 2;
	add.s64 	%rd5, %rd21, %rd22;
	setp.lt.s32 	%p18, %r18, %r12;
	mad.lo.s32 	%r36, %r2, %r38, %r18;
	mov.f32 	%f133, 0f7FC00000;
	mov.f32 	%f132, %f133;
	@%p18 bra 	$L__BB3_21;

	ld.global.nc.f32 	%f112, [%rd5];
	mul.rn.ftz.f32 	%f132, %f120, %f112;

$L__BB3_21:
	setp.lt.s32 	%p19, %r19, %r12;
	setp.ge.s32 	%p20, %r19, %r38;
	or.pred  	%p21, %p20, %p19;
	@%p21 bra 	$L__BB3_23;

	ld.global.nc.f32 	%f114, [%rd5];
	mul.rn.ftz.f32 	%f133, %f121, %f114;

$L__BB3_23:
	cvta.to.global.u64 	%rd23, %rd11;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd6, %rd23, %rd24;
	st.global.f32 	[%rd6], %f132;
	@%p20 bra 	$L__BB3_25;

	st.global.f32 	[%rd6+4], %f133;

$L__BB3_25:
	ret;

}
	// .globl	cwma_batch_tiled_f32_2x_tile256
.visible .entry cwma_batch_tiled_f32_2x_tile256(
	.param .u64 cwma_batch_tiled_f32_2x_tile256_param_0,
	.param .u64 cwma_batch_tiled_f32_2x_tile256_param_1,
	.param .u64 cwma_batch_tiled_f32_2x_tile256_param_2,
	.param .u64 cwma_batch_tiled_f32_2x_tile256_param_3,
	.param .u32 cwma_batch_tiled_f32_2x_tile256_param_4,
	.param .u32 cwma_batch_tiled_f32_2x_tile256_param_5,
	.param .u32 cwma_batch_tiled_f32_2x_tile256_param_6,
	.param .u32 cwma_batch_tiled_f32_2x_tile256_param_7,
	.param .u64 cwma_batch_tiled_f32_2x_tile256_param_8
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<134>;
	.reg .b32 	%r<77>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd7, [cwma_batch_tiled_f32_2x_tile256_param_0];
	ld.param.u64 	%rd8, [cwma_batch_tiled_f32_2x_tile256_param_1];
	ld.param.u64 	%rd9, [cwma_batch_tiled_f32_2x_tile256_param_2];
	ld.param.u64 	%rd10, [cwma_batch_tiled_f32_2x_tile256_param_3];
	ld.param.u32 	%r37, [cwma_batch_tiled_f32_2x_tile256_param_4];
	ld.param.u32 	%r38, [cwma_batch_tiled_f32_2x_tile256_param_5];
	ld.param.u32 	%r39, [cwma_batch_tiled_f32_2x_tile256_param_6];
	ld.param.u32 	%r40, [cwma_batch_tiled_f32_2x_tile256_param_7];
	ld.param.u64 	%rd11, [cwma_batch_tiled_f32_2x_tile256_param_8];
	mov.u32 	%r1, %ntid.x;
	and.b32  	%r41, %r1, 2147483647;
	setp.ne.s32 	%p1, %r41, 128;
	@%p1 bra 	$L__BB4_25;

	mov.u32 	%r2, %ctaid.y;
	setp.ge.s32 	%p2, %r2, %r39;
	@%p2 bra 	$L__BB4_25;

	cvt.s64.s32 	%rd1, %r2;
	cvta.to.global.u64 	%rd12, %rd9;
	mul.wide.s32 	%rd13, %r2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.nc.u32 	%r3, [%rd14];
	add.s32 	%r42, %r3, -1;
	setp.gt.s32 	%p3, %r3, 1;
	selp.b32 	%r4, %r42, 0, %p3;
	mov.u32 	%r43, %ctaid.x;
	shl.b32 	%r5, %r43, 8;
	setp.ge.s32 	%p4, %r5, %r38;
	@%p4 bra 	$L__BB4_25;

	add.s32 	%r6, %r4, 255;
	shl.b32 	%r44, %r4, 2;
	add.s32 	%r45, %r44, 15;
	and.b32  	%r7, %r45, -16;
	mov.u32 	%r8, %tid.x;
	setp.ge.s32 	%p5, %r8, %r4;
	@%p5 bra 	$L__BB4_6;

	cvta.to.global.u64 	%rd2, %rd8;
	mul.lo.s32 	%r46, %r2, %r37;
	cvt.s64.s32 	%rd3, %r46;
	mov.u32 	%r69, %r8;

$L__BB4_5:
	cvt.s64.s32 	%rd15, %r69;
	add.s64 	%rd16, %rd15, %rd3;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.nc.f32 	%f31, [%rd18];
	shl.b32 	%r47, %r69, 2;
	mov.u32 	%r48, shraw;
	add.s32 	%r49, %r48, %r47;
	st.shared.f32 	[%r49], %f31;
	add.s32 	%r69, %r69, %r1;
	setp.lt.s32 	%p6, %r69, %r4;
	@%p6 bra 	$L__BB4_5;

$L__BB4_6:
	mov.u32 	%r50, shraw;
	add.s32 	%r11, %r50, %r7;
	bar.sync 	0;
	add.s32 	%r12, %r4, %r40;
	setp.ge.s32 	%p7, %r8, %r6;
	@%p7 bra 	$L__BB4_11;

	add.s32 	%r51, %r5, 1;
	sub.s32 	%r13, %r51, %r4;
	cvta.to.global.u64 	%rd4, %rd7;
	mov.u32 	%r70, %r8;

$L__BB4_8:
	add.s32 	%r15, %r13, %r70;
	setp.lt.s32 	%p8, %r15, 0;
	setp.ge.s32 	%p9, %r15, %r38;
	mov.f32 	%f115, 0f00000000;
	or.pred  	%p10, %p8, %p9;
	@%p10 bra 	$L__BB4_10;

	mul.wide.s32 	%rd19, %r15, 4;
	add.s64 	%rd20, %rd4, %rd19;
	ld.global.nc.f32 	%f115, [%rd20];

$L__BB4_10:
	shl.b32 	%r52, %r70, 2;
	add.s32 	%r53, %r11, %r52;
	st.shared.f32 	[%r53], %f115;
	add.s32 	%r70, %r70, %r1;
	setp.lt.s32 	%p11, %r70, %r6;
	@%p11 bra 	$L__BB4_8;

$L__BB4_11:
	bar.sync 	0;
	shl.b32 	%r17, %r8, 1;
	add.s32 	%r18, %r5, %r17;
	add.s32 	%r19, %r18, 1;
	setp.ge.s32 	%p12, %r18, %r38;
	@%p12 bra 	$L__BB4_25;

	setp.lt.s32 	%p13, %r3, 2;
	mov.f32 	%f120, 0f00000000;
	mov.f32 	%f121, %f120;
	@%p13 bra 	$L__BB4_19;

	max.s32 	%r20, %r4, 1;
	add.s32 	%r55, %r20, -1;
	and.b32  	%r76, %r20, 3;
	setp.lt.u32 	%p14, %r55, 3;
	mov.f32 	%f122, 0f00000000;
	mov.u32 	%r73, 0;
	mov.f32 	%f121, %f122;
	mov.f32 	%f124, %f122;
	mov.f32 	%f120, %f122;
	@%p14 bra 	$L__BB4_16;

	sub.s32 	%r72, %r20, %r76;
	mov.f32 	%f43, 0f00000000;
	mov.u32 	%r73, 0;
	mov.f32 	%f122, %f43;
	mov.f32 	%f121, %f43;
	mov.f32 	%f124, %f43;
	mov.f32 	%f120, %f43;

$L__BB4_15:
	.pragma "nounroll";
	shl.b32 	%r57, %r73, 2;
	add.s32 	%r59, %r50, %r57;
	ld.shared.v4.f32 	{%f44, %f45, %f46, %f47}, [%r59];
	add.s32 	%r60, %r73, %r17;
	shl.b32 	%r61, %r60, 2;
	add.s32 	%r62, %r11, %r61;
	ld.shared.v2.f32 	{%f52, %f53}, [%r62];
	fma.rn.ftz.f32 	%f57, %f52, %f44, %f43;
	sub.ftz.f32 	%f58, %f57, %f124;
	add.ftz.f32 	%f59, %f120, %f58;
	sub.ftz.f32 	%f60, %f59, %f120;
	sub.ftz.f32 	%f61, %f60, %f58;
	fma.rn.ftz.f32 	%f62, %f53, %f44, %f43;
	sub.ftz.f32 	%f63, %f62, %f122;
	add.ftz.f32 	%f64, %f121, %f63;
	sub.ftz.f32 	%f65, %f64, %f121;
	sub.ftz.f32 	%f66, %f65, %f63;
	ld.shared.f32 	%f67, [%r62+4];
	fma.rn.ftz.f32 	%f68, %f67, %f45, %f43;
	sub.ftz.f32 	%f69, %f68, %f61;
	add.ftz.f32 	%f70, %f59, %f69;
	sub.ftz.f32 	%f71, %f70, %f59;
	sub.ftz.f32 	%f72, %f71, %f69;
	ld.shared.v2.f32 	{%f73, %f74}, [%r62+8];
	fma.rn.ftz.f32 	%f77, %f73, %f45, %f43;
	sub.ftz.f32 	%f78, %f77, %f66;
	add.ftz.f32 	%f79, %f64, %f78;
	sub.ftz.f32 	%f80, %f79, %f64;
	sub.ftz.f32 	%f81, %f80, %f78;
	ld.shared.f32 	%f82, [%r62+8];
	fma.rn.ftz.f32 	%f83, %f82, %f46, %f43;
	sub.ftz.f32 	%f84, %f83, %f72;
	add.ftz.f32 	%f85, %f70, %f84;
	sub.ftz.f32 	%f86, %f85, %f70;
	sub.ftz.f32 	%f87, %f86, %f84;
	fma.rn.ftz.f32 	%f88, %f74, %f46, %f43;
	sub.ftz.f32 	%f89, %f88, %f81;
	add.ftz.f32 	%f90, %f79, %f89;
	sub.ftz.f32 	%f91, %f90, %f79;
	sub.ftz.f32 	%f92, %f91, %f89;
	ld.shared.f32 	%f93, [%r62+12];
	fma.rn.ftz.f32 	%f94, %f93, %f47, %f43;
	sub.ftz.f32 	%f95, %f94, %f87;
	add.ftz.f32 	%f120, %f85, %f95;
	sub.ftz.f32 	%f96, %f120, %f85;
	sub.ftz.f32 	%f124, %f96, %f95;
	ld.shared.f32 	%f97, [%r62+16];
	fma.rn.ftz.f32 	%f98, %f97, %f47, %f43;
	sub.ftz.f32 	%f99, %f98, %f92;
	add.ftz.f32 	%f121, %f90, %f99;
	sub.ftz.f32 	%f100, %f121, %f90;
	sub.ftz.f32 	%f122, %f100, %f99;
	add.s32 	%r73, %r73, 4;
	add.s32 	%r72, %r72, -4;
	setp.ne.s32 	%p15, %r72, 0;
	@%p15 bra 	$L__BB4_15;

$L__BB4_16:
	setp.eq.s32 	%p16, %r76, 0;
	@%p16 bra 	$L__BB4_19;

	shl.b32 	%r63, %r8, 3;
	add.s32 	%r64, %r7, %r63;
	shl.b32 	%r65, %r73, 2;
	add.s32 	%r66, %r64, %r65;
	add.s32 	%r68, %r50, %r66;
	add.s32 	%r75, %r68, 4;
	add.s32 	%r74, %r50, %r65;
	mov.f32 	%f127, %f121;
	mov.f32 	%f129, %f120;

$L__BB4_18:
	.pragma "nounroll";
	ld.shared.f32 	%f101, [%r75+-4];
	ld.shared.f32 	%f102, [%r74];
	mov.f32 	%f103, 0f00000000;
	fma.rn.ftz.f32 	%f104, %f101, %f102, %f103;
	sub.ftz.f32 	%f105, %f104, %f124;
	add.ftz.f32 	%f120, %f129, %f105;
	sub.ftz.f32 	%f106, %f120, %f129;
	sub.ftz.f32 	%f124, %f106, %f105;
	ld.shared.f32 	%f107, [%r75];
	fma.rn.ftz.f32 	%f108, %f107, %f102, %f103;
	sub.ftz.f32 	%f109, %f108, %f122;
	add.ftz.f32 	%f121, %f127, %f109;
	sub.ftz.f32 	%f110, %f121, %f127;
	sub.ftz.f32 	%f122, %f110, %f109;
	add.s32 	%r75, %r75, 4;
	add.s32 	%r74, %r74, 4;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p17, %r76, 0;
	mov.f32 	%f127, %f121;
	mov.f32 	%f129, %f120;
	@%p17 bra 	$L__BB4_18;

$L__BB4_19:
	cvta.to.global.u64 	%rd21, %rd10;
	shl.b64 	%rd22, %rd1, 2;
	add.s64 	%rd5, %rd21, %rd22;
	setp.lt.s32 	%p18, %r18, %r12;
	mad.lo.s32 	%r36, %r2, %r38, %r18;
	mov.f32 	%f133, 0f7FC00000;
	mov.f32 	%f132, %f133;
	@%p18 bra 	$L__BB4_21;

	ld.global.nc.f32 	%f112, [%rd5];
	mul.rn.ftz.f32 	%f132, %f120, %f112;

$L__BB4_21:
	setp.lt.s32 	%p19, %r19, %r12;
	setp.ge.s32 	%p20, %r19, %r38;
	or.pred  	%p21, %p20, %p19;
	@%p21 bra 	$L__BB4_23;

	ld.global.nc.f32 	%f114, [%rd5];
	mul.rn.ftz.f32 	%f133, %f121, %f114;

$L__BB4_23:
	cvta.to.global.u64 	%rd23, %rd11;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd6, %rd23, %rd24;
	st.global.f32 	[%rd6], %f132;
	@%p20 bra 	$L__BB4_25;

	st.global.f32 	[%rd6+4], %f133;

$L__BB4_25:
	ret;

}
	// .globl	cwma_batch_tiled_async_f32_2x_tile128
.visible .entry cwma_batch_tiled_async_f32_2x_tile128(
	.param .u64 cwma_batch_tiled_async_f32_2x_tile128_param_0,
	.param .u64 cwma_batch_tiled_async_f32_2x_tile128_param_1,
	.param .u64 cwma_batch_tiled_async_f32_2x_tile128_param_2,
	.param .u64 cwma_batch_tiled_async_f32_2x_tile128_param_3,
	.param .u32 cwma_batch_tiled_async_f32_2x_tile128_param_4,
	.param .u32 cwma_batch_tiled_async_f32_2x_tile128_param_5,
	.param .u32 cwma_batch_tiled_async_f32_2x_tile128_param_6,
	.param .u32 cwma_batch_tiled_async_f32_2x_tile128_param_7,
	.param .u64 cwma_batch_tiled_async_f32_2x_tile128_param_8
)
{
	.reg .pred 	%p<59>;
	.reg .b16 	%rs<35>;
	.reg .f32 	%f<139>;
	.reg .b32 	%r<192>;
	.reg .b64 	%rd<71>;
	// demoted variable
	.shared .align 8 .b8 _ZZN32CwmaBatchTiledPrecomputed2xAsyncILi128ELi2EE3runEPKfS2_PKiS2_iiiiPfE3pss[40];

	ld.param.u64 	%rd17, [cwma_batch_tiled_async_f32_2x_tile128_param_0];
	ld.param.u64 	%rd18, [cwma_batch_tiled_async_f32_2x_tile128_param_1];
	ld.param.u64 	%rd19, [cwma_batch_tiled_async_f32_2x_tile128_param_2];
	ld.param.u64 	%rd20, [cwma_batch_tiled_async_f32_2x_tile128_param_3];
	ld.param.u32 	%r63, [cwma_batch_tiled_async_f32_2x_tile128_param_4];
	ld.param.u32 	%r64, [cwma_batch_tiled_async_f32_2x_tile128_param_5];
	ld.param.u32 	%r65, [cwma_batch_tiled_async_f32_2x_tile128_param_6];
	ld.param.u32 	%r66, [cwma_batch_tiled_async_f32_2x_tile128_param_7];
	ld.param.u64 	%rd21, [cwma_batch_tiled_async_f32_2x_tile128_param_8];
	mov.u32 	%r1, %ntid.x;
	and.b32  	%r67, %r1, 2147483647;
	setp.ne.s32 	%p1, %r67, 64;
	@%p1 bra 	$L__BB5_55;

	mov.u32 	%r2, %ctaid.y;
	setp.ge.s32 	%p2, %r2, %r65;
	@%p2 bra 	$L__BB5_55;

	cvt.s64.s32 	%rd1, %r2;
	cvta.to.global.u64 	%rd22, %rd19;
	mul.wide.s32 	%rd23, %r2, 4;
	add.s64 	%rd24, %rd22, %rd23;
	ld.global.nc.u32 	%r3, [%rd24];
	add.s32 	%r68, %r3, -1;
	setp.gt.s32 	%p3, %r3, 1;
	selp.b32 	%r4, %r68, 0, %p3;
	add.s32 	%r5, %r4, 127;
	add.s32 	%r6, %r4, %r66;
	mov.u32 	%r7, %tid.x;
	setp.ge.s32 	%p4, %r7, %r4;
	@%p4 bra 	$L__BB5_5;

	cvta.to.global.u64 	%rd2, %rd18;
	mul.lo.s32 	%r69, %r2, %r63;
	cvt.s64.s32 	%rd3, %r69;
	mov.u32 	%r179, %r7;

$L__BB5_4:
	cvt.s64.s32 	%rd25, %r179;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.f32 	%f31, [%rd28];
	shl.b32 	%r70, %r179, 2;
	mov.u32 	%r71, shraw;
	add.s32 	%r72, %r71, %r70;
	st.shared.f32 	[%r72], %f31;
	add.s32 	%r179, %r179, %r1;
	setp.lt.s32 	%p5, %r179, %r4;
	@%p5 bra 	$L__BB5_4;

$L__BB5_5:
	bar.sync 	0;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r73, %tid.z;
	mov.u32 	%r74, %tid.y;
	mad.lo.s32 	%r75, %r10, %r73, %r74;
	mul.lo.s32 	%r76, %r75, %r1;
	neg.s32 	%r77, %r7;
	setp.ne.s32 	%p6, %r76, %r77;
	@%p6 bra 	$L__BB5_7;

	mov.u32 	%r87, %ntid.z;
	mul.lo.s32 	%r88, %r1, %r10;
	mul.lo.s32 	%r86, %r88, %r87;
	mov.u32 	%r80, _ZZN32CwmaBatchTiledPrecomputed2xAsyncILi128ELi2EE3runEPKfS2_PKiS2_iiiiPfE3pss;
	add.s32 	%r78, %r80, 8;
	// begin inline asm
	mbarrier.init.shared.b64 [%r78], %r86;
	// end inline asm
	// begin inline asm
	mbarrier.init.shared.b64 [%r80], %r86;
	// end inline asm
	add.s32 	%r84, %r80, 16;
	add.s32 	%r82, %r80, 24;
	// begin inline asm
	mbarrier.init.shared.b64 [%r82], %r86;
	// end inline asm
	// begin inline asm
	mbarrier.init.shared.b64 [%r84], %r86;
	// end inline asm
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r80;
	  cvta.shared.u64 	%rd30, %tmp; }
	add.s64 	%rd29, %rd30, 32;
	// begin inline asm
	st.relaxed.cta.b32 [%rd29],%r86;
	// end inline asm

$L__BB5_7:
	shl.b32 	%r90, %r4, 2;
	add.s32 	%r91, %r90, 15;
	and.b32  	%r92, %r91, -16;
	mov.u32 	%r93, shraw;
	add.s32 	%r11, %r93, %r92;
	mov.u32 	%r180, 0;
	barrier.sync 	0;
	mov.u32 	%r12, %nctaid.x;
	shl.b32 	%r13, %r12, 7;
	mov.u32 	%r94, %ctaid.x;
	shl.b32 	%r185, %r94, 7;
	or.b32  	%r95, %r185, 1;
	sub.s32 	%r15, %r95, %r4;
	cvta.to.global.u64 	%rd4, %rd17;
	// begin inline asm
	mov.u64 %rd31, %globaltimer;
	// end inline asm
	mul.lo.s32 	%r16, %r2, %r64;
	cvta.to.global.u64 	%rd6, %rd21;
	cvta.to.global.u64 	%rd7, %rd20;
	bra.uni 	$L__BB5_8;

$L__BB5_79:
	add.s32 	%r180, %r180, 1;

$L__BB5_8:
	mov.u32 	%r98, _ZZN32CwmaBatchTiledPrecomputed2xAsyncILi128ELi2EE3runEPKfS2_PKiS2_iiiiPfE3pss;
	add.s32 	%r96, %r98, 8;
	mov.u32 	%r97, 1;
	// begin inline asm
	{.reg .pred %p;mbarrier.test_wait.parity.shared.b64 %p, [%r96], %r97;selp.u16 %rs10, 1, 0, %p;}
	// end inline asm
	setp.eq.s16 	%p7, %rs10, 0;
	@%p7 bra 	$L__BB5_74;
	bra.uni 	$L__BB5_9;

$L__BB5_74:
	setp.lt.s32 	%p56, %r180, 16;
	@%p56 bra 	$L__BB5_79;

	// begin inline asm
	mov.u64 %rd66, %globaltimer;
	// end inline asm
	sub.s64 	%rd16, %rd66, %rd31;
	setp.lt.s64 	%p57, %rd16, 4000000;
	@%p57 bra 	$L__BB5_77;
	bra.uni 	$L__BB5_76;

$L__BB5_77:
	setp.lt.s64 	%p58, %rd16, 40000;
	@%p58 bra 	$L__BB5_8;

	shr.s64 	%rd67, %rd16, 63;
	shr.u64 	%rd68, %rd67, 62;
	add.s64 	%rd69, %rd16, %rd68;
	shr.u64 	%rd70, %rd69, 2;
	cvt.u32.u64 	%r178, %rd70;
	// begin inline asm
	nanosleep.u32 %r178;
	// end inline asm
	bra.uni 	$L__BB5_8;

$L__BB5_76:
	mov.u32 	%r177, 1000000;
	// begin inline asm
	nanosleep.u32 %r177;
	// end inline asm
	bra.uni 	$L__BB5_8;

$L__BB5_9:
	setp.ge.s32 	%p8, %r7, %r5;
	@%p8 bra 	$L__BB5_15;

	mov.u32 	%r181, %r7;

$L__BB5_11:
	add.s32 	%r19, %r15, %r181;
	setp.gt.s32 	%p9, %r19, -1;
	setp.lt.s32 	%p10, %r19, %r64;
	and.pred  	%p11, %p9, %p10;
	shl.b32 	%r99, %r181, 2;
	add.s32 	%r20, %r11, %r99;
	@%p11 bra 	$L__BB5_13;
	bra.uni 	$L__BB5_12;

$L__BB5_13:
	mul.wide.s32 	%rd33, %r19, 4;
	add.s64 	%rd32, %rd4, %rd33;
	// begin inline asm
	cp.async.ca.shared.global [%r20], [%rd32], 4, 4;
	// end inline asm
	bra.uni 	$L__BB5_14;

$L__BB5_12:
	mov.u32 	%r100, 0;
	st.shared.u32 	[%r20], %r100;

$L__BB5_14:
	add.s32 	%r181, %r181, %r1;
	setp.lt.s32 	%p12, %r181, %r5;
	@%p12 bra 	$L__BB5_11;

$L__BB5_15:
	// begin inline asm
	cp.async.mbarrier.arrive.shared.b64 [%r98];
	// end inline asm
	// begin inline asm
	mbarrier.arrive.shared.b64                                  %rd34,  [%r98];           // 1. 
	// end inline asm
	// begin inline asm
	mov.u64 %rd35, %globaltimer;
	// end inline asm
	mov.u32 	%r182, 0;
	add.s32 	%r105, %r98, 24;
	bra.uni 	$L__BB5_16;

$L__BB5_73:
	add.s32 	%r182, %r182, 1;

$L__BB5_16:
	mov.u32 	%r106, 1;
	// begin inline asm
	{.reg .pred %p;mbarrier.test_wait.parity.shared.b64 %p, [%r105], %r106;selp.u16 %rs11, 1, 0, %p;}
	// end inline asm
	setp.eq.s16 	%p13, %rs11, 0;
	@%p13 bra 	$L__BB5_68;
	bra.uni 	$L__BB5_17;

$L__BB5_68:
	setp.lt.s32 	%p53, %r182, 16;
	@%p53 bra 	$L__BB5_73;

	// begin inline asm
	mov.u64 %rd61, %globaltimer;
	// end inline asm
	sub.s64 	%rd15, %rd61, %rd35;
	setp.lt.s64 	%p54, %rd15, 4000000;
	@%p54 bra 	$L__BB5_71;
	bra.uni 	$L__BB5_70;

$L__BB5_71:
	setp.lt.s64 	%p55, %rd15, 40000;
	@%p55 bra 	$L__BB5_16;

	shr.s64 	%rd62, %rd15, 63;
	shr.u64 	%rd63, %rd62, 62;
	add.s64 	%rd64, %rd15, %rd63;
	shr.u64 	%rd65, %rd64, 2;
	cvt.u32.u64 	%r176, %rd65;
	// begin inline asm
	nanosleep.u32 %r176;
	// end inline asm
	bra.uni 	$L__BB5_16;

$L__BB5_70:
	mov.u32 	%r175, 1000000;
	// begin inline asm
	nanosleep.u32 %r175;
	// end inline asm
	bra.uni 	$L__BB5_16;

$L__BB5_17:
	@%p8 bra 	$L__BB5_23;

	add.s32 	%r23, %r15, %r13;
	mov.u32 	%r183, %r7;

$L__BB5_19:
	add.s32 	%r25, %r23, %r183;
	setp.gt.s32 	%p15, %r25, -1;
	setp.lt.s32 	%p16, %r25, %r64;
	and.pred  	%p17, %p15, %p16;
	add.s32 	%r108, %r183, %r5;
	shl.b32 	%r109, %r108, 2;
	add.s32 	%r26, %r11, %r109;
	@%p17 bra 	$L__BB5_21;
	bra.uni 	$L__BB5_20;

$L__BB5_21:
	mul.wide.s32 	%rd37, %r25, 4;
	add.s64 	%rd36, %rd4, %rd37;
	// begin inline asm
	cp.async.ca.shared.global [%r26], [%rd36], 4, 4;
	// end inline asm
	bra.uni 	$L__BB5_22;

$L__BB5_20:
	mov.u32 	%r110, 0;
	st.shared.u32 	[%r26], %r110;

$L__BB5_22:
	add.s32 	%r183, %r183, %r1;
	setp.lt.s32 	%p18, %r183, %r5;
	@%p18 bra 	$L__BB5_19;

$L__BB5_23:
	add.s32 	%r113, %r98, 16;
	// begin inline asm
	cp.async.mbarrier.arrive.shared.b64 [%r113];
	// end inline asm
	// begin inline asm
	mbarrier.arrive.shared.b64                                  %rd38,  [%r113];           // 1. 
	// end inline asm
	setp.ge.s32 	%p19, %r185, %r64;
	@%p19 bra 	$L__BB5_52;

	shl.b32 	%r28, %r7, 1;
	shl.b64 	%rd39, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd39;
	shl.b32 	%r116, %r12, 8;
	or.b32  	%r117, %r116, 1;
	sub.s32 	%r29, %r117, %r4;
	max.s32 	%r118, %r4, 1;
	add.s32 	%r30, %r118, -1;
	and.b32  	%r31, %r118, 3;
	sub.s32 	%r32, %r118, %r31;
	mov.u16 	%rs34, 244;
	mov.u16 	%rs32, 0;
	mov.u32 	%r115, 0;
	mov.u32 	%r184, %r115;
	mov.u16 	%rs33, %rs32;

$L__BB5_25:
	and.b16  	%rs15, %rs33, 255;
	mul.wide.u16 	%r120, %rs15, 16;
	and.b16  	%rs4, %rs34, 2;
	shr.u16 	%rs16, %rs4, 1;
	// begin inline asm
	mov.u64 %rd40, %globaltimer;
	// end inline asm
	add.s32 	%r35, %r98, %r120;
	cvt.u32.u16 	%r36, %rs16;
	mov.u32 	%r186, %r115;
	bra.uni 	$L__BB5_26;

$L__BB5_67:
	add.s32 	%r186, %r186, 1;

$L__BB5_26:
	// begin inline asm
	{.reg .pred %p;mbarrier.test_wait.parity.shared.b64 %p, [%r35], %r36;selp.u16 %rs17, 1, 0, %p;}
	// end inline asm
	setp.eq.s16 	%p20, %rs17, 0;
	@%p20 bra 	$L__BB5_62;
	bra.uni 	$L__BB5_27;

$L__BB5_62:
	setp.lt.s32 	%p50, %r186, 16;
	@%p50 bra 	$L__BB5_67;

	// begin inline asm
	mov.u64 %rd56, %globaltimer;
	// end inline asm
	sub.s64 	%rd14, %rd56, %rd40;
	setp.lt.s64 	%p51, %rd14, 4000000;
	@%p51 bra 	$L__BB5_65;
	bra.uni 	$L__BB5_64;

$L__BB5_65:
	setp.lt.s64 	%p52, %rd14, 40000;
	@%p52 bra 	$L__BB5_26;

	shr.s64 	%rd57, %rd14, 63;
	shr.u64 	%rd58, %rd57, 62;
	add.s64 	%rd59, %rd14, %rd58;
	shr.u64 	%rd60, %rd59, 2;
	cvt.u32.u64 	%r174, %rd60;
	// begin inline asm
	nanosleep.u32 %r174;
	// end inline asm
	bra.uni 	$L__BB5_26;

$L__BB5_64:
	mov.u32 	%r173, 1000000;
	// begin inline asm
	nanosleep.u32 %r173;
	// end inline asm
	bra.uni 	$L__BB5_26;

$L__BB5_27:
	bar.sync 	0;
	mul.lo.s32 	%r38, %r184, %r5;
	add.s32 	%r39, %r185, %r28;
	setp.ge.s32 	%p21, %r39, %r64;
	@%p21 bra 	$L__BB5_42;

	setp.lt.s32 	%p22, %r3, 2;
	mov.f32 	%f135, 0f00000000;
	mov.f32 	%f136, %f135;
	@%p22 bra 	$L__BB5_36;

	setp.lt.u32 	%p23, %r30, 3;
	mov.f32 	%f131, 0f00000000;
	mov.u32 	%r189, 0;
	mov.f32 	%f136, %f131;
	mov.f32 	%f133, %f131;
	mov.f32 	%f135, %f131;
	@%p23 bra 	$L__BB5_32;

	mov.f32 	%f42, 0f00000000;
	mov.u32 	%r189, 0;
	mov.f32 	%f131, %f42;
	mov.f32 	%f136, %f42;
	mov.f32 	%f133, %f42;
	mov.f32 	%f135, %f42;
	mov.u32 	%r188, %r32;

$L__BB5_31:
	.pragma "nounroll";
	shl.b32 	%r126, %r189, 2;
	add.s32 	%r128, %r93, %r126;
	ld.shared.v4.f32 	{%f43, %f44, %f45, %f46}, [%r128];
	add.s32 	%r129, %r189, %r28;
	add.s32 	%r130, %r129, %r38;
	shl.b32 	%r131, %r130, 2;
	add.s32 	%r132, %r11, %r131;
	ld.shared.f32 	%f51, [%r132];
	fma.rn.ftz.f32 	%f53, %f51, %f43, %f42;
	sub.ftz.f32 	%f54, %f53, %f133;
	add.ftz.f32 	%f55, %f135, %f54;
	sub.ftz.f32 	%f56, %f55, %f135;
	sub.ftz.f32 	%f57, %f56, %f54;
	ld.shared.f32 	%f58, [%r132+4];
	fma.rn.ftz.f32 	%f59, %f58, %f43, %f42;
	sub.ftz.f32 	%f60, %f59, %f131;
	add.ftz.f32 	%f61, %f136, %f60;
	sub.ftz.f32 	%f62, %f61, %f136;
	sub.ftz.f32 	%f63, %f62, %f60;
	fma.rn.ftz.f32 	%f64, %f58, %f44, %f42;
	sub.ftz.f32 	%f65, %f64, %f57;
	add.ftz.f32 	%f66, %f55, %f65;
	sub.ftz.f32 	%f67, %f66, %f55;
	sub.ftz.f32 	%f68, %f67, %f65;
	ld.shared.f32 	%f69, [%r132+8];
	fma.rn.ftz.f32 	%f70, %f69, %f44, %f42;
	sub.ftz.f32 	%f71, %f70, %f63;
	add.ftz.f32 	%f72, %f61, %f71;
	sub.ftz.f32 	%f73, %f72, %f61;
	sub.ftz.f32 	%f74, %f73, %f71;
	fma.rn.ftz.f32 	%f75, %f69, %f45, %f42;
	sub.ftz.f32 	%f76, %f75, %f68;
	add.ftz.f32 	%f77, %f66, %f76;
	sub.ftz.f32 	%f78, %f77, %f66;
	sub.ftz.f32 	%f79, %f78, %f76;
	ld.shared.f32 	%f80, [%r132+12];
	fma.rn.ftz.f32 	%f81, %f80, %f45, %f42;
	sub.ftz.f32 	%f82, %f81, %f74;
	add.ftz.f32 	%f83, %f72, %f82;
	sub.ftz.f32 	%f84, %f83, %f72;
	sub.ftz.f32 	%f85, %f84, %f82;
	fma.rn.ftz.f32 	%f86, %f80, %f46, %f42;
	sub.ftz.f32 	%f87, %f86, %f79;
	add.ftz.f32 	%f135, %f77, %f87;
	sub.ftz.f32 	%f88, %f135, %f77;
	sub.ftz.f32 	%f133, %f88, %f87;
	ld.shared.f32 	%f89, [%r132+16];
	fma.rn.ftz.f32 	%f90, %f89, %f46, %f42;
	sub.ftz.f32 	%f91, %f90, %f85;
	add.ftz.f32 	%f136, %f83, %f91;
	sub.ftz.f32 	%f92, %f136, %f83;
	sub.ftz.f32 	%f131, %f92, %f91;
	add.s32 	%r189, %r189, 4;
	add.s32 	%r188, %r188, -4;
	setp.ne.s32 	%p24, %r188, 0;
	@%p24 bra 	$L__BB5_31;

$L__BB5_32:
	setp.eq.s32 	%p25, %r31, 0;
	@%p25 bra 	$L__BB5_36;

	setp.eq.s32 	%p26, %r31, 1;
	add.s32 	%r133, %r189, %r28;
	add.s32 	%r134, %r133, %r38;
	shl.b32 	%r135, %r134, 2;
	add.s32 	%r45, %r11, %r135;
	ld.shared.f32 	%f93, [%r45];
	shl.b32 	%r136, %r189, 2;
	add.s32 	%r46, %r93, %r136;
	ld.shared.f32 	%f94, [%r46];
	mov.f32 	%f95, 0f00000000;
	fma.rn.ftz.f32 	%f96, %f93, %f94, %f95;
	sub.ftz.f32 	%f97, %f96, %f133;
	add.ftz.f32 	%f15, %f135, %f97;
	sub.ftz.f32 	%f98, %f15, %f135;
	sub.ftz.f32 	%f16, %f98, %f97;
	ld.shared.f32 	%f99, [%r45+4];
	fma.rn.ftz.f32 	%f100, %f99, %f94, %f95;
	sub.ftz.f32 	%f101, %f100, %f131;
	add.ftz.f32 	%f17, %f136, %f101;
	sub.ftz.f32 	%f102, %f17, %f136;
	sub.ftz.f32 	%f18, %f102, %f101;
	mov.f32 	%f135, %f15;
	mov.f32 	%f136, %f17;
	@%p26 bra 	$L__BB5_36;

	setp.eq.s32 	%p27, %r31, 2;
	ld.shared.f32 	%f103, [%r45+4];
	ld.shared.f32 	%f104, [%r46+4];
	fma.rn.ftz.f32 	%f106, %f103, %f104, %f95;
	sub.ftz.f32 	%f107, %f106, %f16;
	add.ftz.f32 	%f135, %f15, %f107;
	sub.ftz.f32 	%f108, %f135, %f15;
	sub.ftz.f32 	%f20, %f108, %f107;
	ld.shared.f32 	%f109, [%r45+8];
	fma.rn.ftz.f32 	%f110, %f109, %f104, %f95;
	sub.ftz.f32 	%f111, %f110, %f18;
	add.ftz.f32 	%f136, %f17, %f111;
	sub.ftz.f32 	%f112, %f136, %f17;
	sub.ftz.f32 	%f22, %f112, %f111;
	@%p27 bra 	$L__BB5_36;

	ld.shared.f32 	%f113, [%r45+8];
	ld.shared.f32 	%f114, [%r46+8];
	mov.f32 	%f115, 0f00000000;
	fma.rn.ftz.f32 	%f116, %f113, %f114, %f115;
	sub.ftz.f32 	%f117, %f116, %f20;
	add.ftz.f32 	%f135, %f135, %f117;
	ld.shared.f32 	%f118, [%r45+12];
	fma.rn.ftz.f32 	%f119, %f118, %f114, %f115;
	sub.ftz.f32 	%f120, %f119, %f22;
	add.ftz.f32 	%f136, %f136, %f120;

$L__BB5_36:
	setp.lt.s32 	%p28, %r39, %r6;
	mov.f32 	%f138, 0f7FC00000;
	mov.f32 	%f137, %f138;
	@%p28 bra 	$L__BB5_38;

	ld.global.nc.f32 	%f122, [%rd9];
	mul.rn.ftz.f32 	%f137, %f135, %f122;

$L__BB5_38:
	add.s32 	%r138, %r39, 1;
	setp.ge.s32 	%p29, %r138, %r64;
	setp.lt.s32 	%p30, %r138, %r6;
	or.pred  	%p31, %p29, %p30;
	@%p31 bra 	$L__BB5_40;

	ld.global.nc.f32 	%f124, [%rd9];
	mul.rn.ftz.f32 	%f138, %f136, %f124;

$L__BB5_40:
	add.s32 	%r140, %r39, %r16;
	mul.wide.s32 	%rd41, %r140, 4;
	add.s64 	%rd11, %rd6, %rd41;
	st.global.f32 	[%rd11], %f137;
	@%p29 bra 	$L__BB5_42;

	st.global.f32 	[%rd11+4], %f138;

$L__BB5_42:
	bar.sync 	0;
	add.s32 	%r141, %r35, 8;
	// begin inline asm
	mbarrier.arrive.shared.b64                                  %rd42,  [%r141];           // 1. 
	// end inline asm
	add.s16 	%rs18, %rs33, 1;
	and.b16  	%rs19, %rs18, 255;
	setp.eq.s16 	%p33, %rs19, 2;
	setp.eq.s16 	%p34, %rs4, 0;
	selp.u16 	%rs20, 1, 0, %p34;
	shl.b16 	%rs21, %rs20, 1;
	and.b16  	%rs22, %rs34, -3;
	or.b16  	%rs23, %rs21, %rs22;
	selp.b16 	%rs5, %rs23, %rs34, %p33;
	selp.b16 	%rs33, 0, %rs18, %p33;
	and.b16  	%rs24, %rs32, 255;
	mul.wide.u16 	%r47, %rs24, 16;
	and.b16  	%rs7, %rs5, 1;
	// begin inline asm
	mov.u64 %rd43, %globaltimer;
	// end inline asm
	add.s32 	%r144, %r98, %r47;
	add.s32 	%r48, %r144, 8;
	cvt.u32.u16 	%r49, %rs7;
	mov.u32 	%r190, 0;
	bra.uni 	$L__BB5_43;

$L__BB5_61:
	add.s32 	%r190, %r190, 1;

$L__BB5_43:
	// begin inline asm
	{.reg .pred %p;mbarrier.test_wait.parity.shared.b64 %p, [%r48], %r49;selp.u16 %rs25, 1, 0, %p;}
	// end inline asm
	setp.eq.s16 	%p35, %rs25, 0;
	@%p35 bra 	$L__BB5_56;
	bra.uni 	$L__BB5_44;

$L__BB5_56:
	setp.lt.s32 	%p47, %r190, 16;
	@%p47 bra 	$L__BB5_61;

	// begin inline asm
	mov.u64 %rd51, %globaltimer;
	// end inline asm
	sub.s64 	%rd13, %rd51, %rd43;
	setp.lt.s64 	%p48, %rd13, 4000000;
	@%p48 bra 	$L__BB5_59;
	bra.uni 	$L__BB5_58;

$L__BB5_59:
	setp.lt.s64 	%p49, %rd13, 40000;
	@%p49 bra 	$L__BB5_43;

	shr.s64 	%rd52, %rd13, 63;
	shr.u64 	%rd53, %rd52, 62;
	add.s64 	%rd54, %rd13, %rd53;
	shr.u64 	%rd55, %rd54, 2;
	cvt.u32.u64 	%r172, %rd55;
	// begin inline asm
	nanosleep.u32 %r172;
	// end inline asm
	bra.uni 	$L__BB5_43;

$L__BB5_58:
	mov.u32 	%r171, 1000000;
	// begin inline asm
	nanosleep.u32 %r171;
	// end inline asm
	bra.uni 	$L__BB5_43;

$L__BB5_44:
	@%p8 bra 	$L__BB5_50;

	add.s32 	%r51, %r29, %r185;
	mov.u32 	%r191, %r7;

$L__BB5_46:
	add.s32 	%r53, %r51, %r191;
	setp.gt.s32 	%p37, %r53, -1;
	setp.lt.s32 	%p38, %r53, %r64;
	and.pred  	%p39, %p37, %p38;
	add.s32 	%r147, %r191, %r38;
	shl.b32 	%r148, %r147, 2;
	add.s32 	%r54, %r11, %r148;
	@%p39 bra 	$L__BB5_48;
	bra.uni 	$L__BB5_47;

$L__BB5_48:
	mul.wide.s32 	%rd45, %r53, 4;
	add.s64 	%rd44, %rd4, %rd45;
	// begin inline asm
	cp.async.ca.shared.global [%r54], [%rd44], 4, 4;
	// end inline asm
	bra.uni 	$L__BB5_49;

$L__BB5_47:
	mov.u32 	%r149, 0;
	st.shared.u32 	[%r54], %r149;

$L__BB5_49:
	add.s32 	%r191, %r191, %r1;
	setp.lt.s32 	%p40, %r191, %r5;
	@%p40 bra 	$L__BB5_46;

$L__BB5_50:
	// begin inline asm
	cp.async.mbarrier.arrive.shared.b64 [%r144];
	// end inline asm
	// begin inline asm
	mbarrier.arrive.shared.b64                                  %rd46,  [%r144];           // 1. 
	// end inline asm
	add.s16 	%rs26, %rs32, 1;
	and.b16  	%rs27, %rs26, 255;
	setp.eq.s16 	%p41, %rs27, 2;
	setp.eq.s16 	%p42, %rs7, 0;
	selp.u16 	%rs28, 1, 0, %p42;
	and.b16  	%rs29, %rs5, -2;
	or.b16  	%rs30, %rs29, %rs28;
	selp.b16 	%rs34, %rs30, %rs5, %p41;
	selp.b16 	%rs32, 0, %rs26, %p41;
	add.s32 	%r154, %r184, 1;
	shr.u32 	%r155, %r154, 31;
	add.s32 	%r156, %r154, %r155;
	and.b32  	%r157, %r156, -2;
	sub.s32 	%r184, %r154, %r157;
	add.s32 	%r185, %r185, %r13;
	setp.lt.s32 	%p43, %r185, %r64;
	@%p43 bra 	$L__BB5_25;

	and.b16  	%rs31, %rs34, 4;
	setp.eq.s16 	%p44, %rs31, 0;
	@%p44 bra 	$L__BB5_55;

$L__BB5_52:
	// begin inline asm
	activemask.b32 %r158;
	// end inline asm
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r98;
	  cvta.shared.u64 	%rd47, %tmp; }
	add.s64 	%rd48, %rd47, 32;
	match.any.sync.b64 	%r58, %rd48, %r158;
	brev.b32 	%r161, %r58;
	bfind.shiftamt.u32 	%r162, %r161;
	// begin inline asm
	mov.u32 %r159, %laneid;
	// end inline asm
	setp.ne.s32 	%p45, %r159, %r162;
	@%p45 bra 	$L__BB5_55;

	popc.b32 	%r165, %r58;
	neg.s32 	%r164, %r165;
	// begin inline asm
	fence.sc.cta;
	// end inline asm
	// begin inline asm
	atom.add.acquire.cta.u32 %r163,[%rd48],%r164;
	// end inline asm
	setp.ne.s32 	%p46, %r163, %r165;
	@%p46 bra 	$L__BB5_55;

	// begin inline asm
	mbarrier.inval.shared.b64 [%r98];
	// end inline asm
	// begin inline asm
	mbarrier.inval.shared.b64 [%r96];
	// end inline asm
	// begin inline asm
	mbarrier.inval.shared.b64 [%r113];
	// end inline asm
	// begin inline asm
	mbarrier.inval.shared.b64 [%r105];
	// end inline asm

$L__BB5_55:
	ret;

}
	// .globl	cwma_batch_tiled_async_f32_2x_tile256
.visible .entry cwma_batch_tiled_async_f32_2x_tile256(
	.param .u64 cwma_batch_tiled_async_f32_2x_tile256_param_0,
	.param .u64 cwma_batch_tiled_async_f32_2x_tile256_param_1,
	.param .u64 cwma_batch_tiled_async_f32_2x_tile256_param_2,
	.param .u64 cwma_batch_tiled_async_f32_2x_tile256_param_3,
	.param .u32 cwma_batch_tiled_async_f32_2x_tile256_param_4,
	.param .u32 cwma_batch_tiled_async_f32_2x_tile256_param_5,
	.param .u32 cwma_batch_tiled_async_f32_2x_tile256_param_6,
	.param .u32 cwma_batch_tiled_async_f32_2x_tile256_param_7,
	.param .u64 cwma_batch_tiled_async_f32_2x_tile256_param_8
)
{
	.reg .pred 	%p<59>;
	.reg .b16 	%rs<35>;
	.reg .f32 	%f<139>;
	.reg .b32 	%r<192>;
	.reg .b64 	%rd<71>;
	// demoted variable
	.shared .align 8 .b8 _ZZN32CwmaBatchTiledPrecomputed2xAsyncILi256ELi2EE3runEPKfS2_PKiS2_iiiiPfE3pss[40];

	ld.param.u64 	%rd17, [cwma_batch_tiled_async_f32_2x_tile256_param_0];
	ld.param.u64 	%rd18, [cwma_batch_tiled_async_f32_2x_tile256_param_1];
	ld.param.u64 	%rd19, [cwma_batch_tiled_async_f32_2x_tile256_param_2];
	ld.param.u64 	%rd20, [cwma_batch_tiled_async_f32_2x_tile256_param_3];
	ld.param.u32 	%r63, [cwma_batch_tiled_async_f32_2x_tile256_param_4];
	ld.param.u32 	%r64, [cwma_batch_tiled_async_f32_2x_tile256_param_5];
	ld.param.u32 	%r65, [cwma_batch_tiled_async_f32_2x_tile256_param_6];
	ld.param.u32 	%r66, [cwma_batch_tiled_async_f32_2x_tile256_param_7];
	ld.param.u64 	%rd21, [cwma_batch_tiled_async_f32_2x_tile256_param_8];
	mov.u32 	%r1, %ntid.x;
	and.b32  	%r67, %r1, 2147483647;
	setp.ne.s32 	%p1, %r67, 128;
	@%p1 bra 	$L__BB6_55;

	mov.u32 	%r2, %ctaid.y;
	setp.ge.s32 	%p2, %r2, %r65;
	@%p2 bra 	$L__BB6_55;

	cvt.s64.s32 	%rd1, %r2;
	cvta.to.global.u64 	%rd22, %rd19;
	mul.wide.s32 	%rd23, %r2, 4;
	add.s64 	%rd24, %rd22, %rd23;
	ld.global.nc.u32 	%r3, [%rd24];
	add.s32 	%r68, %r3, -1;
	setp.gt.s32 	%p3, %r3, 1;
	selp.b32 	%r4, %r68, 0, %p3;
	add.s32 	%r5, %r4, 255;
	add.s32 	%r6, %r4, %r66;
	mov.u32 	%r7, %tid.x;
	setp.ge.s32 	%p4, %r7, %r4;
	@%p4 bra 	$L__BB6_5;

	cvta.to.global.u64 	%rd2, %rd18;
	mul.lo.s32 	%r69, %r2, %r63;
	cvt.s64.s32 	%rd3, %r69;
	mov.u32 	%r179, %r7;

$L__BB6_4:
	cvt.s64.s32 	%rd25, %r179;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.f32 	%f31, [%rd28];
	shl.b32 	%r70, %r179, 2;
	mov.u32 	%r71, shraw;
	add.s32 	%r72, %r71, %r70;
	st.shared.f32 	[%r72], %f31;
	add.s32 	%r179, %r179, %r1;
	setp.lt.s32 	%p5, %r179, %r4;
	@%p5 bra 	$L__BB6_4;

$L__BB6_5:
	bar.sync 	0;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r73, %tid.z;
	mov.u32 	%r74, %tid.y;
	mad.lo.s32 	%r75, %r10, %r73, %r74;
	mul.lo.s32 	%r76, %r75, %r1;
	neg.s32 	%r77, %r7;
	setp.ne.s32 	%p6, %r76, %r77;
	@%p6 bra 	$L__BB6_7;

	mov.u32 	%r87, %ntid.z;
	mul.lo.s32 	%r88, %r1, %r10;
	mul.lo.s32 	%r86, %r88, %r87;
	mov.u32 	%r80, _ZZN32CwmaBatchTiledPrecomputed2xAsyncILi256ELi2EE3runEPKfS2_PKiS2_iiiiPfE3pss;
	add.s32 	%r78, %r80, 8;
	// begin inline asm
	mbarrier.init.shared.b64 [%r78], %r86;
	// end inline asm
	// begin inline asm
	mbarrier.init.shared.b64 [%r80], %r86;
	// end inline asm
	add.s32 	%r84, %r80, 16;
	add.s32 	%r82, %r80, 24;
	// begin inline asm
	mbarrier.init.shared.b64 [%r82], %r86;
	// end inline asm
	// begin inline asm
	mbarrier.init.shared.b64 [%r84], %r86;
	// end inline asm
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r80;
	  cvta.shared.u64 	%rd30, %tmp; }
	add.s64 	%rd29, %rd30, 32;
	// begin inline asm
	st.relaxed.cta.b32 [%rd29],%r86;
	// end inline asm

$L__BB6_7:
	shl.b32 	%r90, %r4, 2;
	add.s32 	%r91, %r90, 15;
	and.b32  	%r92, %r91, -16;
	mov.u32 	%r93, shraw;
	add.s32 	%r11, %r93, %r92;
	mov.u32 	%r180, 0;
	barrier.sync 	0;
	mov.u32 	%r12, %nctaid.x;
	shl.b32 	%r13, %r12, 8;
	mov.u32 	%r94, %ctaid.x;
	shl.b32 	%r185, %r94, 8;
	or.b32  	%r95, %r185, 1;
	sub.s32 	%r15, %r95, %r4;
	cvta.to.global.u64 	%rd4, %rd17;
	// begin inline asm
	mov.u64 %rd31, %globaltimer;
	// end inline asm
	mul.lo.s32 	%r16, %r2, %r64;
	cvta.to.global.u64 	%rd6, %rd21;
	cvta.to.global.u64 	%rd7, %rd20;
	bra.uni 	$L__BB6_8;

$L__BB6_79:
	add.s32 	%r180, %r180, 1;

$L__BB6_8:
	mov.u32 	%r98, _ZZN32CwmaBatchTiledPrecomputed2xAsyncILi256ELi2EE3runEPKfS2_PKiS2_iiiiPfE3pss;
	add.s32 	%r96, %r98, 8;
	mov.u32 	%r97, 1;
	// begin inline asm
	{.reg .pred %p;mbarrier.test_wait.parity.shared.b64 %p, [%r96], %r97;selp.u16 %rs10, 1, 0, %p;}
	// end inline asm
	setp.eq.s16 	%p7, %rs10, 0;
	@%p7 bra 	$L__BB6_74;
	bra.uni 	$L__BB6_9;

$L__BB6_74:
	setp.lt.s32 	%p56, %r180, 16;
	@%p56 bra 	$L__BB6_79;

	// begin inline asm
	mov.u64 %rd66, %globaltimer;
	// end inline asm
	sub.s64 	%rd16, %rd66, %rd31;
	setp.lt.s64 	%p57, %rd16, 4000000;
	@%p57 bra 	$L__BB6_77;
	bra.uni 	$L__BB6_76;

$L__BB6_77:
	setp.lt.s64 	%p58, %rd16, 40000;
	@%p58 bra 	$L__BB6_8;

	shr.s64 	%rd67, %rd16, 63;
	shr.u64 	%rd68, %rd67, 62;
	add.s64 	%rd69, %rd16, %rd68;
	shr.u64 	%rd70, %rd69, 2;
	cvt.u32.u64 	%r178, %rd70;
	// begin inline asm
	nanosleep.u32 %r178;
	// end inline asm
	bra.uni 	$L__BB6_8;

$L__BB6_76:
	mov.u32 	%r177, 1000000;
	// begin inline asm
	nanosleep.u32 %r177;
	// end inline asm
	bra.uni 	$L__BB6_8;

$L__BB6_9:
	setp.ge.s32 	%p8, %r7, %r5;
	@%p8 bra 	$L__BB6_15;

	mov.u32 	%r181, %r7;

$L__BB6_11:
	add.s32 	%r19, %r15, %r181;
	setp.gt.s32 	%p9, %r19, -1;
	setp.lt.s32 	%p10, %r19, %r64;
	and.pred  	%p11, %p9, %p10;
	shl.b32 	%r99, %r181, 2;
	add.s32 	%r20, %r11, %r99;
	@%p11 bra 	$L__BB6_13;
	bra.uni 	$L__BB6_12;

$L__BB6_13:
	mul.wide.s32 	%rd33, %r19, 4;
	add.s64 	%rd32, %rd4, %rd33;
	// begin inline asm
	cp.async.ca.shared.global [%r20], [%rd32], 4, 4;
	// end inline asm
	bra.uni 	$L__BB6_14;

$L__BB6_12:
	mov.u32 	%r100, 0;
	st.shared.u32 	[%r20], %r100;

$L__BB6_14:
	add.s32 	%r181, %r181, %r1;
	setp.lt.s32 	%p12, %r181, %r5;
	@%p12 bra 	$L__BB6_11;

$L__BB6_15:
	// begin inline asm
	cp.async.mbarrier.arrive.shared.b64 [%r98];
	// end inline asm
	// begin inline asm
	mbarrier.arrive.shared.b64                                  %rd34,  [%r98];           // 1. 
	// end inline asm
	// begin inline asm
	mov.u64 %rd35, %globaltimer;
	// end inline asm
	mov.u32 	%r182, 0;
	add.s32 	%r105, %r98, 24;
	bra.uni 	$L__BB6_16;

$L__BB6_73:
	add.s32 	%r182, %r182, 1;

$L__BB6_16:
	mov.u32 	%r106, 1;
	// begin inline asm
	{.reg .pred %p;mbarrier.test_wait.parity.shared.b64 %p, [%r105], %r106;selp.u16 %rs11, 1, 0, %p;}
	// end inline asm
	setp.eq.s16 	%p13, %rs11, 0;
	@%p13 bra 	$L__BB6_68;
	bra.uni 	$L__BB6_17;

$L__BB6_68:
	setp.lt.s32 	%p53, %r182, 16;
	@%p53 bra 	$L__BB6_73;

	// begin inline asm
	mov.u64 %rd61, %globaltimer;
	// end inline asm
	sub.s64 	%rd15, %rd61, %rd35;
	setp.lt.s64 	%p54, %rd15, 4000000;
	@%p54 bra 	$L__BB6_71;
	bra.uni 	$L__BB6_70;

$L__BB6_71:
	setp.lt.s64 	%p55, %rd15, 40000;
	@%p55 bra 	$L__BB6_16;

	shr.s64 	%rd62, %rd15, 63;
	shr.u64 	%rd63, %rd62, 62;
	add.s64 	%rd64, %rd15, %rd63;
	shr.u64 	%rd65, %rd64, 2;
	cvt.u32.u64 	%r176, %rd65;
	// begin inline asm
	nanosleep.u32 %r176;
	// end inline asm
	bra.uni 	$L__BB6_16;

$L__BB6_70:
	mov.u32 	%r175, 1000000;
	// begin inline asm
	nanosleep.u32 %r175;
	// end inline asm
	bra.uni 	$L__BB6_16;

$L__BB6_17:
	@%p8 bra 	$L__BB6_23;

	add.s32 	%r23, %r15, %r13;
	mov.u32 	%r183, %r7;

$L__BB6_19:
	add.s32 	%r25, %r23, %r183;
	setp.gt.s32 	%p15, %r25, -1;
	setp.lt.s32 	%p16, %r25, %r64;
	and.pred  	%p17, %p15, %p16;
	add.s32 	%r108, %r183, %r5;
	shl.b32 	%r109, %r108, 2;
	add.s32 	%r26, %r11, %r109;
	@%p17 bra 	$L__BB6_21;
	bra.uni 	$L__BB6_20;

$L__BB6_21:
	mul.wide.s32 	%rd37, %r25, 4;
	add.s64 	%rd36, %rd4, %rd37;
	// begin inline asm
	cp.async.ca.shared.global [%r26], [%rd36], 4, 4;
	// end inline asm
	bra.uni 	$L__BB6_22;

$L__BB6_20:
	mov.u32 	%r110, 0;
	st.shared.u32 	[%r26], %r110;

$L__BB6_22:
	add.s32 	%r183, %r183, %r1;
	setp.lt.s32 	%p18, %r183, %r5;
	@%p18 bra 	$L__BB6_19;

$L__BB6_23:
	add.s32 	%r113, %r98, 16;
	// begin inline asm
	cp.async.mbarrier.arrive.shared.b64 [%r113];
	// end inline asm
	// begin inline asm
	mbarrier.arrive.shared.b64                                  %rd38,  [%r113];           // 1. 
	// end inline asm
	setp.ge.s32 	%p19, %r185, %r64;
	@%p19 bra 	$L__BB6_52;

	shl.b32 	%r28, %r7, 1;
	shl.b64 	%rd39, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd39;
	shl.b32 	%r116, %r12, 9;
	or.b32  	%r117, %r116, 1;
	sub.s32 	%r29, %r117, %r4;
	max.s32 	%r118, %r4, 1;
	add.s32 	%r30, %r118, -1;
	and.b32  	%r31, %r118, 3;
	sub.s32 	%r32, %r118, %r31;
	mov.u16 	%rs34, 244;
	mov.u16 	%rs32, 0;
	mov.u32 	%r115, 0;
	mov.u32 	%r184, %r115;
	mov.u16 	%rs33, %rs32;

$L__BB6_25:
	and.b16  	%rs15, %rs33, 255;
	mul.wide.u16 	%r120, %rs15, 16;
	and.b16  	%rs4, %rs34, 2;
	shr.u16 	%rs16, %rs4, 1;
	// begin inline asm
	mov.u64 %rd40, %globaltimer;
	// end inline asm
	add.s32 	%r35, %r98, %r120;
	cvt.u32.u16 	%r36, %rs16;
	mov.u32 	%r186, %r115;
	bra.uni 	$L__BB6_26;

$L__BB6_67:
	add.s32 	%r186, %r186, 1;

$L__BB6_26:
	// begin inline asm
	{.reg .pred %p;mbarrier.test_wait.parity.shared.b64 %p, [%r35], %r36;selp.u16 %rs17, 1, 0, %p;}
	// end inline asm
	setp.eq.s16 	%p20, %rs17, 0;
	@%p20 bra 	$L__BB6_62;
	bra.uni 	$L__BB6_27;

$L__BB6_62:
	setp.lt.s32 	%p50, %r186, 16;
	@%p50 bra 	$L__BB6_67;

	// begin inline asm
	mov.u64 %rd56, %globaltimer;
	// end inline asm
	sub.s64 	%rd14, %rd56, %rd40;
	setp.lt.s64 	%p51, %rd14, 4000000;
	@%p51 bra 	$L__BB6_65;
	bra.uni 	$L__BB6_64;

$L__BB6_65:
	setp.lt.s64 	%p52, %rd14, 40000;
	@%p52 bra 	$L__BB6_26;

	shr.s64 	%rd57, %rd14, 63;
	shr.u64 	%rd58, %rd57, 62;
	add.s64 	%rd59, %rd14, %rd58;
	shr.u64 	%rd60, %rd59, 2;
	cvt.u32.u64 	%r174, %rd60;
	// begin inline asm
	nanosleep.u32 %r174;
	// end inline asm
	bra.uni 	$L__BB6_26;

$L__BB6_64:
	mov.u32 	%r173, 1000000;
	// begin inline asm
	nanosleep.u32 %r173;
	// end inline asm
	bra.uni 	$L__BB6_26;

$L__BB6_27:
	bar.sync 	0;
	mul.lo.s32 	%r38, %r184, %r5;
	add.s32 	%r39, %r185, %r28;
	setp.ge.s32 	%p21, %r39, %r64;
	@%p21 bra 	$L__BB6_42;

	setp.lt.s32 	%p22, %r3, 2;
	mov.f32 	%f135, 0f00000000;
	mov.f32 	%f136, %f135;
	@%p22 bra 	$L__BB6_36;

	setp.lt.u32 	%p23, %r30, 3;
	mov.f32 	%f131, 0f00000000;
	mov.u32 	%r189, 0;
	mov.f32 	%f136, %f131;
	mov.f32 	%f133, %f131;
	mov.f32 	%f135, %f131;
	@%p23 bra 	$L__BB6_32;

	mov.f32 	%f42, 0f00000000;
	mov.u32 	%r189, 0;
	mov.f32 	%f131, %f42;
	mov.f32 	%f136, %f42;
	mov.f32 	%f133, %f42;
	mov.f32 	%f135, %f42;
	mov.u32 	%r188, %r32;

$L__BB6_31:
	.pragma "nounroll";
	shl.b32 	%r126, %r189, 2;
	add.s32 	%r128, %r93, %r126;
	ld.shared.v4.f32 	{%f43, %f44, %f45, %f46}, [%r128];
	add.s32 	%r129, %r189, %r28;
	add.s32 	%r130, %r129, %r38;
	shl.b32 	%r131, %r130, 2;
	add.s32 	%r132, %r11, %r131;
	ld.shared.f32 	%f51, [%r132];
	fma.rn.ftz.f32 	%f53, %f51, %f43, %f42;
	sub.ftz.f32 	%f54, %f53, %f133;
	add.ftz.f32 	%f55, %f135, %f54;
	sub.ftz.f32 	%f56, %f55, %f135;
	sub.ftz.f32 	%f57, %f56, %f54;
	ld.shared.f32 	%f58, [%r132+4];
	fma.rn.ftz.f32 	%f59, %f58, %f43, %f42;
	sub.ftz.f32 	%f60, %f59, %f131;
	add.ftz.f32 	%f61, %f136, %f60;
	sub.ftz.f32 	%f62, %f61, %f136;
	sub.ftz.f32 	%f63, %f62, %f60;
	fma.rn.ftz.f32 	%f64, %f58, %f44, %f42;
	sub.ftz.f32 	%f65, %f64, %f57;
	add.ftz.f32 	%f66, %f55, %f65;
	sub.ftz.f32 	%f67, %f66, %f55;
	sub.ftz.f32 	%f68, %f67, %f65;
	ld.shared.f32 	%f69, [%r132+8];
	fma.rn.ftz.f32 	%f70, %f69, %f44, %f42;
	sub.ftz.f32 	%f71, %f70, %f63;
	add.ftz.f32 	%f72, %f61, %f71;
	sub.ftz.f32 	%f73, %f72, %f61;
	sub.ftz.f32 	%f74, %f73, %f71;
	fma.rn.ftz.f32 	%f75, %f69, %f45, %f42;
	sub.ftz.f32 	%f76, %f75, %f68;
	add.ftz.f32 	%f77, %f66, %f76;
	sub.ftz.f32 	%f78, %f77, %f66;
	sub.ftz.f32 	%f79, %f78, %f76;
	ld.shared.f32 	%f80, [%r132+12];
	fma.rn.ftz.f32 	%f81, %f80, %f45, %f42;
	sub.ftz.f32 	%f82, %f81, %f74;
	add.ftz.f32 	%f83, %f72, %f82;
	sub.ftz.f32 	%f84, %f83, %f72;
	sub.ftz.f32 	%f85, %f84, %f82;
	fma.rn.ftz.f32 	%f86, %f80, %f46, %f42;
	sub.ftz.f32 	%f87, %f86, %f79;
	add.ftz.f32 	%f135, %f77, %f87;
	sub.ftz.f32 	%f88, %f135, %f77;
	sub.ftz.f32 	%f133, %f88, %f87;
	ld.shared.f32 	%f89, [%r132+16];
	fma.rn.ftz.f32 	%f90, %f89, %f46, %f42;
	sub.ftz.f32 	%f91, %f90, %f85;
	add.ftz.f32 	%f136, %f83, %f91;
	sub.ftz.f32 	%f92, %f136, %f83;
	sub.ftz.f32 	%f131, %f92, %f91;
	add.s32 	%r189, %r189, 4;
	add.s32 	%r188, %r188, -4;
	setp.ne.s32 	%p24, %r188, 0;
	@%p24 bra 	$L__BB6_31;

$L__BB6_32:
	setp.eq.s32 	%p25, %r31, 0;
	@%p25 bra 	$L__BB6_36;

	setp.eq.s32 	%p26, %r31, 1;
	add.s32 	%r133, %r189, %r28;
	add.s32 	%r134, %r133, %r38;
	shl.b32 	%r135, %r134, 2;
	add.s32 	%r45, %r11, %r135;
	ld.shared.f32 	%f93, [%r45];
	shl.b32 	%r136, %r189, 2;
	add.s32 	%r46, %r93, %r136;
	ld.shared.f32 	%f94, [%r46];
	mov.f32 	%f95, 0f00000000;
	fma.rn.ftz.f32 	%f96, %f93, %f94, %f95;
	sub.ftz.f32 	%f97, %f96, %f133;
	add.ftz.f32 	%f15, %f135, %f97;
	sub.ftz.f32 	%f98, %f15, %f135;
	sub.ftz.f32 	%f16, %f98, %f97;
	ld.shared.f32 	%f99, [%r45+4];
	fma.rn.ftz.f32 	%f100, %f99, %f94, %f95;
	sub.ftz.f32 	%f101, %f100, %f131;
	add.ftz.f32 	%f17, %f136, %f101;
	sub.ftz.f32 	%f102, %f17, %f136;
	sub.ftz.f32 	%f18, %f102, %f101;
	mov.f32 	%f135, %f15;
	mov.f32 	%f136, %f17;
	@%p26 bra 	$L__BB6_36;

	setp.eq.s32 	%p27, %r31, 2;
	ld.shared.f32 	%f103, [%r45+4];
	ld.shared.f32 	%f104, [%r46+4];
	fma.rn.ftz.f32 	%f106, %f103, %f104, %f95;
	sub.ftz.f32 	%f107, %f106, %f16;
	add.ftz.f32 	%f135, %f15, %f107;
	sub.ftz.f32 	%f108, %f135, %f15;
	sub.ftz.f32 	%f20, %f108, %f107;
	ld.shared.f32 	%f109, [%r45+8];
	fma.rn.ftz.f32 	%f110, %f109, %f104, %f95;
	sub.ftz.f32 	%f111, %f110, %f18;
	add.ftz.f32 	%f136, %f17, %f111;
	sub.ftz.f32 	%f112, %f136, %f17;
	sub.ftz.f32 	%f22, %f112, %f111;
	@%p27 bra 	$L__BB6_36;

	ld.shared.f32 	%f113, [%r45+8];
	ld.shared.f32 	%f114, [%r46+8];
	mov.f32 	%f115, 0f00000000;
	fma.rn.ftz.f32 	%f116, %f113, %f114, %f115;
	sub.ftz.f32 	%f117, %f116, %f20;
	add.ftz.f32 	%f135, %f135, %f117;
	ld.shared.f32 	%f118, [%r45+12];
	fma.rn.ftz.f32 	%f119, %f118, %f114, %f115;
	sub.ftz.f32 	%f120, %f119, %f22;
	add.ftz.f32 	%f136, %f136, %f120;

$L__BB6_36:
	setp.lt.s32 	%p28, %r39, %r6;
	mov.f32 	%f138, 0f7FC00000;
	mov.f32 	%f137, %f138;
	@%p28 bra 	$L__BB6_38;

	ld.global.nc.f32 	%f122, [%rd9];
	mul.rn.ftz.f32 	%f137, %f135, %f122;

$L__BB6_38:
	add.s32 	%r138, %r39, 1;
	setp.ge.s32 	%p29, %r138, %r64;
	setp.lt.s32 	%p30, %r138, %r6;
	or.pred  	%p31, %p29, %p30;
	@%p31 bra 	$L__BB6_40;

	ld.global.nc.f32 	%f124, [%rd9];
	mul.rn.ftz.f32 	%f138, %f136, %f124;

$L__BB6_40:
	add.s32 	%r140, %r39, %r16;
	mul.wide.s32 	%rd41, %r140, 4;
	add.s64 	%rd11, %rd6, %rd41;
	st.global.f32 	[%rd11], %f137;
	@%p29 bra 	$L__BB6_42;

	st.global.f32 	[%rd11+4], %f138;

$L__BB6_42:
	bar.sync 	0;
	add.s32 	%r141, %r35, 8;
	// begin inline asm
	mbarrier.arrive.shared.b64                                  %rd42,  [%r141];           // 1. 
	// end inline asm
	add.s16 	%rs18, %rs33, 1;
	and.b16  	%rs19, %rs18, 255;
	setp.eq.s16 	%p33, %rs19, 2;
	setp.eq.s16 	%p34, %rs4, 0;
	selp.u16 	%rs20, 1, 0, %p34;
	shl.b16 	%rs21, %rs20, 1;
	and.b16  	%rs22, %rs34, -3;
	or.b16  	%rs23, %rs21, %rs22;
	selp.b16 	%rs5, %rs23, %rs34, %p33;
	selp.b16 	%rs33, 0, %rs18, %p33;
	and.b16  	%rs24, %rs32, 255;
	mul.wide.u16 	%r47, %rs24, 16;
	and.b16  	%rs7, %rs5, 1;
	// begin inline asm
	mov.u64 %rd43, %globaltimer;
	// end inline asm
	add.s32 	%r144, %r98, %r47;
	add.s32 	%r48, %r144, 8;
	cvt.u32.u16 	%r49, %rs7;
	mov.u32 	%r190, 0;
	bra.uni 	$L__BB6_43;

$L__BB6_61:
	add.s32 	%r190, %r190, 1;

$L__BB6_43:
	// begin inline asm
	{.reg .pred %p;mbarrier.test_wait.parity.shared.b64 %p, [%r48], %r49;selp.u16 %rs25, 1, 0, %p;}
	// end inline asm
	setp.eq.s16 	%p35, %rs25, 0;
	@%p35 bra 	$L__BB6_56;
	bra.uni 	$L__BB6_44;

$L__BB6_56:
	setp.lt.s32 	%p47, %r190, 16;
	@%p47 bra 	$L__BB6_61;

	// begin inline asm
	mov.u64 %rd51, %globaltimer;
	// end inline asm
	sub.s64 	%rd13, %rd51, %rd43;
	setp.lt.s64 	%p48, %rd13, 4000000;
	@%p48 bra 	$L__BB6_59;
	bra.uni 	$L__BB6_58;

$L__BB6_59:
	setp.lt.s64 	%p49, %rd13, 40000;
	@%p49 bra 	$L__BB6_43;

	shr.s64 	%rd52, %rd13, 63;
	shr.u64 	%rd53, %rd52, 62;
	add.s64 	%rd54, %rd13, %rd53;
	shr.u64 	%rd55, %rd54, 2;
	cvt.u32.u64 	%r172, %rd55;
	// begin inline asm
	nanosleep.u32 %r172;
	// end inline asm
	bra.uni 	$L__BB6_43;

$L__BB6_58:
	mov.u32 	%r171, 1000000;
	// begin inline asm
	nanosleep.u32 %r171;
	// end inline asm
	bra.uni 	$L__BB6_43;

$L__BB6_44:
	@%p8 bra 	$L__BB6_50;

	add.s32 	%r51, %r29, %r185;
	mov.u32 	%r191, %r7;

$L__BB6_46:
	add.s32 	%r53, %r51, %r191;
	setp.gt.s32 	%p37, %r53, -1;
	setp.lt.s32 	%p38, %r53, %r64;
	and.pred  	%p39, %p37, %p38;
	add.s32 	%r147, %r191, %r38;
	shl.b32 	%r148, %r147, 2;
	add.s32 	%r54, %r11, %r148;
	@%p39 bra 	$L__BB6_48;
	bra.uni 	$L__BB6_47;

$L__BB6_48:
	mul.wide.s32 	%rd45, %r53, 4;
	add.s64 	%rd44, %rd4, %rd45;
	// begin inline asm
	cp.async.ca.shared.global [%r54], [%rd44], 4, 4;
	// end inline asm
	bra.uni 	$L__BB6_49;

$L__BB6_47:
	mov.u32 	%r149, 0;
	st.shared.u32 	[%r54], %r149;

$L__BB6_49:
	add.s32 	%r191, %r191, %r1;
	setp.lt.s32 	%p40, %r191, %r5;
	@%p40 bra 	$L__BB6_46;

$L__BB6_50:
	// begin inline asm
	cp.async.mbarrier.arrive.shared.b64 [%r144];
	// end inline asm
	// begin inline asm
	mbarrier.arrive.shared.b64                                  %rd46,  [%r144];           // 1. 
	// end inline asm
	add.s16 	%rs26, %rs32, 1;
	and.b16  	%rs27, %rs26, 255;
	setp.eq.s16 	%p41, %rs27, 2;
	setp.eq.s16 	%p42, %rs7, 0;
	selp.u16 	%rs28, 1, 0, %p42;
	and.b16  	%rs29, %rs5, -2;
	or.b16  	%rs30, %rs29, %rs28;
	selp.b16 	%rs34, %rs30, %rs5, %p41;
	selp.b16 	%rs32, 0, %rs26, %p41;
	add.s32 	%r154, %r184, 1;
	shr.u32 	%r155, %r154, 31;
	add.s32 	%r156, %r154, %r155;
	and.b32  	%r157, %r156, -2;
	sub.s32 	%r184, %r154, %r157;
	add.s32 	%r185, %r185, %r13;
	setp.lt.s32 	%p43, %r185, %r64;
	@%p43 bra 	$L__BB6_25;

	and.b16  	%rs31, %rs34, 4;
	setp.eq.s16 	%p44, %rs31, 0;
	@%p44 bra 	$L__BB6_55;

$L__BB6_52:
	// begin inline asm
	activemask.b32 %r158;
	// end inline asm
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r98;
	  cvta.shared.u64 	%rd47, %tmp; }
	add.s64 	%rd48, %rd47, 32;
	match.any.sync.b64 	%r58, %rd48, %r158;
	brev.b32 	%r161, %r58;
	bfind.shiftamt.u32 	%r162, %r161;
	// begin inline asm
	mov.u32 %r159, %laneid;
	// end inline asm
	setp.ne.s32 	%p45, %r159, %r162;
	@%p45 bra 	$L__BB6_55;

	popc.b32 	%r165, %r58;
	neg.s32 	%r164, %r165;
	// begin inline asm
	fence.sc.cta;
	// end inline asm
	// begin inline asm
	atom.add.acquire.cta.u32 %r163,[%rd48],%r164;
	// end inline asm
	setp.ne.s32 	%p46, %r163, %r165;
	@%p46 bra 	$L__BB6_55;

	// begin inline asm
	mbarrier.inval.shared.b64 [%r98];
	// end inline asm
	// begin inline asm
	mbarrier.inval.shared.b64 [%r96];
	// end inline asm
	// begin inline asm
	mbarrier.inval.shared.b64 [%r113];
	// end inline asm
	// begin inline asm
	mbarrier.inval.shared.b64 [%r105];
	// end inline asm

$L__BB6_55:
	ret;

}
	// .globl	cwma_multi_series_one_param_time_major_f32
.visible .entry cwma_multi_series_one_param_time_major_f32(
	.param .u64 cwma_multi_series_one_param_time_major_f32_param_0,
	.param .u64 cwma_multi_series_one_param_time_major_f32_param_1,
	.param .u32 cwma_multi_series_one_param_time_major_f32_param_2,
	.param .f32 cwma_multi_series_one_param_time_major_f32_param_3,
	.param .u32 cwma_multi_series_one_param_time_major_f32_param_4,
	.param .u32 cwma_multi_series_one_param_time_major_f32_param_5,
	.param .u64 cwma_multi_series_one_param_time_major_f32_param_6,
	.param .u64 cwma_multi_series_one_param_time_major_f32_param_7
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<92>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd13, [cwma_multi_series_one_param_time_major_f32_param_0];
	ld.param.u64 	%rd11, [cwma_multi_series_one_param_time_major_f32_param_1];
	ld.param.u32 	%r42, [cwma_multi_series_one_param_time_major_f32_param_2];
	ld.param.f32 	%f14, [cwma_multi_series_one_param_time_major_f32_param_3];
	ld.param.u32 	%r40, [cwma_multi_series_one_param_time_major_f32_param_4];
	ld.param.u32 	%r41, [cwma_multi_series_one_param_time_major_f32_param_5];
	ld.param.u64 	%rd12, [cwma_multi_series_one_param_time_major_f32_param_6];
	ld.param.u64 	%rd14, [cwma_multi_series_one_param_time_major_f32_param_7];
	cvta.to.global.u64 	%rd1, %rd13;
	cvta.to.global.u64 	%rd2, %rd14;
	add.s32 	%r43, %r42, -1;
	setp.gt.s32 	%p1, %r42, 0;
	selp.b32 	%r1, %r43, 0, %p1;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p2, %r2, %r1;
	@%p2 bra 	$L__BB7_3;

	mov.u32 	%r3, %ntid.x;
	cvta.to.global.u64 	%rd3, %rd11;
	mov.u32 	%r82, %r2;

$L__BB7_2:
	mul.wide.s32 	%rd15, %r82, 4;
	add.s64 	%rd16, %rd3, %rd15;
	ld.global.nc.f32 	%f15, [%rd16];
	shl.b32 	%r44, %r82, 2;
	mov.u32 	%r45, shared_weights;
	add.s32 	%r46, %r45, %r44;
	st.shared.f32 	[%r46], %f15;
	add.s32 	%r82, %r82, %r3;
	setp.lt.s32 	%p3, %r82, %r1;
	@%p3 bra 	$L__BB7_2;

$L__BB7_3:
	bar.sync 	0;
	mov.u32 	%r6, %ctaid.y;
	setp.ge.s32 	%p4, %r6, %r40;
	@%p4 bra 	$L__BB7_38;

	cvta.to.global.u64 	%rd17, %rd12;
	mul.wide.s32 	%rd18, %r6, 4;
	add.s64 	%rd19, %rd17, %rd18;
	ld.global.nc.u32 	%r47, [%rd19];
	add.s32 	%r7, %r47, %r1;
	mov.u32 	%r48, %ntid.x;
	mov.u32 	%r49, %ctaid.x;
	mad.lo.s32 	%r86, %r49, %r48, %r2;
	mov.u32 	%r50, %nctaid.x;
	mul.lo.s32 	%r9, %r50, %r48;
	setp.ge.s32 	%p5, %r86, %r41;
	@%p5 bra 	$L__BB7_38;

	setp.gt.s32 	%p6, %r1, 0;
	@%p6 bra 	$L__BB7_26;
	bra.uni 	$L__BB7_6;

$L__BB7_26:
	add.s32 	%r26, %r1, -1;
	and.b32  	%r27, %r1, 3;
	sub.s32 	%r28, %r1, %r27;
	mul.wide.s32 	%rd9, %r40, 4;
	mov.u32 	%r65, 1;
	sub.s32 	%r29, %r65, %r1;

$L__BB7_27:
	mad.lo.s32 	%r66, %r86, %r40, %r6;
	mul.wide.s32 	%rd25, %r66, 4;
	add.s64 	%rd10, %rd2, %rd25;
	setp.lt.s32 	%p16, %r86, %r7;
	@%p16 bra 	$L__BB7_36;
	bra.uni 	$L__BB7_28;

$L__BB7_36:
	mov.u32 	%r81, 2143289344;
	st.global.u32 	[%rd10], %r81;
	bra.uni 	$L__BB7_37;

$L__BB7_28:
	setp.lt.u32 	%p17, %r26, 3;
	add.s32 	%r31, %r29, %r86;
	mov.f32 	%f83, 0f00000000;
	mov.u32 	%r91, 0;
	mov.f32 	%f85, %f83;
	@%p17 bra 	$L__BB7_31;

	mov.f32 	%f30, 0f00000000;
	mov.u32 	%r91, 0;
	mov.f32 	%f83, %f30;
	mov.f32 	%f85, %f30;
	mov.u32 	%r90, %r28;

$L__BB7_30:
	.pragma "nounroll";
	add.s32 	%r69, %r31, %r91;
	mad.lo.s32 	%r70, %r69, %r40, %r6;
	mul.wide.s32 	%rd26, %r70, 4;
	add.s64 	%rd27, %rd1, %rd26;
	shl.b32 	%r71, %r91, 2;
	mov.u32 	%r72, shared_weights;
	add.s32 	%r73, %r72, %r71;
	ld.shared.v4.f32 	{%f31, %f32, %f33, %f34}, [%r73];
	ld.global.nc.f32 	%f39, [%rd27];
	fma.rn.ftz.f32 	%f41, %f39, %f31, %f30;
	sub.ftz.f32 	%f42, %f41, %f83;
	add.ftz.f32 	%f43, %f85, %f42;
	sub.ftz.f32 	%f44, %f43, %f85;
	sub.ftz.f32 	%f45, %f44, %f42;
	add.s64 	%rd28, %rd27, %rd9;
	ld.global.nc.f32 	%f46, [%rd28];
	fma.rn.ftz.f32 	%f47, %f46, %f32, %f30;
	sub.ftz.f32 	%f48, %f47, %f45;
	add.ftz.f32 	%f49, %f43, %f48;
	sub.ftz.f32 	%f50, %f49, %f43;
	sub.ftz.f32 	%f51, %f50, %f48;
	add.s64 	%rd29, %rd28, %rd9;
	ld.global.nc.f32 	%f52, [%rd29];
	fma.rn.ftz.f32 	%f53, %f52, %f33, %f30;
	sub.ftz.f32 	%f54, %f53, %f51;
	add.ftz.f32 	%f55, %f49, %f54;
	sub.ftz.f32 	%f56, %f55, %f49;
	sub.ftz.f32 	%f57, %f56, %f54;
	add.s64 	%rd30, %rd29, %rd9;
	ld.global.nc.f32 	%f58, [%rd30];
	fma.rn.ftz.f32 	%f59, %f58, %f34, %f30;
	sub.ftz.f32 	%f60, %f59, %f57;
	add.ftz.f32 	%f85, %f55, %f60;
	sub.ftz.f32 	%f61, %f85, %f55;
	sub.ftz.f32 	%f83, %f61, %f60;
	add.s32 	%r91, %r91, 4;
	add.s32 	%r90, %r90, -4;
	setp.ne.s32 	%p18, %r90, 0;
	@%p18 bra 	$L__BB7_30;

$L__BB7_31:
	setp.eq.s32 	%p19, %r27, 0;
	@%p19 bra 	$L__BB7_35;

	setp.eq.s32 	%p20, %r27, 1;
	add.s32 	%r37, %r31, %r91;
	mad.lo.s32 	%r74, %r37, %r40, %r6;
	mul.wide.s32 	%rd31, %r74, 4;
	add.s64 	%rd32, %rd1, %rd31;
	shl.b32 	%r75, %r91, 2;
	mov.u32 	%r76, shared_weights;
	add.s32 	%r38, %r76, %r75;
	ld.shared.f32 	%f62, [%r38];
	ld.global.nc.f32 	%f63, [%rd32];
	mov.f32 	%f64, 0f00000000;
	fma.rn.ftz.f32 	%f65, %f63, %f62, %f64;
	sub.ftz.f32 	%f66, %f65, %f83;
	add.ftz.f32 	%f8, %f85, %f66;
	sub.ftz.f32 	%f67, %f8, %f85;
	sub.ftz.f32 	%f9, %f67, %f66;
	mov.f32 	%f85, %f8;
	@%p20 bra 	$L__BB7_35;

	setp.eq.s32 	%p21, %r27, 2;
	add.s32 	%r77, %r37, 1;
	mad.lo.s32 	%r78, %r77, %r40, %r6;
	mul.wide.s32 	%rd33, %r78, 4;
	add.s64 	%rd34, %rd1, %rd33;
	ld.shared.f32 	%f68, [%r38+4];
	ld.global.nc.f32 	%f69, [%rd34];
	fma.rn.ftz.f32 	%f71, %f69, %f68, %f64;
	sub.ftz.f32 	%f72, %f71, %f9;
	add.ftz.f32 	%f85, %f8, %f72;
	sub.ftz.f32 	%f73, %f85, %f8;
	sub.ftz.f32 	%f11, %f73, %f72;
	@%p21 bra 	$L__BB7_35;

	add.s32 	%r79, %r37, 2;
	mad.lo.s32 	%r80, %r79, %r40, %r6;
	mul.wide.s32 	%rd35, %r80, 4;
	add.s64 	%rd36, %rd1, %rd35;
	ld.shared.f32 	%f74, [%r38+8];
	ld.global.nc.f32 	%f75, [%rd36];
	mov.f32 	%f76, 0f00000000;
	fma.rn.ftz.f32 	%f77, %f75, %f74, %f76;
	sub.ftz.f32 	%f78, %f77, %f11;
	add.ftz.f32 	%f85, %f85, %f78;

$L__BB7_35:
	mul.rn.ftz.f32 	%f79, %f85, %f14;
	st.global.f32 	[%rd10], %f79;

$L__BB7_37:
	add.s32 	%r86, %r86, %r9;
	setp.lt.s32 	%p22, %r86, %r41;
	@%p22 bra 	$L__BB7_27;
	bra.uni 	$L__BB7_38;

$L__BB7_6:
	add.s32 	%r51, %r9, %r41;
	add.s32 	%r52, %r86, %r9;
	not.b32 	%r53, %r52;
	add.s32 	%r54, %r51, %r53;
	div.u32 	%r10, %r54, %r9;
	add.s32 	%r55, %r10, 1;
	and.b32  	%r85, %r55, 3;
	setp.eq.s32 	%p7, %r85, 0;
	@%p7 bra 	$L__BB7_12;

	mad.lo.s32 	%r83, %r40, %r86, %r6;
	mul.lo.s32 	%r13, %r9, %r40;

$L__BB7_8:
	.pragma "nounroll";
	mul.wide.s32 	%rd20, %r83, 4;
	add.s64 	%rd4, %rd2, %rd20;
	setp.lt.s32 	%p8, %r86, %r7;
	@%p8 bra 	$L__BB7_10;
	bra.uni 	$L__BB7_9;

$L__BB7_10:
	mov.u32 	%r56, 2143289344;
	st.global.u32 	[%rd4], %r56;
	bra.uni 	$L__BB7_11;

$L__BB7_9:
	mov.f32 	%f16, 0f00000000;
	mul.rn.ftz.f32 	%f17, %f16, %f14;
	st.global.f32 	[%rd4], %f17;

$L__BB7_11:
	add.s32 	%r86, %r86, %r9;
	add.s32 	%r83, %r83, %r13;
	add.s32 	%r85, %r85, -1;
	setp.ne.s32 	%p9, %r85, 0;
	@%p9 bra 	$L__BB7_8;

$L__BB7_12:
	setp.lt.u32 	%p10, %r10, 3;
	@%p10 bra 	$L__BB7_38;

$L__BB7_13:
	mad.lo.s32 	%r57, %r86, %r40, %r6;
	mul.wide.s32 	%rd21, %r57, 4;
	add.s64 	%rd5, %rd2, %rd21;
	setp.lt.s32 	%p11, %r86, %r7;
	@%p11 bra 	$L__BB7_15;
	bra.uni 	$L__BB7_14;

$L__BB7_15:
	mov.u32 	%r58, 2143289344;
	st.global.u32 	[%rd5], %r58;
	bra.uni 	$L__BB7_16;

$L__BB7_14:
	mov.f32 	%f18, 0f00000000;
	mul.rn.ftz.f32 	%f19, %f18, %f14;
	st.global.f32 	[%rd5], %f19;

$L__BB7_16:
	add.s32 	%r22, %r86, %r9;
	mad.lo.s32 	%r59, %r22, %r40, %r6;
	setp.lt.s32 	%p12, %r22, %r7;
	mul.wide.s32 	%rd22, %r59, 4;
	add.s64 	%rd6, %rd2, %rd22;
	@%p12 bra 	$L__BB7_18;
	bra.uni 	$L__BB7_17;

$L__BB7_18:
	mov.u32 	%r60, 2143289344;
	st.global.u32 	[%rd6], %r60;
	bra.uni 	$L__BB7_19;

$L__BB7_17:
	mov.f32 	%f20, 0f00000000;
	mul.rn.ftz.f32 	%f21, %f20, %f14;
	st.global.f32 	[%rd6], %f21;

$L__BB7_19:
	add.s32 	%r23, %r22, %r9;
	mad.lo.s32 	%r61, %r23, %r40, %r6;
	setp.lt.s32 	%p13, %r23, %r7;
	mul.wide.s32 	%rd23, %r61, 4;
	add.s64 	%rd7, %rd2, %rd23;
	@%p13 bra 	$L__BB7_21;
	bra.uni 	$L__BB7_20;

$L__BB7_21:
	mov.u32 	%r62, 2143289344;
	st.global.u32 	[%rd7], %r62;
	bra.uni 	$L__BB7_22;

$L__BB7_20:
	mov.f32 	%f22, 0f00000000;
	mul.rn.ftz.f32 	%f23, %f22, %f14;
	st.global.f32 	[%rd7], %f23;

$L__BB7_22:
	add.s32 	%r24, %r23, %r9;
	mad.lo.s32 	%r63, %r24, %r40, %r6;
	setp.lt.s32 	%p14, %r24, %r7;
	mul.wide.s32 	%rd24, %r63, 4;
	add.s64 	%rd8, %rd2, %rd24;
	@%p14 bra 	$L__BB7_24;
	bra.uni 	$L__BB7_23;

$L__BB7_24:
	mov.u32 	%r64, 2143289344;
	st.global.u32 	[%rd8], %r64;
	bra.uni 	$L__BB7_25;

$L__BB7_23:
	mov.f32 	%f24, 0f00000000;
	mul.rn.ftz.f32 	%f25, %f24, %f14;
	st.global.f32 	[%rd8], %f25;

$L__BB7_25:
	add.s32 	%r86, %r24, %r9;
	setp.lt.s32 	%p15, %r86, %r41;
	@%p15 bra 	$L__BB7_13;

$L__BB7_38:
	ret;

}
	// .globl	cwma_ms1p_tiled_f32_tx128_ty2
.visible .entry cwma_ms1p_tiled_f32_tx128_ty2(
	.param .u64 cwma_ms1p_tiled_f32_tx128_ty2_param_0,
	.param .u64 cwma_ms1p_tiled_f32_tx128_ty2_param_1,
	.param .u32 cwma_ms1p_tiled_f32_tx128_ty2_param_2,
	.param .f32 cwma_ms1p_tiled_f32_tx128_ty2_param_3,
	.param .u32 cwma_ms1p_tiled_f32_tx128_ty2_param_4,
	.param .u32 cwma_ms1p_tiled_f32_tx128_ty2_param_5,
	.param .u64 cwma_ms1p_tiled_f32_tx128_ty2_param_6,
	.param .u64 cwma_ms1p_tiled_f32_tx128_ty2_param_7
)
{
	.reg .pred 	%p<23>;
	.reg .f32 	%f<71>;
	.reg .b32 	%r<91>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd4, [cwma_ms1p_tiled_f32_tx128_ty2_param_0];
	ld.param.u64 	%rd5, [cwma_ms1p_tiled_f32_tx128_ty2_param_1];
	ld.param.u32 	%r44, [cwma_ms1p_tiled_f32_tx128_ty2_param_2];
	ld.param.f32 	%f15, [cwma_ms1p_tiled_f32_tx128_ty2_param_3];
	ld.param.u32 	%r45, [cwma_ms1p_tiled_f32_tx128_ty2_param_4];
	ld.param.u32 	%r46, [cwma_ms1p_tiled_f32_tx128_ty2_param_5];
	ld.param.u64 	%rd6, [cwma_ms1p_tiled_f32_tx128_ty2_param_6];
	ld.param.u64 	%rd7, [cwma_ms1p_tiled_f32_tx128_ty2_param_7];
	setp.gt.s32 	%p1, %r44, 1;
	add.s32 	%r47, %r44, -1;
	selp.b32 	%r1, %r47, 0, %p1;
	mov.u32 	%r48, %ctaid.x;
	shl.b32 	%r2, %r48, 7;
	mov.u32 	%r49, %ctaid.y;
	shl.b32 	%r3, %r49, 1;
	setp.ge.s32 	%p2, %r2, %r46;
	setp.ge.s32 	%p3, %r3, %r45;
	or.pred  	%p4, %p3, %p2;
	@%p4 bra 	$L__BB8_24;

	add.s32 	%r4, %r1, 127;
	shl.b32 	%r50, %r1, 2;
	add.s32 	%r51, %r50, 15;
	and.b32  	%r5, %r51, -16;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.y;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r81, %r7, %r6, %r8;
	setp.ge.s32 	%p5, %r81, %r1;
	@%p5 bra 	$L__BB8_4;

	mov.u32 	%r52, %ntid.y;
	mul.lo.s32 	%r10, %r6, %r52;
	cvta.to.global.u64 	%rd1, %rd5;

$L__BB8_3:
	mul.wide.s32 	%rd8, %r81, 4;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.nc.f32 	%f16, [%rd9];
	shl.b32 	%r53, %r81, 2;
	mov.u32 	%r54, shraw;
	add.s32 	%r55, %r54, %r53;
	st.shared.f32 	[%r55], %f16;
	add.s32 	%r81, %r81, %r10;
	setp.lt.s32 	%p6, %r81, %r1;
	@%p6 bra 	$L__BB8_3;

$L__BB8_4:
	bar.sync 	0;
	setp.lt.s32 	%p7, %r8, %r4;
	@%p7 bra 	$L__BB8_5;
	bra.uni 	$L__BB8_13;

$L__BB8_5:
	add.s32 	%r56, %r2, 1;
	sub.s32 	%r13, %r56, %r1;
	add.s32 	%r14, %r3, %r7;
	mul.lo.s32 	%r15, %r4, 3;
	mov.u32 	%r57, shraw;
	add.s32 	%r16, %r57, %r5;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r82, %r8;

$L__BB8_6:
	add.s32 	%r18, %r13, %r82;
	setp.gt.s32 	%p8, %r18, -1;
	setp.lt.s32 	%p9, %r18, %r46;
	and.pred  	%p10, %p8, %p9;
	mad.lo.s32 	%r58, %r82, 3, %r7;
	shl.b32 	%r59, %r58, 2;
	add.s32 	%r19, %r16, %r59;
	@%p10 bra 	$L__BB8_9;
	bra.uni 	$L__BB8_7;

$L__BB8_9:
	setp.ge.s32 	%p12, %r14, %r45;
	mov.f32 	%f62, 0f00000000;
	@%p12 bra 	$L__BB8_11;

	mad.lo.s32 	%r62, %r18, %r45, %r14;
	mul.wide.s32 	%rd10, %r62, 4;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.nc.f32 	%f62, [%rd11];

$L__BB8_11:
	st.shared.f32 	[%r19], %f62;
	bra.uni 	$L__BB8_12;

$L__BB8_7:
	setp.ge.s32 	%p11, %r58, %r15;
	@%p11 bra 	$L__BB8_12;

	mov.u32 	%r61, 0;
	st.shared.u32 	[%r19], %r61;

$L__BB8_12:
	add.s32 	%r82, %r82, %r6;
	setp.lt.s32 	%p13, %r82, %r4;
	@%p13 bra 	$L__BB8_6;

$L__BB8_13:
	add.s32 	%r21, %r3, %r7;
	bar.sync 	0;
	setp.ge.s32 	%p14, %r21, %r45;
	add.s32 	%r22, %r2, %r8;
	setp.ge.s32 	%p15, %r22, %r46;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	$L__BB8_24;

	cvta.to.global.u64 	%rd12, %rd6;
	mul.wide.s32 	%rd13, %r21, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.nc.u32 	%r63, [%rd14];
	add.s32 	%r64, %r63, %r1;
	mad.lo.s32 	%r65, %r22, %r45, %r21;
	setp.lt.s32 	%p17, %r22, %r64;
	cvta.to.global.u64 	%rd15, %rd7;
	mul.wide.s32 	%rd16, %r65, 4;
	add.s64 	%rd3, %rd15, %rd16;
	@%p17 bra 	$L__BB8_23;
	bra.uni 	$L__BB8_15;

$L__BB8_23:
	mov.u32 	%r80, 2143289344;
	st.global.u32 	[%rd3], %r80;
	bra.uni 	$L__BB8_24;

$L__BB8_15:
	setp.lt.s32 	%p18, %r44, 2;
	mov.f32 	%f65, 0f00000000;
	@%p18 bra 	$L__BB8_22;

	max.s32 	%r23, %r1, 1;
	add.s32 	%r67, %r23, -1;
	and.b32  	%r90, %r23, 3;
	setp.lt.u32 	%p19, %r67, 3;
	mov.f32 	%f66, 0f00000000;
	mov.u32 	%r87, 0;
	mov.f32 	%f65, %f66;
	@%p19 bra 	$L__BB8_19;

	sub.s32 	%r86, %r23, %r90;
	mad.lo.s32 	%r70, %r8, 12, %r5;
	shl.b32 	%r71, %r7, 2;
	add.s32 	%r72, %r70, %r71;
	mov.u32 	%r84, shraw;
	add.s32 	%r73, %r84, %r72;
	add.s32 	%r83, %r73, 24;
	mov.f32 	%f23, 0f00000000;
	mov.u32 	%r87, 0;
	mov.f32 	%f66, %f23;
	mov.f32 	%f65, %f23;

$L__BB8_18:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f24, %f25, %f26, %f27}, [%r84];
	ld.shared.f32 	%f32, [%r83+-24];
	fma.rn.ftz.f32 	%f34, %f32, %f24, %f23;
	sub.ftz.f32 	%f35, %f34, %f66;
	add.ftz.f32 	%f36, %f65, %f35;
	sub.ftz.f32 	%f37, %f36, %f65;
	sub.ftz.f32 	%f38, %f37, %f35;
	ld.shared.f32 	%f39, [%r83+-12];
	fma.rn.ftz.f32 	%f40, %f39, %f25, %f23;
	sub.ftz.f32 	%f41, %f40, %f38;
	add.ftz.f32 	%f42, %f36, %f41;
	sub.ftz.f32 	%f43, %f42, %f36;
	sub.ftz.f32 	%f44, %f43, %f41;
	ld.shared.f32 	%f45, [%r83];
	fma.rn.ftz.f32 	%f46, %f45, %f26, %f23;
	sub.ftz.f32 	%f47, %f46, %f44;
	add.ftz.f32 	%f48, %f42, %f47;
	sub.ftz.f32 	%f49, %f48, %f42;
	sub.ftz.f32 	%f50, %f49, %f47;
	ld.shared.f32 	%f51, [%r83+12];
	fma.rn.ftz.f32 	%f52, %f51, %f27, %f23;
	sub.ftz.f32 	%f53, %f52, %f50;
	add.ftz.f32 	%f65, %f48, %f53;
	sub.ftz.f32 	%f54, %f65, %f48;
	sub.ftz.f32 	%f66, %f54, %f53;
	add.s32 	%r87, %r87, 4;
	add.s32 	%r84, %r84, 16;
	add.s32 	%r83, %r83, 48;
	add.s32 	%r86, %r86, -4;
	setp.ne.s32 	%p20, %r86, 0;
	@%p20 bra 	$L__BB8_18;

$L__BB8_19:
	setp.eq.s32 	%p21, %r90, 0;
	@%p21 bra 	$L__BB8_22;

	shl.b32 	%r74, %r87, 2;
	mov.u32 	%r75, shraw;
	add.s32 	%r89, %r75, %r74;
	add.s32 	%r76, %r8, %r87;
	mad.lo.s32 	%r77, %r76, 12, %r5;
	shl.b32 	%r78, %r7, 2;
	add.s32 	%r79, %r77, %r78;
	add.s32 	%r88, %r75, %r79;
	mov.f32 	%f69, %f65;

$L__BB8_21:
	.pragma "nounroll";
	ld.shared.f32 	%f55, [%r89];
	ld.shared.f32 	%f56, [%r88];
	mov.f32 	%f57, 0f00000000;
	fma.rn.ftz.f32 	%f58, %f56, %f55, %f57;
	sub.ftz.f32 	%f59, %f58, %f66;
	add.ftz.f32 	%f65, %f69, %f59;
	sub.ftz.f32 	%f60, %f65, %f69;
	sub.ftz.f32 	%f66, %f60, %f59;
	add.s32 	%r89, %r89, 4;
	add.s32 	%r88, %r88, 12;
	add.s32 	%r90, %r90, -1;
	setp.ne.s32 	%p22, %r90, 0;
	mov.f32 	%f69, %f65;
	@%p22 bra 	$L__BB8_21;

$L__BB8_22:
	mul.rn.ftz.f32 	%f61, %f65, %f15;
	st.global.f32 	[%rd3], %f61;

$L__BB8_24:
	ret;

}
	// .globl	cwma_ms1p_tiled_f32_tx128_ty4
.visible .entry cwma_ms1p_tiled_f32_tx128_ty4(
	.param .u64 cwma_ms1p_tiled_f32_tx128_ty4_param_0,
	.param .u64 cwma_ms1p_tiled_f32_tx128_ty4_param_1,
	.param .u32 cwma_ms1p_tiled_f32_tx128_ty4_param_2,
	.param .f32 cwma_ms1p_tiled_f32_tx128_ty4_param_3,
	.param .u32 cwma_ms1p_tiled_f32_tx128_ty4_param_4,
	.param .u32 cwma_ms1p_tiled_f32_tx128_ty4_param_5,
	.param .u64 cwma_ms1p_tiled_f32_tx128_ty4_param_6,
	.param .u64 cwma_ms1p_tiled_f32_tx128_ty4_param_7
)
{
	.reg .pred 	%p<24>;
	.reg .f32 	%f<79>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<19>;


	ld.param.u64 	%rd7, [cwma_ms1p_tiled_f32_tx128_ty4_param_0];
	ld.param.u64 	%rd4, [cwma_ms1p_tiled_f32_tx128_ty4_param_1];
	ld.param.u32 	%r45, [cwma_ms1p_tiled_f32_tx128_ty4_param_2];
	ld.param.f32 	%f15, [cwma_ms1p_tiled_f32_tx128_ty4_param_3];
	ld.param.u32 	%r46, [cwma_ms1p_tiled_f32_tx128_ty4_param_4];
	ld.param.u32 	%r47, [cwma_ms1p_tiled_f32_tx128_ty4_param_5];
	ld.param.u64 	%rd5, [cwma_ms1p_tiled_f32_tx128_ty4_param_6];
	ld.param.u64 	%rd6, [cwma_ms1p_tiled_f32_tx128_ty4_param_7];
	cvta.to.global.u64 	%rd1, %rd7;
	setp.gt.s32 	%p1, %r45, 1;
	add.s32 	%r48, %r45, -1;
	selp.b32 	%r1, %r48, 0, %p1;
	mov.u32 	%r49, %ctaid.x;
	shl.b32 	%r2, %r49, 7;
	mov.u32 	%r50, %ctaid.y;
	shl.b32 	%r3, %r50, 2;
	setp.ge.s32 	%p2, %r2, %r47;
	setp.ge.s32 	%p3, %r3, %r46;
	or.pred  	%p4, %p3, %p2;
	@%p4 bra 	$L__BB9_26;

	add.s32 	%r4, %r1, 127;
	shl.b32 	%r51, %r1, 2;
	add.s32 	%r52, %r51, 15;
	and.b32  	%r5, %r52, -16;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.y;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r85, %r7, %r6, %r8;
	setp.ge.s32 	%p5, %r85, %r1;
	@%p5 bra 	$L__BB9_4;

	mov.u32 	%r53, %ntid.y;
	mul.lo.s32 	%r10, %r6, %r53;
	cvta.to.global.u64 	%rd2, %rd4;

$L__BB9_3:
	mul.wide.s32 	%rd8, %r85, 4;
	add.s64 	%rd9, %rd2, %rd8;
	ld.global.nc.f32 	%f16, [%rd9];
	shl.b32 	%r54, %r85, 2;
	mov.u32 	%r55, shraw;
	add.s32 	%r56, %r55, %r54;
	st.shared.f32 	[%r56], %f16;
	add.s32 	%r85, %r85, %r10;
	setp.lt.s32 	%p6, %r85, %r1;
	@%p6 bra 	$L__BB9_3;

$L__BB9_4:
	bar.sync 	0;
	setp.lt.s32 	%p7, %r8, %r4;
	@%p7 bra 	$L__BB9_5;
	bra.uni 	$L__BB9_15;

$L__BB9_5:
	mov.u32 	%r57, shraw;
	add.s32 	%r13, %r57, %r5;
	and.b32  	%r58, %r46, 3;
	add.s32 	%r59, %r2, 1;
	sub.s32 	%r14, %r59, %r1;
	or.b32  	%r15, %r58, %r7;
	mul.lo.s32 	%r16, %r4, 5;
	add.s32 	%r17, %r3, %r7;
	mov.u32 	%r86, %r8;

$L__BB9_6:
	add.s32 	%r19, %r14, %r86;
	setp.gt.s32 	%p8, %r19, -1;
	setp.lt.s32 	%p9, %r19, %r47;
	and.pred  	%p10, %p8, %p9;
	mad.lo.s32 	%r60, %r86, 5, %r7;
	shl.b32 	%r61, %r60, 2;
	add.s32 	%r20, %r13, %r61;
	@%p10 bra 	$L__BB9_9;
	bra.uni 	$L__BB9_7;

$L__BB9_9:
	setp.eq.s32 	%p12, %r15, 0;
	@%p12 bra 	$L__BB9_13;

	setp.ge.s32 	%p13, %r17, %r46;
	mov.f32 	%f70, 0f00000000;
	@%p13 bra 	$L__BB9_12;

	mad.lo.s32 	%r64, %r19, %r46, %r17;
	mul.wide.s32 	%rd10, %r64, 4;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.f32 	%f70, [%rd11];

$L__BB9_12:
	st.shared.f32 	[%r20], %f70;
	bra.uni 	$L__BB9_14;

$L__BB9_7:
	setp.ge.s32 	%p11, %r60, %r16;
	@%p11 bra 	$L__BB9_14;

	mov.u32 	%r63, 0;
	st.shared.u32 	[%r20], %r63;
	bra.uni 	$L__BB9_14;

$L__BB9_13:
	mad.lo.s32 	%r65, %r19, %r46, %r3;
	mul.wide.s32 	%rd12, %r65, 4;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.v4.f32 	{%f18, %f19, %f20, %f21}, [%rd13];
	mad.lo.s32 	%r66, %r86, 20, %r13;
	st.shared.f32 	[%r66], %f18;
	st.shared.f32 	[%r66+4], %f19;
	st.shared.f32 	[%r66+8], %f20;
	st.shared.f32 	[%r66+12], %f21;

$L__BB9_14:
	add.s32 	%r86, %r86, %r6;
	setp.lt.s32 	%p14, %r86, %r4;
	@%p14 bra 	$L__BB9_6;

$L__BB9_15:
	add.s32 	%r22, %r3, %r7;
	bar.sync 	0;
	setp.ge.s32 	%p15, %r22, %r46;
	add.s32 	%r23, %r2, %r8;
	setp.ge.s32 	%p16, %r23, %r47;
	or.pred  	%p17, %p15, %p16;
	@%p17 bra 	$L__BB9_26;

	cvta.to.global.u64 	%rd14, %rd5;
	mul.wide.s32 	%rd15, %r22, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.nc.u32 	%r67, [%rd16];
	add.s32 	%r68, %r67, %r1;
	mad.lo.s32 	%r69, %r23, %r46, %r22;
	setp.lt.s32 	%p18, %r23, %r68;
	cvta.to.global.u64 	%rd17, %rd6;
	mul.wide.s32 	%rd18, %r69, 4;
	add.s64 	%rd3, %rd17, %rd18;
	@%p18 bra 	$L__BB9_25;
	bra.uni 	$L__BB9_17;

$L__BB9_25:
	mov.u32 	%r84, 2143289344;
	st.global.u32 	[%rd3], %r84;
	bra.uni 	$L__BB9_26;

$L__BB9_17:
	setp.lt.s32 	%p19, %r45, 2;
	mov.f32 	%f73, 0f00000000;
	@%p19 bra 	$L__BB9_24;

	max.s32 	%r24, %r1, 1;
	add.s32 	%r71, %r24, -1;
	and.b32  	%r94, %r24, 3;
	setp.lt.u32 	%p20, %r71, 3;
	mov.f32 	%f74, 0f00000000;
	mov.u32 	%r91, 0;
	mov.f32 	%f73, %f74;
	@%p20 bra 	$L__BB9_21;

	sub.s32 	%r90, %r24, %r94;
	mad.lo.s32 	%r74, %r8, 20, %r5;
	shl.b32 	%r75, %r7, 2;
	add.s32 	%r76, %r74, %r75;
	mov.u32 	%r88, shraw;
	add.s32 	%r77, %r88, %r76;
	add.s32 	%r87, %r77, 40;
	mov.f32 	%f31, 0f00000000;
	mov.u32 	%r91, 0;
	mov.f32 	%f74, %f31;
	mov.f32 	%f73, %f31;

$L__BB9_20:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f32, %f33, %f34, %f35}, [%r88];
	ld.shared.f32 	%f40, [%r87+-40];
	fma.rn.ftz.f32 	%f42, %f40, %f32, %f31;
	sub.ftz.f32 	%f43, %f42, %f74;
	add.ftz.f32 	%f44, %f73, %f43;
	sub.ftz.f32 	%f45, %f44, %f73;
	sub.ftz.f32 	%f46, %f45, %f43;
	ld.shared.f32 	%f47, [%r87+-20];
	fma.rn.ftz.f32 	%f48, %f47, %f33, %f31;
	sub.ftz.f32 	%f49, %f48, %f46;
	add.ftz.f32 	%f50, %f44, %f49;
	sub.ftz.f32 	%f51, %f50, %f44;
	sub.ftz.f32 	%f52, %f51, %f49;
	ld.shared.f32 	%f53, [%r87];
	fma.rn.ftz.f32 	%f54, %f53, %f34, %f31;
	sub.ftz.f32 	%f55, %f54, %f52;
	add.ftz.f32 	%f56, %f50, %f55;
	sub.ftz.f32 	%f57, %f56, %f50;
	sub.ftz.f32 	%f58, %f57, %f55;
	ld.shared.f32 	%f59, [%r87+20];
	fma.rn.ftz.f32 	%f60, %f59, %f35, %f31;
	sub.ftz.f32 	%f61, %f60, %f58;
	add.ftz.f32 	%f73, %f56, %f61;
	sub.ftz.f32 	%f62, %f73, %f56;
	sub.ftz.f32 	%f74, %f62, %f61;
	add.s32 	%r91, %r91, 4;
	add.s32 	%r88, %r88, 16;
	add.s32 	%r87, %r87, 80;
	add.s32 	%r90, %r90, -4;
	setp.ne.s32 	%p21, %r90, 0;
	@%p21 bra 	$L__BB9_20;

$L__BB9_21:
	setp.eq.s32 	%p22, %r94, 0;
	@%p22 bra 	$L__BB9_24;

	shl.b32 	%r78, %r91, 2;
	mov.u32 	%r79, shraw;
	add.s32 	%r93, %r79, %r78;
	add.s32 	%r80, %r8, %r91;
	mad.lo.s32 	%r81, %r80, 20, %r5;
	shl.b32 	%r82, %r7, 2;
	add.s32 	%r83, %r81, %r82;
	add.s32 	%r92, %r79, %r83;
	mov.f32 	%f77, %f73;

$L__BB9_23:
	.pragma "nounroll";
	ld.shared.f32 	%f63, [%r93];
	ld.shared.f32 	%f64, [%r92];
	mov.f32 	%f65, 0f00000000;
	fma.rn.ftz.f32 	%f66, %f64, %f63, %f65;
	sub.ftz.f32 	%f67, %f66, %f74;
	add.ftz.f32 	%f73, %f77, %f67;
	sub.ftz.f32 	%f68, %f73, %f77;
	sub.ftz.f32 	%f74, %f68, %f67;
	add.s32 	%r93, %r93, 4;
	add.s32 	%r92, %r92, 20;
	add.s32 	%r94, %r94, -1;
	setp.ne.s32 	%p23, %r94, 0;
	mov.f32 	%f77, %f73;
	@%p23 bra 	$L__BB9_23;

$L__BB9_24:
	mul.rn.ftz.f32 	%f69, %f73, %f15;
	st.global.f32 	[%rd3], %f69;

$L__BB9_26:
	ret;

}
	// .globl	cwma_precompute_weights_f32
.visible .entry cwma_precompute_weights_f32(
	.param .u64 cwma_precompute_weights_f32_param_0,
	.param .u32 cwma_precompute_weights_f32_param_1,
	.param .u32 cwma_precompute_weights_f32_param_2,
	.param .u64 cwma_precompute_weights_f32_param_3,
	.param .u64 cwma_precompute_weights_f32_param_4
)
{
	.reg .pred 	%p<22>;
	.reg .f32 	%f<92>;
	.reg .b32 	%r<70>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd12, [cwma_precompute_weights_f32_param_0];
	ld.param.u32 	%r37, [cwma_precompute_weights_f32_param_1];
	ld.param.u32 	%r36, [cwma_precompute_weights_f32_param_2];
	ld.param.u64 	%rd14, [cwma_precompute_weights_f32_param_3];
	ld.param.u64 	%rd13, [cwma_precompute_weights_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd14;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r37;
	@%p1 bra 	$L__BB10_29;

	cvta.to.global.u64 	%rd15, %rd12;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.nc.u32 	%r2, [%rd17];
	add.s32 	%r38, %r2, -1;
	setp.gt.s32 	%p2, %r2, 1;
	selp.b32 	%r3, %r38, 0, %p2;
	mul.lo.s32 	%r4, %r1, %r36;
	mov.u32 	%r5, %tid.x;
	setp.ge.s32 	%p3, %r5, %r3;
	@%p3 bra 	$L__BB10_4;

	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r57, %r5;

$L__BB10_3:
	sub.s32 	%r39, %r2, %r57;
	cvt.rn.f32.s32 	%f21, %r39;
	mul.ftz.f32 	%f22, %f21, %f21;
	mul.ftz.f32 	%f23, %f22, %f21;
	add.s32 	%r40, %r57, %r4;
	mul.wide.s32 	%rd18, %r40, 4;
	add.s64 	%rd19, %rd1, %rd18;
	st.global.f32 	[%rd19], %f23;
	add.s32 	%r57, %r57, %r6;
	setp.lt.s32 	%p4, %r57, %r3;
	@%p4 bra 	$L__BB10_3;

$L__BB10_4:
	bar.sync 	0;
	setp.ne.s32 	%p5, %r5, 0;
	@%p5 bra 	$L__BB10_29;

	setp.lt.s32 	%p6, %r3, 1;
	mov.f32 	%f81, 0f00000000;
	@%p6 bra 	$L__BB10_12;

	add.s32 	%r42, %r3, -1;
	and.b32  	%r61, %r3, 3;
	setp.lt.u32 	%p7, %r42, 3;
	mov.f32 	%f82, 0f00000000;
	mov.u32 	%r60, 0;
	mov.f32 	%f81, %f82;
	@%p7 bra 	$L__BB10_9;

	sub.s32 	%r59, %r3, %r61;
	mov.f32 	%f82, 0f00000000;
	mov.u32 	%r60, 0;

$L__BB10_8:
	add.s32 	%r44, %r60, %r4;
	mul.wide.s32 	%rd20, %r44, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.f32 	%f30, [%rd21];
	sub.ftz.f32 	%f31, %f30, %f82;
	add.ftz.f32 	%f32, %f81, %f31;
	sub.ftz.f32 	%f33, %f32, %f81;
	sub.ftz.f32 	%f34, %f33, %f31;
	ld.global.f32 	%f35, [%rd21+4];
	sub.ftz.f32 	%f36, %f35, %f34;
	add.ftz.f32 	%f37, %f32, %f36;
	sub.ftz.f32 	%f38, %f37, %f32;
	sub.ftz.f32 	%f39, %f38, %f36;
	ld.global.f32 	%f40, [%rd21+8];
	sub.ftz.f32 	%f41, %f40, %f39;
	add.ftz.f32 	%f42, %f37, %f41;
	sub.ftz.f32 	%f43, %f42, %f37;
	sub.ftz.f32 	%f44, %f43, %f41;
	ld.global.f32 	%f45, [%rd21+12];
	sub.ftz.f32 	%f46, %f45, %f44;
	add.ftz.f32 	%f81, %f42, %f46;
	sub.ftz.f32 	%f47, %f81, %f42;
	sub.ftz.f32 	%f82, %f47, %f46;
	add.s32 	%r60, %r60, 4;
	add.s32 	%r59, %r59, -4;
	setp.ne.s32 	%p8, %r59, 0;
	@%p8 bra 	$L__BB10_8;

$L__BB10_9:
	setp.eq.s32 	%p9, %r61, 0;
	@%p9 bra 	$L__BB10_12;

	add.s32 	%r45, %r60, %r4;
	mul.wide.s32 	%rd22, %r45, 4;
	add.s64 	%rd34, %rd1, %rd22;
	mov.f32 	%f85, %f81;

$L__BB10_11:
	.pragma "nounroll";
	ld.global.f32 	%f48, [%rd34];
	sub.ftz.f32 	%f49, %f48, %f82;
	add.ftz.f32 	%f81, %f85, %f49;
	sub.ftz.f32 	%f50, %f81, %f85;
	sub.ftz.f32 	%f82, %f50, %f49;
	add.s64 	%rd34, %rd34, 4;
	add.s32 	%r61, %r61, -1;
	setp.ne.s32 	%p10, %r61, 0;
	mov.f32 	%f85, %f81;
	@%p10 bra 	$L__BB10_11;

$L__BB10_12:
	mov.f32 	%f51, 0f0DA24260;
	max.ftz.f32 	%f52, %f81, %f51;
	rcp.rn.ftz.f32 	%f13, %f52;
	@%p6 bra 	$L__BB10_19;

	add.s32 	%r47, %r3, -1;
	and.b32  	%r65, %r3, 3;
	setp.lt.u32 	%p12, %r47, 3;
	mov.u32 	%r64, 0;
	@%p12 bra 	$L__BB10_16;

	sub.s32 	%r63, %r3, %r65;
	mov.u32 	%r64, 0;

$L__BB10_15:
	add.s32 	%r49, %r64, %r4;
	mul.wide.s32 	%rd23, %r49, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f32 	%f53, [%rd24];
	mul.rn.ftz.f32 	%f54, %f53, %f13;
	st.global.f32 	[%rd24], %f54;
	ld.global.f32 	%f55, [%rd24+4];
	mul.rn.ftz.f32 	%f56, %f55, %f13;
	st.global.f32 	[%rd24+4], %f56;
	ld.global.f32 	%f57, [%rd24+8];
	mul.rn.ftz.f32 	%f58, %f57, %f13;
	st.global.f32 	[%rd24+8], %f58;
	ld.global.f32 	%f59, [%rd24+12];
	mul.rn.ftz.f32 	%f60, %f59, %f13;
	st.global.f32 	[%rd24+12], %f60;
	add.s32 	%r64, %r64, 4;
	add.s32 	%r63, %r63, -4;
	setp.ne.s32 	%p13, %r63, 0;
	@%p13 bra 	$L__BB10_15;

$L__BB10_16:
	setp.eq.s32 	%p14, %r65, 0;
	@%p14 bra 	$L__BB10_19;

	add.s32 	%r50, %r64, %r4;
	mul.wide.s32 	%rd25, %r50, 4;
	add.s64 	%rd35, %rd1, %rd25;

$L__BB10_18:
	.pragma "nounroll";
	ld.global.f32 	%f61, [%rd35];
	mul.rn.ftz.f32 	%f62, %f61, %f13;
	st.global.f32 	[%rd35], %f62;
	add.s64 	%rd35, %rd35, 4;
	add.s32 	%r65, %r65, -1;
	setp.ne.s32 	%p15, %r65, 0;
	@%p15 bra 	$L__BB10_18;

$L__BB10_19:
	setp.lt.s32 	%p16, %r2, 2;
	@%p16 bra 	$L__BB10_28;

	mov.f32 	%f91, 0f00000000;
	@%p6 bra 	$L__BB10_27;

	add.s32 	%r52, %r3, -1;
	and.b32  	%r69, %r3, 3;
	setp.lt.u32 	%p18, %r52, 3;
	mov.f32 	%f91, 0f00000000;
	mov.u32 	%r68, 0;
	@%p18 bra 	$L__BB10_24;

	sub.s32 	%r67, %r3, %r69;
	mov.f32 	%f91, 0f00000000;
	mov.u32 	%r68, 0;

$L__BB10_23:
	add.s32 	%r54, %r68, %r4;
	mul.wide.s32 	%rd26, %r54, 4;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.f32 	%f67, [%rd27];
	add.rn.ftz.f32 	%f68, %f91, %f67;
	ld.global.f32 	%f69, [%rd27+4];
	add.rn.ftz.f32 	%f70, %f68, %f69;
	ld.global.f32 	%f71, [%rd27+8];
	add.rn.ftz.f32 	%f72, %f70, %f71;
	ld.global.f32 	%f73, [%rd27+12];
	add.rn.ftz.f32 	%f91, %f72, %f73;
	add.s32 	%r68, %r68, 4;
	add.s32 	%r67, %r67, -4;
	setp.ne.s32 	%p19, %r67, 0;
	@%p19 bra 	$L__BB10_23;

$L__BB10_24:
	setp.eq.s32 	%p20, %r69, 0;
	@%p20 bra 	$L__BB10_27;

	add.s32 	%r55, %r68, %r4;
	mul.wide.s32 	%rd28, %r55, 4;
	add.s64 	%rd36, %rd1, %rd28;

$L__BB10_26:
	.pragma "nounroll";
	ld.global.f32 	%f74, [%rd36];
	add.rn.ftz.f32 	%f91, %f91, %f74;
	add.s64 	%rd36, %rd36, 4;
	add.s32 	%r69, %r69, -1;
	setp.ne.s32 	%p21, %r69, 0;
	@%p21 bra 	$L__BB10_26;

$L__BB10_27:
	mul.wide.s32 	%rd29, %r4, 4;
	add.s64 	%rd30, %rd1, %rd29;
	mov.f32 	%f75, 0f3F800000;
	sub.rn.ftz.f32 	%f76, %f75, %f91;
	ld.global.f32 	%f77, [%rd30];
	add.rn.ftz.f32 	%f78, %f77, %f76;
	st.global.f32 	[%rd30], %f78;

$L__BB10_28:
	cvta.to.global.u64 	%rd31, %rd13;
	shl.b64 	%rd32, %rd2, 2;
	add.s64 	%rd33, %rd31, %rd32;
	mov.u32 	%r56, 1065353216;
	st.global.u32 	[%rd33], %r56;

$L__BB10_29:
	ret;

}
	// .globl	cwma_precompute_weights_oldest_first_f32
.visible .entry cwma_precompute_weights_oldest_first_f32(
	.param .u64 cwma_precompute_weights_oldest_first_f32_param_0,
	.param .u32 cwma_precompute_weights_oldest_first_f32_param_1,
	.param .u32 cwma_precompute_weights_oldest_first_f32_param_2,
	.param .u64 cwma_precompute_weights_oldest_first_f32_param_3,
	.param .u64 cwma_precompute_weights_oldest_first_f32_param_4
)
{
	.reg .pred 	%p<22>;
	.reg .f32 	%f<92>;
	.reg .b32 	%r<72>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd12, [cwma_precompute_weights_oldest_first_f32_param_0];
	ld.param.u32 	%r37, [cwma_precompute_weights_oldest_first_f32_param_1];
	ld.param.u32 	%r36, [cwma_precompute_weights_oldest_first_f32_param_2];
	ld.param.u64 	%rd14, [cwma_precompute_weights_oldest_first_f32_param_3];
	ld.param.u64 	%rd13, [cwma_precompute_weights_oldest_first_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd14;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r37;
	@%p1 bra 	$L__BB11_29;

	cvta.to.global.u64 	%rd15, %rd12;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.nc.u32 	%r2, [%rd17];
	add.s32 	%r38, %r2, -1;
	setp.gt.s32 	%p2, %r2, 1;
	selp.b32 	%r3, %r38, 0, %p2;
	mul.lo.s32 	%r4, %r1, %r36;
	mov.u32 	%r5, %tid.x;
	setp.ge.s32 	%p3, %r5, %r3;
	@%p3 bra 	$L__BB11_4;

	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r59, %r5;

$L__BB11_3:
	add.s32 	%r39, %r59, 2;
	cvt.rn.f32.s32 	%f21, %r39;
	mul.ftz.f32 	%f22, %f21, %f21;
	mul.ftz.f32 	%f23, %f22, %f21;
	add.s32 	%r40, %r59, %r4;
	mul.wide.s32 	%rd18, %r40, 4;
	add.s64 	%rd19, %rd1, %rd18;
	st.global.f32 	[%rd19], %f23;
	add.s32 	%r59, %r59, %r6;
	setp.lt.s32 	%p4, %r59, %r3;
	@%p4 bra 	$L__BB11_3;

$L__BB11_4:
	bar.sync 	0;
	setp.ne.s32 	%p5, %r5, 0;
	@%p5 bra 	$L__BB11_29;

	setp.lt.s32 	%p6, %r3, 1;
	mov.f32 	%f81, 0f00000000;
	@%p6 bra 	$L__BB11_12;

	add.s32 	%r42, %r3, -1;
	and.b32  	%r63, %r3, 3;
	setp.lt.u32 	%p7, %r42, 3;
	mov.f32 	%f82, 0f00000000;
	mov.u32 	%r62, 0;
	mov.f32 	%f81, %f82;
	@%p7 bra 	$L__BB11_9;

	sub.s32 	%r61, %r3, %r63;
	mov.f32 	%f82, 0f00000000;
	mov.u32 	%r62, 0;

$L__BB11_8:
	add.s32 	%r44, %r62, %r4;
	mul.wide.s32 	%rd20, %r44, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.f32 	%f30, [%rd21];
	sub.ftz.f32 	%f31, %f30, %f82;
	add.ftz.f32 	%f32, %f81, %f31;
	sub.ftz.f32 	%f33, %f32, %f81;
	sub.ftz.f32 	%f34, %f33, %f31;
	ld.global.f32 	%f35, [%rd21+4];
	sub.ftz.f32 	%f36, %f35, %f34;
	add.ftz.f32 	%f37, %f32, %f36;
	sub.ftz.f32 	%f38, %f37, %f32;
	sub.ftz.f32 	%f39, %f38, %f36;
	ld.global.f32 	%f40, [%rd21+8];
	sub.ftz.f32 	%f41, %f40, %f39;
	add.ftz.f32 	%f42, %f37, %f41;
	sub.ftz.f32 	%f43, %f42, %f37;
	sub.ftz.f32 	%f44, %f43, %f41;
	ld.global.f32 	%f45, [%rd21+12];
	sub.ftz.f32 	%f46, %f45, %f44;
	add.ftz.f32 	%f81, %f42, %f46;
	sub.ftz.f32 	%f47, %f81, %f42;
	sub.ftz.f32 	%f82, %f47, %f46;
	add.s32 	%r62, %r62, 4;
	add.s32 	%r61, %r61, -4;
	setp.ne.s32 	%p8, %r61, 0;
	@%p8 bra 	$L__BB11_8;

$L__BB11_9:
	setp.eq.s32 	%p9, %r63, 0;
	@%p9 bra 	$L__BB11_12;

	add.s32 	%r45, %r62, %r4;
	mul.wide.s32 	%rd22, %r45, 4;
	add.s64 	%rd34, %rd1, %rd22;
	mov.f32 	%f85, %f81;

$L__BB11_11:
	.pragma "nounroll";
	ld.global.f32 	%f48, [%rd34];
	sub.ftz.f32 	%f49, %f48, %f82;
	add.ftz.f32 	%f81, %f85, %f49;
	sub.ftz.f32 	%f50, %f81, %f85;
	sub.ftz.f32 	%f82, %f50, %f49;
	add.s64 	%rd34, %rd34, 4;
	add.s32 	%r63, %r63, -1;
	setp.ne.s32 	%p10, %r63, 0;
	mov.f32 	%f85, %f81;
	@%p10 bra 	$L__BB11_11;

$L__BB11_12:
	mov.f32 	%f51, 0f0DA24260;
	max.ftz.f32 	%f52, %f81, %f51;
	rcp.rn.ftz.f32 	%f13, %f52;
	@%p6 bra 	$L__BB11_19;

	add.s32 	%r47, %r3, -1;
	and.b32  	%r67, %r3, 3;
	setp.lt.u32 	%p12, %r47, 3;
	mov.u32 	%r66, 0;
	@%p12 bra 	$L__BB11_16;

	sub.s32 	%r65, %r3, %r67;
	mov.u32 	%r66, 0;

$L__BB11_15:
	add.s32 	%r49, %r66, %r4;
	mul.wide.s32 	%rd23, %r49, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f32 	%f53, [%rd24];
	mul.rn.ftz.f32 	%f54, %f53, %f13;
	st.global.f32 	[%rd24], %f54;
	ld.global.f32 	%f55, [%rd24+4];
	mul.rn.ftz.f32 	%f56, %f55, %f13;
	st.global.f32 	[%rd24+4], %f56;
	ld.global.f32 	%f57, [%rd24+8];
	mul.rn.ftz.f32 	%f58, %f57, %f13;
	st.global.f32 	[%rd24+8], %f58;
	ld.global.f32 	%f59, [%rd24+12];
	mul.rn.ftz.f32 	%f60, %f59, %f13;
	st.global.f32 	[%rd24+12], %f60;
	add.s32 	%r66, %r66, 4;
	add.s32 	%r65, %r65, -4;
	setp.ne.s32 	%p13, %r65, 0;
	@%p13 bra 	$L__BB11_15;

$L__BB11_16:
	setp.eq.s32 	%p14, %r67, 0;
	@%p14 bra 	$L__BB11_19;

	add.s32 	%r50, %r66, %r4;
	mul.wide.s32 	%rd25, %r50, 4;
	add.s64 	%rd35, %rd1, %rd25;

$L__BB11_18:
	.pragma "nounroll";
	ld.global.f32 	%f61, [%rd35];
	mul.rn.ftz.f32 	%f62, %f61, %f13;
	st.global.f32 	[%rd35], %f62;
	add.s64 	%rd35, %rd35, 4;
	add.s32 	%r67, %r67, -1;
	setp.ne.s32 	%p15, %r67, 0;
	@%p15 bra 	$L__BB11_18;

$L__BB11_19:
	setp.lt.s32 	%p16, %r2, 2;
	@%p16 bra 	$L__BB11_28;

	mov.f32 	%f91, 0f00000000;
	@%p6 bra 	$L__BB11_27;

	add.s32 	%r52, %r3, -1;
	and.b32  	%r71, %r3, 3;
	setp.lt.u32 	%p18, %r52, 3;
	mov.f32 	%f91, 0f00000000;
	mov.u32 	%r70, 0;
	@%p18 bra 	$L__BB11_24;

	sub.s32 	%r69, %r3, %r71;
	mov.f32 	%f91, 0f00000000;
	mov.u32 	%r70, 0;

$L__BB11_23:
	add.s32 	%r54, %r70, %r4;
	mul.wide.s32 	%rd26, %r54, 4;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.f32 	%f67, [%rd27];
	add.rn.ftz.f32 	%f68, %f91, %f67;
	ld.global.f32 	%f69, [%rd27+4];
	add.rn.ftz.f32 	%f70, %f68, %f69;
	ld.global.f32 	%f71, [%rd27+8];
	add.rn.ftz.f32 	%f72, %f70, %f71;
	ld.global.f32 	%f73, [%rd27+12];
	add.rn.ftz.f32 	%f91, %f72, %f73;
	add.s32 	%r70, %r70, 4;
	add.s32 	%r69, %r69, -4;
	setp.ne.s32 	%p19, %r69, 0;
	@%p19 bra 	$L__BB11_23;

$L__BB11_24:
	setp.eq.s32 	%p20, %r71, 0;
	@%p20 bra 	$L__BB11_27;

	add.s32 	%r55, %r70, %r4;
	mul.wide.s32 	%rd28, %r55, 4;
	add.s64 	%rd36, %rd1, %rd28;

$L__BB11_26:
	.pragma "nounroll";
	ld.global.f32 	%f74, [%rd36];
	add.rn.ftz.f32 	%f91, %f91, %f74;
	add.s64 	%rd36, %rd36, 4;
	add.s32 	%r71, %r71, -1;
	setp.ne.s32 	%p21, %r71, 0;
	@%p21 bra 	$L__BB11_26;

$L__BB11_27:
	add.s32 	%r56, %r4, %r3;
	add.s32 	%r57, %r56, -1;
	mul.wide.s32 	%rd29, %r57, 4;
	add.s64 	%rd30, %rd1, %rd29;
	mov.f32 	%f75, 0f3F800000;
	sub.rn.ftz.f32 	%f76, %f75, %f91;
	ld.global.f32 	%f77, [%rd30];
	add.rn.ftz.f32 	%f78, %f77, %f76;
	st.global.f32 	[%rd30], %f78;

$L__BB11_28:
	cvta.to.global.u64 	%rd31, %rd13;
	shl.b64 	%rd32, %rd2, 2;
	add.s64 	%rd33, %rd31, %rd32;
	mov.u32 	%r58, 1065353216;
	st.global.u32 	[%rd33], %r58;

$L__BB11_29:
	ret;

}

