//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_89
.address_size 64

	// .globl	alma_batch_f32_onthefly
// _ZZ23alma_batch_f32_ontheflyE8period_s has been demoted
// _ZZ23alma_batch_f32_ontheflyE8offset_s has been demoted
// _ZZ23alma_batch_f32_ontheflyE7sigma_s has been demoted
// _ZZ23alma_batch_f32_ontheflyE10inv_norm_s has been demoted
.shared .align 4 .b8 _ZZ14alma_block_sumfE8warp_buf[128];
// _ZZ27alma_precompute_weights_f32E10inv_norm_s has been demoted
.extern .shared .align 16 .b8 shraw[];
.extern .shared .align 16 .b8 sh[];

.visible .entry alma_batch_f32_onthefly(
	.param .u64 alma_batch_f32_onthefly_param_0,
	.param .u64 alma_batch_f32_onthefly_param_1,
	.param .u64 alma_batch_f32_onthefly_param_2,
	.param .u64 alma_batch_f32_onthefly_param_3,
	.param .u32 alma_batch_f32_onthefly_param_4,
	.param .u32 alma_batch_f32_onthefly_param_5,
	.param .u32 alma_batch_f32_onthefly_param_6,
	.param .u64 alma_batch_f32_onthefly_param_7
)
{
	.reg .pred 	%p<39>;
	.reg .f32 	%f<102>;
	.reg .b32 	%r<132>;
	.reg .b64 	%rd<39>;
	// demoted variable
	.shared .align 4 .u32 _ZZ23alma_batch_f32_ontheflyE8period_s;
	// demoted variable
	.shared .align 4 .f32 _ZZ23alma_batch_f32_ontheflyE8offset_s;
	// demoted variable
	.shared .align 4 .f32 _ZZ23alma_batch_f32_ontheflyE7sigma_s;
	// demoted variable
	.shared .align 4 .f32 _ZZ23alma_batch_f32_ontheflyE10inv_norm_s;

	ld.param.u64 	%rd9, [alma_batch_f32_onthefly_param_0];
	ld.param.u64 	%rd10, [alma_batch_f32_onthefly_param_1];
	ld.param.u64 	%rd11, [alma_batch_f32_onthefly_param_2];
	ld.param.u64 	%rd12, [alma_batch_f32_onthefly_param_3];
	ld.param.u32 	%r40, [alma_batch_f32_onthefly_param_4];
	ld.param.u32 	%r42, [alma_batch_f32_onthefly_param_5];
	ld.param.u32 	%r41, [alma_batch_f32_onthefly_param_6];
	ld.param.u64 	%rd13, [alma_batch_f32_onthefly_param_7];
	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p1, %r1, %r42;
	@%p1 bra 	$L__BB0_35;

	mov.u32 	%r121, %tid.x;
	setp.ne.s32 	%p2, %r121, 0;
	@%p2 bra 	$L__BB0_3;

	cvta.to.global.u64 	%rd14, %rd10;
	mul.wide.s32 	%rd15, %r1, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.nc.u32 	%r43, [%rd16];
	st.shared.u32 	[_ZZ23alma_batch_f32_ontheflyE8period_s], %r43;
	cvta.to.global.u64 	%rd17, %rd11;
	add.s64 	%rd18, %rd17, %rd15;
	ld.global.nc.f32 	%f20, [%rd18];
	st.shared.f32 	[_ZZ23alma_batch_f32_ontheflyE8offset_s], %f20;
	cvta.to.global.u64 	%rd19, %rd12;
	add.s64 	%rd20, %rd19, %rd15;
	ld.global.nc.f32 	%f21, [%rd20];
	st.shared.f32 	[_ZZ23alma_batch_f32_ontheflyE7sigma_s], %f21;

$L__BB0_3:
	bar.sync 	0;
	ld.shared.u32 	%r3, [_ZZ23alma_batch_f32_ontheflyE8period_s];
	add.s32 	%r4, %r3, -1;
	add.s32 	%r5, %r4, %r41;
	mul.lo.s32 	%r6, %r1, %r40;
	setp.ge.s32 	%p3, %r121, %r3;
	mov.f32 	%f95, 0f00000000;
	@%p3 bra 	$L__BB0_6;

	ld.shared.f32 	%f24, [_ZZ23alma_batch_f32_ontheflyE8offset_s];
	ld.shared.f32 	%f25, [_ZZ23alma_batch_f32_ontheflyE7sigma_s];
	mov.f32 	%f26, 0f358637BD;
	max.ftz.f32 	%f27, %f25, %f26;
	cvt.rn.f32.s32 	%f28, %r3;
	div.approx.ftz.f32 	%f29, %f28, %f27;
	mov.u32 	%r7, %ntid.x;
	add.ftz.f32 	%f30, %f29, %f29;
	mul.ftz.f32 	%f1, %f29, %f30;
	cvt.rn.f32.s32 	%f31, %r4;
	mul.ftz.f32 	%f2, %f24, %f31;
	mov.f32 	%f95, 0f00000000;

$L__BB0_5:
	cvt.rn.f32.s32 	%f32, %r121;
	sub.ftz.f32 	%f33, %f32, %f2;
	mul.ftz.f32 	%f34, %f33, %f33;
	neg.ftz.f32 	%f35, %f34;
	div.approx.ftz.f32 	%f36, %f35, %f1;
	mul.ftz.f32 	%f37, %f36, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f38, %f37;
	shl.b32 	%r44, %r121, 2;
	mov.u32 	%r45, sh;
	add.s32 	%r46, %r45, %r44;
	st.shared.f32 	[%r46], %f38;
	add.ftz.f32 	%f95, %f95, %f38;
	add.s32 	%r121, %r121, %r7;
	setp.lt.s32 	%p4, %r121, %r3;
	@%p4 bra 	$L__BB0_5;

$L__BB0_6:
	mov.u32 	%r47, %tid.x;
	and.b32  	%r10, %r47, 31;
	mov.u32 	%r48, 31;
	shr.u32 	%r11, %r47, 5;
	mov.b32 	%r49, %f95;
	mov.u32 	%r50, 2;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.down.b32 	%r53|%p5, %r49, %r51, %r48, %r52;
	mov.b32 	%f39, %r53;
	add.ftz.f32 	%f40, %f95, %f39;
	mov.b32 	%r54, %f40;
	mov.u32 	%r55, 8;
	shfl.sync.down.b32 	%r56|%p6, %r54, %r55, %r48, %r52;
	mov.b32 	%f41, %r56;
	add.ftz.f32 	%f42, %f40, %f41;
	mov.b32 	%r57, %f42;
	mov.u32 	%r58, 4;
	shfl.sync.down.b32 	%r59|%p7, %r57, %r58, %r48, %r52;
	mov.b32 	%f43, %r59;
	add.ftz.f32 	%f44, %f42, %f43;
	mov.b32 	%r60, %f44;
	shfl.sync.down.b32 	%r61|%p8, %r60, %r50, %r48, %r52;
	mov.b32 	%f45, %r61;
	add.ftz.f32 	%f46, %f44, %f45;
	mov.b32 	%r62, %f46;
	mov.u32 	%r63, 1;
	shfl.sync.down.b32 	%r64|%p9, %r62, %r63, %r48, %r52;
	mov.b32 	%f47, %r64;
	add.ftz.f32 	%f6, %f46, %f47;
	setp.ne.s32 	%p10, %r10, 0;
	@%p10 bra 	$L__BB0_8;

	shl.b32 	%r65, %r11, 2;
	mov.u32 	%r66, _ZZ14alma_block_sumfE8warp_buf;
	add.s32 	%r67, %r66, %r65;
	st.shared.f32 	[%r67], %f6;

$L__BB0_8:
	bar.sync 	0;
	setp.ne.s32 	%p11, %r11, 0;
	mov.f32 	%f97, 0f00000000;
	@%p11 bra 	$L__BB0_12;

	mov.u32 	%r68, %ntid.x;
	add.s32 	%r69, %r68, 31;
	shr.u32 	%r70, %r69, 5;
	setp.ge.u32 	%p12, %r10, %r70;
	mov.f32 	%f96, 0f00000000;
	@%p12 bra 	$L__BB0_11;

	shl.b32 	%r71, %r10, 2;
	mov.u32 	%r72, _ZZ14alma_block_sumfE8warp_buf;
	add.s32 	%r73, %r72, %r71;
	ld.shared.f32 	%f96, [%r73];

$L__BB0_11:
	mov.b32 	%r74, %f96;
	mov.u32 	%r75, 2;
	mov.u32 	%r76, 31;
	mov.u32 	%r77, 16;
	mov.u32 	%r78, -1;
	shfl.sync.down.b32 	%r79|%p13, %r74, %r77, %r76, %r78;
	mov.b32 	%f50, %r79;
	add.ftz.f32 	%f51, %f96, %f50;
	mov.b32 	%r80, %f51;
	mov.u32 	%r81, 8;
	shfl.sync.down.b32 	%r82|%p14, %r80, %r81, %r76, %r78;
	mov.b32 	%f52, %r82;
	add.ftz.f32 	%f53, %f51, %f52;
	mov.b32 	%r83, %f53;
	mov.u32 	%r84, 4;
	shfl.sync.down.b32 	%r85|%p15, %r83, %r84, %r76, %r78;
	mov.b32 	%f54, %r85;
	add.ftz.f32 	%f55, %f53, %f54;
	mov.b32 	%r86, %f55;
	shfl.sync.down.b32 	%r87|%p16, %r86, %r75, %r76, %r78;
	mov.b32 	%f56, %r87;
	add.ftz.f32 	%f57, %f55, %f56;
	mov.b32 	%r88, %f57;
	mov.u32 	%r89, 1;
	shfl.sync.down.b32 	%r90|%p17, %r88, %r89, %r76, %r78;
	mov.b32 	%f58, %r90;
	add.ftz.f32 	%f97, %f57, %f58;

$L__BB0_12:
	setp.ne.s32 	%p18, %r47, 0;
	@%p18 bra 	$L__BB0_14;

	mov.f32 	%f59, 0f1E3CE508;
	max.ftz.f32 	%f60, %f97, %f59;
	rcp.approx.ftz.f32 	%f61, %f60;
	st.shared.f32 	[_ZZ23alma_batch_f32_ontheflyE10inv_norm_s], %f61;

$L__BB0_14:
	setp.lt.s32 	%p19, %r47, %r3;
	bar.sync 	0;
	@%p19 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_17;

$L__BB0_15:
	ld.shared.f32 	%f11, [_ZZ23alma_batch_f32_ontheflyE10inv_norm_s];
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r94, sh;
	mov.u32 	%r122, %r47;

$L__BB0_16:
	shl.b32 	%r93, %r122, 2;
	add.s32 	%r95, %r94, %r93;
	ld.shared.f32 	%f62, [%r95];
	mul.ftz.f32 	%f63, %f11, %f62;
	st.shared.f32 	[%r95], %f63;
	add.s32 	%r122, %r122, %r12;
	setp.lt.s32 	%p20, %r122, %r3;
	@%p20 bra 	$L__BB0_16;

$L__BB0_17:
	mov.u32 	%r96, %ntid.x;
	bar.sync 	0;
	mov.u32 	%r97, %ctaid.x;
	mad.lo.s32 	%r125, %r97, %r96, %r47;
	mov.u32 	%r99, %nctaid.x;
	mul.lo.s32 	%r17, %r99, %r96;
	setp.ge.s32 	%p21, %r125, %r40;
	@%p21 bra 	$L__BB0_35;

	setp.gt.s32 	%p22, %r3, 0;
	@%p22 bra 	$L__BB0_25;
	bra.uni 	$L__BB0_19;

$L__BB0_25:
	and.b32  	%r27, %r3, 3;
	sub.s32 	%r28, %r3, %r27;
	mov.u32 	%r110, 1;
	sub.s32 	%r29, %r110, %r3;
	cvta.to.global.u64 	%rd28, %rd9;

$L__BB0_26:
	setp.lt.s32 	%p32, %r125, %r5;
	mov.f32 	%f101, 0f7FC00000;
	@%p32 bra 	$L__BB0_34;

	setp.lt.u32 	%p33, %r4, 3;
	add.s32 	%r112, %r29, %r125;
	cvt.s64.s32 	%rd4, %r112;
	mov.f32 	%f101, 0f00000000;
	mov.u32 	%r131, 0;
	@%p33 bra 	$L__BB0_30;

	shl.b64 	%rd29, %rd4, 2;
	add.s64 	%rd30, %rd28, %rd29;
	add.s64 	%rd38, %rd30, 8;
	mov.f32 	%f101, 0f00000000;
	mov.u32 	%r131, 0;
	mov.u32 	%r128, sh;
	mov.u32 	%r130, %r28;

$L__BB0_29:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f73, %f74, %f75, %f76}, [%r128];
	ld.global.nc.f32 	%f81, [%rd38+-8];
	fma.rn.ftz.f32 	%f82, %f81, %f73, %f101;
	ld.global.nc.f32 	%f83, [%rd38+-4];
	fma.rn.ftz.f32 	%f84, %f83, %f74, %f82;
	ld.global.nc.f32 	%f85, [%rd38];
	fma.rn.ftz.f32 	%f86, %f85, %f75, %f84;
	ld.global.nc.f32 	%f87, [%rd38+4];
	fma.rn.ftz.f32 	%f101, %f87, %f76, %f86;
	add.s32 	%r131, %r131, 4;
	add.s64 	%rd38, %rd38, 16;
	add.s32 	%r128, %r128, 16;
	add.s32 	%r130, %r130, -4;
	setp.ne.s32 	%p34, %r130, 0;
	@%p34 bra 	$L__BB0_29;

$L__BB0_30:
	setp.eq.s32 	%p35, %r27, 0;
	@%p35 bra 	$L__BB0_34;

	setp.eq.s32 	%p36, %r27, 1;
	cvt.s64.s32 	%rd31, %r131;
	add.s64 	%rd32, %rd31, %rd4;
	shl.b64 	%rd34, %rd32, 2;
	add.s64 	%rd8, %rd28, %rd34;
	shl.b32 	%r115, %r131, 2;
	mov.u32 	%r116, sh;
	add.s32 	%r38, %r116, %r115;
	ld.shared.f32 	%f88, [%r38];
	ld.global.nc.f32 	%f89, [%rd8];
	fma.rn.ftz.f32 	%f101, %f89, %f88, %f101;
	@%p36 bra 	$L__BB0_34;

	setp.eq.s32 	%p37, %r27, 2;
	ld.shared.f32 	%f90, [%r38+4];
	ld.global.nc.f32 	%f91, [%rd8+4];
	fma.rn.ftz.f32 	%f101, %f91, %f90, %f101;
	@%p37 bra 	$L__BB0_34;

	ld.shared.f32 	%f92, [%r38+8];
	ld.global.nc.f32 	%f93, [%rd8+8];
	fma.rn.ftz.f32 	%f101, %f93, %f92, %f101;

$L__BB0_34:
	mad.lo.s32 	%r118, %r1, %r40, %r125;
	cvta.to.global.u64 	%rd35, %rd13;
	mul.wide.s32 	%rd36, %r118, 4;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f101;
	mad.lo.s32 	%r125, %r99, %r96, %r125;
	setp.lt.s32 	%p38, %r125, %r40;
	@%p38 bra 	$L__BB0_26;
	bra.uni 	$L__BB0_35;

$L__BB0_19:
	add.s32 	%r100, %r17, %r40;
	add.s32 	%r101, %r125, %r17;
	not.b32 	%r102, %r101;
	add.s32 	%r103, %r100, %r102;
	div.u32 	%r18, %r103, %r17;
	add.s32 	%r104, %r18, 1;
	and.b32  	%r124, %r104, 3;
	setp.eq.s32 	%p23, %r124, 0;
	@%p23 bra 	$L__BB0_22;

	cvta.to.global.u64 	%rd1, %rd13;

$L__BB0_21:
	.pragma "nounroll";
	setp.lt.s32 	%p24, %r125, %r5;
	selp.f32 	%f64, 0f7FC00000, 0f00000000, %p24;
	add.s32 	%r105, %r125, %r6;
	mul.wide.s32 	%rd21, %r105, 4;
	add.s64 	%rd22, %rd1, %rd21;
	st.global.f32 	[%rd22], %f64;
	add.s32 	%r125, %r125, %r17;
	add.s32 	%r124, %r124, -1;
	setp.ne.s32 	%p25, %r124, 0;
	@%p25 bra 	$L__BB0_21;

$L__BB0_22:
	setp.lt.u32 	%p26, %r18, 3;
	@%p26 bra 	$L__BB0_35;

	mul.wide.s32 	%rd2, %r17, 4;
	cvta.to.global.u64 	%rd3, %rd13;

$L__BB0_24:
	setp.lt.s32 	%p27, %r125, %r5;
	selp.f32 	%f65, 0f7FC00000, 0f00000000, %p27;
	add.s32 	%r106, %r125, %r6;
	mul.wide.s32 	%rd23, %r106, 4;
	add.s64 	%rd24, %rd3, %rd23;
	st.global.f32 	[%rd24], %f65;
	add.s32 	%r107, %r125, %r17;
	setp.lt.s32 	%p28, %r107, %r5;
	selp.f32 	%f66, 0f7FC00000, 0f00000000, %p28;
	add.s64 	%rd25, %rd24, %rd2;
	st.global.f32 	[%rd25], %f66;
	add.s32 	%r108, %r107, %r17;
	setp.lt.s32 	%p29, %r108, %r5;
	selp.f32 	%f67, 0f7FC00000, 0f00000000, %p29;
	add.s64 	%rd26, %rd25, %rd2;
	st.global.f32 	[%rd26], %f67;
	add.s32 	%r109, %r108, %r17;
	setp.lt.s32 	%p30, %r109, %r5;
	selp.f32 	%f68, 0f7FC00000, 0f00000000, %p30;
	add.s64 	%rd27, %rd26, %rd2;
	st.global.f32 	[%rd27], %f68;
	add.s32 	%r125, %r109, %r17;
	setp.lt.s32 	%p31, %r125, %r40;
	@%p31 bra 	$L__BB0_24;

$L__BB0_35:
	ret;

}
	// .globl	alma_batch_f32
.visible .entry alma_batch_f32(
	.param .u64 alma_batch_f32_param_0,
	.param .u64 alma_batch_f32_param_1,
	.param .u64 alma_batch_f32_param_2,
	.param .u64 alma_batch_f32_param_3,
	.param .u32 alma_batch_f32_param_4,
	.param .u32 alma_batch_f32_param_5,
	.param .u32 alma_batch_f32_param_6,
	.param .u32 alma_batch_f32_param_7,
	.param .u64 alma_batch_f32_param_8
)
{
	.reg .pred 	%p<22>;
	.reg .f32 	%f<44>;
	.reg .b32 	%r<75>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd13, [alma_batch_f32_param_0];
	ld.param.u64 	%rd11, [alma_batch_f32_param_1];
	ld.param.u64 	%rd12, [alma_batch_f32_param_2];
	ld.param.u32 	%r35, [alma_batch_f32_param_4];
	ld.param.u32 	%r36, [alma_batch_f32_param_5];
	ld.param.u32 	%r38, [alma_batch_f32_param_6];
	ld.param.u32 	%r37, [alma_batch_f32_param_7];
	ld.param.u64 	%rd14, [alma_batch_f32_param_8];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p1, %r1, %r38;
	@%p1 bra 	$L__BB1_21;

	cvta.to.global.u64 	%rd15, %rd12;
	mul.wide.s32 	%rd16, %r1, 4;
	add.s64 	%rd17, %rd15, %rd16;
	mov.u32 	%r2, %tid.x;
	ld.global.nc.u32 	%r3, [%rd17];
	setp.lt.s32 	%p2, %r2, %r3;
	@%p2 bra 	$L__BB1_2;
	bra.uni 	$L__BB1_4;

$L__BB1_2:
	mul.lo.s32 	%r4, %r1, %r35;
	mov.u32 	%r5, %ntid.x;
	cvta.to.global.u64 	%rd3, %rd11;
	mov.u32 	%r65, %r2;

$L__BB1_3:
	add.s32 	%r39, %r65, %r4;
	mul.wide.s32 	%rd18, %r39, 4;
	add.s64 	%rd19, %rd3, %rd18;
	ld.global.nc.f32 	%f9, [%rd19];
	shl.b32 	%r40, %r65, 2;
	mov.u32 	%r41, sh;
	add.s32 	%r42, %r41, %r40;
	st.shared.f32 	[%r42], %f9;
	add.s32 	%r65, %r65, %r5;
	setp.lt.s32 	%p3, %r65, %r3;
	@%p3 bra 	$L__BB1_3;

$L__BB1_4:
	mov.u32 	%r43, %ntid.x;
	bar.sync 	0;
	add.s32 	%r44, %r37, %r3;
	add.s32 	%r8, %r44, -1;
	mul.lo.s32 	%r9, %r1, %r36;
	mov.u32 	%r45, %ctaid.x;
	mad.lo.s32 	%r68, %r45, %r43, %r2;
	mov.u32 	%r46, %nctaid.x;
	mul.lo.s32 	%r11, %r46, %r43;
	setp.ge.s32 	%p4, %r68, %r36;
	@%p4 bra 	$L__BB1_21;

	setp.gt.s32 	%p5, %r3, 0;
	@%p5 bra 	$L__BB1_11;
	bra.uni 	$L__BB1_6;

$L__BB1_11:
	add.s32 	%r21, %r3, -1;
	and.b32  	%r22, %r3, 3;
	sub.s32 	%r23, %r3, %r22;
	add.s64 	%rd5, %rd2, 8;
	mov.u32 	%r57, 1;
	sub.s32 	%r24, %r57, %r3;

$L__BB1_12:
	setp.lt.s32 	%p15, %r68, %r8;
	mov.f32 	%f43, 0f7FC00000;
	@%p15 bra 	$L__BB1_20;

	setp.lt.u32 	%p16, %r21, 3;
	add.s32 	%r59, %r24, %r68;
	cvt.s64.s32 	%rd6, %r59;
	mov.f32 	%f43, 0f00000000;
	mov.u32 	%r74, 0;
	@%p16 bra 	$L__BB1_16;

	shl.b64 	%rd27, %rd6, 2;
	add.s64 	%rd33, %rd5, %rd27;
	mov.f32 	%f43, 0f00000000;
	mov.u32 	%r74, 0;
	mov.u32 	%r71, sh;
	mov.u32 	%r73, %r23;

$L__BB1_15:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f19, %f20, %f21, %f22}, [%r71];
	ld.global.nc.f32 	%f27, [%rd33+-8];
	fma.rn.ftz.f32 	%f28, %f27, %f19, %f43;
	ld.global.nc.f32 	%f29, [%rd33+-4];
	fma.rn.ftz.f32 	%f30, %f29, %f20, %f28;
	ld.global.nc.f32 	%f31, [%rd33];
	fma.rn.ftz.f32 	%f32, %f31, %f21, %f30;
	ld.global.nc.f32 	%f33, [%rd33+4];
	fma.rn.ftz.f32 	%f43, %f33, %f22, %f32;
	add.s32 	%r74, %r74, 4;
	add.s64 	%rd33, %rd33, 16;
	add.s32 	%r71, %r71, 16;
	add.s32 	%r73, %r73, -4;
	setp.ne.s32 	%p17, %r73, 0;
	@%p17 bra 	$L__BB1_15;

$L__BB1_16:
	setp.eq.s32 	%p18, %r22, 0;
	@%p18 bra 	$L__BB1_20;

	setp.eq.s32 	%p19, %r22, 1;
	cvt.s64.s32 	%rd28, %r74;
	add.s64 	%rd29, %rd28, %rd6;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd10, %rd2, %rd30;
	shl.b32 	%r62, %r74, 2;
	mov.u32 	%r63, sh;
	add.s32 	%r33, %r63, %r62;
	ld.shared.f32 	%f34, [%r33];
	ld.global.nc.f32 	%f35, [%rd10];
	fma.rn.ftz.f32 	%f43, %f35, %f34, %f43;
	@%p19 bra 	$L__BB1_20;

	setp.eq.s32 	%p20, %r22, 2;
	ld.shared.f32 	%f36, [%r33+4];
	ld.global.nc.f32 	%f37, [%rd10+4];
	fma.rn.ftz.f32 	%f43, %f37, %f36, %f43;
	@%p20 bra 	$L__BB1_20;

	ld.shared.f32 	%f38, [%r33+8];
	ld.global.nc.f32 	%f39, [%rd10+8];
	fma.rn.ftz.f32 	%f43, %f39, %f38, %f43;

$L__BB1_20:
	add.s32 	%r64, %r68, %r9;
	mul.wide.s32 	%rd31, %r64, 4;
	add.s64 	%rd32, %rd1, %rd31;
	st.global.f32 	[%rd32], %f43;
	add.s32 	%r68, %r68, %r11;
	setp.lt.s32 	%p21, %r68, %r36;
	@%p21 bra 	$L__BB1_12;
	bra.uni 	$L__BB1_21;

$L__BB1_6:
	add.s32 	%r47, %r11, %r36;
	add.s32 	%r48, %r68, %r11;
	not.b32 	%r49, %r48;
	add.s32 	%r50, %r47, %r49;
	div.u32 	%r12, %r50, %r11;
	add.s32 	%r51, %r12, 1;
	and.b32  	%r67, %r51, 3;
	setp.eq.s32 	%p6, %r67, 0;
	@%p6 bra 	$L__BB1_8;

$L__BB1_7:
	.pragma "nounroll";
	setp.lt.s32 	%p7, %r68, %r8;
	selp.f32 	%f10, 0f7FC00000, 0f00000000, %p7;
	add.s32 	%r52, %r68, %r9;
	mul.wide.s32 	%rd20, %r52, 4;
	add.s64 	%rd21, %rd1, %rd20;
	st.global.f32 	[%rd21], %f10;
	add.s32 	%r68, %r68, %r11;
	add.s32 	%r67, %r67, -1;
	setp.ne.s32 	%p8, %r67, 0;
	@%p8 bra 	$L__BB1_7;

$L__BB1_8:
	setp.lt.u32 	%p9, %r12, 3;
	@%p9 bra 	$L__BB1_21;

	mul.wide.s32 	%rd4, %r11, 4;

$L__BB1_10:
	setp.lt.s32 	%p10, %r68, %r8;
	selp.f32 	%f11, 0f7FC00000, 0f00000000, %p10;
	add.s32 	%r53, %r68, %r9;
	mul.wide.s32 	%rd22, %r53, 4;
	add.s64 	%rd23, %rd1, %rd22;
	st.global.f32 	[%rd23], %f11;
	add.s32 	%r54, %r68, %r11;
	setp.lt.s32 	%p11, %r54, %r8;
	selp.f32 	%f12, 0f7FC00000, 0f00000000, %p11;
	add.s64 	%rd24, %rd23, %rd4;
	st.global.f32 	[%rd24], %f12;
	add.s32 	%r55, %r54, %r11;
	setp.lt.s32 	%p12, %r55, %r8;
	selp.f32 	%f13, 0f7FC00000, 0f00000000, %p12;
	add.s64 	%rd25, %rd24, %rd4;
	st.global.f32 	[%rd25], %f13;
	add.s32 	%r56, %r55, %r11;
	setp.lt.s32 	%p13, %r56, %r8;
	selp.f32 	%f14, 0f7FC00000, 0f00000000, %p13;
	add.s64 	%rd26, %rd25, %rd4;
	st.global.f32 	[%rd26], %f14;
	add.s32 	%r68, %r56, %r11;
	setp.lt.s32 	%p14, %r68, %r36;
	@%p14 bra 	$L__BB1_10;

$L__BB1_21:
	ret;

}
	// .globl	alma_batch_tiled_f32_tile128
.visible .entry alma_batch_tiled_f32_tile128(
	.param .u64 alma_batch_tiled_f32_tile128_param_0,
	.param .u64 alma_batch_tiled_f32_tile128_param_1,
	.param .u64 alma_batch_tiled_f32_tile128_param_2,
	.param .u64 alma_batch_tiled_f32_tile128_param_3,
	.param .u32 alma_batch_tiled_f32_tile128_param_4,
	.param .u32 alma_batch_tiled_f32_tile128_param_5,
	.param .u32 alma_batch_tiled_f32_tile128_param_6,
	.param .u32 alma_batch_tiled_f32_tile128_param_7,
	.param .u64 alma_batch_tiled_f32_tile128_param_8
)
{
	.reg .pred 	%p<64>;
	.reg .f32 	%f<67>;
	.reg .b32 	%r<346>;
	.reg .b64 	%rd<106>;


	ld.param.u64 	%rd41, [alma_batch_tiled_f32_tile128_param_0];
	ld.param.u64 	%rd42, [alma_batch_tiled_f32_tile128_param_1];
	ld.param.u64 	%rd43, [alma_batch_tiled_f32_tile128_param_2];
	ld.param.u32 	%r131, [alma_batch_tiled_f32_tile128_param_4];
	ld.param.u32 	%r132, [alma_batch_tiled_f32_tile128_param_5];
	ld.param.u32 	%r133, [alma_batch_tiled_f32_tile128_param_6];
	ld.param.u32 	%r134, [alma_batch_tiled_f32_tile128_param_7];
	ld.param.u64 	%rd44, [alma_batch_tiled_f32_tile128_param_8];
	cvta.to.global.u64 	%rd1, %rd41;
	cvta.to.global.u64 	%rd2, %rd42;
	mov.u32 	%r135, %ntid.x;
	setp.ne.s32 	%p1, %r135, 128;
	@%p1 bra 	$L__BB2_67;

	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p2, %r1, %r133;
	@%p2 bra 	$L__BB2_67;

	cvta.to.global.u64 	%rd45, %rd43;
	mul.wide.s32 	%rd46, %r1, 4;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.nc.u32 	%r2, [%rd47];
	mov.u32 	%r136, %ctaid.x;
	shl.b32 	%r3, %r136, 7;
	setp.ge.s32 	%p3, %r3, %r132;
	@%p3 bra 	$L__BB2_67;

	add.s32 	%r4, %r2, 127;
	shl.b32 	%r137, %r2, 2;
	add.s32 	%r138, %r137, 15;
	and.b32  	%r5, %r138, -16;
	mul.lo.s32 	%r139, %r1, %r131;
	cvt.s64.s32 	%rd3, %r139;
	mul.wide.s32 	%rd48, %r139, 4;
	add.s64 	%rd49, %rd42, %rd48;
	and.b64  	%rd50, %rd49, 15;
	setp.eq.s64 	%p4, %rd50, 0;
	@%p4 bra 	$L__BB2_11;

	mov.u32 	%r304, %tid.x;
	setp.ge.s32 	%p5, %r304, %r2;
	@%p5 bra 	$L__BB2_21;

	not.b32 	%r140, %r304;
	add.s32 	%r7, %r2, %r140;
	shr.u32 	%r141, %r7, 7;
	add.s32 	%r142, %r141, 1;
	and.b32  	%r303, %r142, 3;
	setp.eq.s32 	%p6, %r303, 0;
	@%p6 bra 	$L__BB2_8;

	shl.b32 	%r143, %r304, 2;
	mov.u32 	%r144, shraw;
	add.s32 	%r301, %r144, %r143;
	cvt.s64.s32 	%rd51, %r304;
	add.s64 	%rd52, %rd51, %rd3;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd94, %rd2, %rd53;

$L__BB2_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f18, [%rd94];
	st.shared.f32 	[%r301], %f18;
	add.s32 	%r304, %r304, 128;
	add.s32 	%r301, %r301, 512;
	add.s64 	%rd94, %rd94, 512;
	add.s32 	%r303, %r303, -1;
	setp.ne.s32 	%p7, %r303, 0;
	@%p7 bra 	$L__BB2_7;

$L__BB2_8:
	setp.lt.u32 	%p8, %r7, 384;
	@%p8 bra 	$L__BB2_21;

	shl.b32 	%r145, %r304, 2;
	mov.u32 	%r146, shraw;
	add.s32 	%r147, %r146, %r145;
	add.s32 	%r305, %r147, 1024;
	cvt.s64.s32 	%rd54, %r304;
	add.s64 	%rd55, %rd54, %rd3;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd57, %rd2, %rd56;
	add.s64 	%rd95, %rd57, 1024;

$L__BB2_10:
	ld.global.nc.f32 	%f19, [%rd95+-1024];
	st.shared.f32 	[%r305+-1024], %f19;
	ld.global.nc.f32 	%f20, [%rd95+-512];
	st.shared.f32 	[%r305+-512], %f20;
	ld.global.nc.f32 	%f21, [%rd95];
	st.shared.f32 	[%r305], %f21;
	ld.global.nc.f32 	%f22, [%rd95+512];
	st.shared.f32 	[%r305+512], %f22;
	add.s32 	%r305, %r305, 2048;
	add.s64 	%rd95, %rd95, 2048;
	add.s32 	%r304, %r304, 512;
	setp.lt.s32 	%p9, %r304, %r2;
	@%p9 bra 	$L__BB2_10;
	bra.uni 	$L__BB2_21;

$L__BB2_11:
	shr.s32 	%r22, %r2, 2;
	mov.u32 	%r23, %tid.x;
	setp.ge.s32 	%p10, %r23, %r22;
	@%p10 bra 	$L__BB2_18;

	not.b32 	%r148, %r23;
	add.s32 	%r24, %r22, %r148;
	shr.u32 	%r149, %r24, 7;
	add.s32 	%r150, %r149, 1;
	and.b32  	%r309, %r150, 3;
	setp.eq.s32 	%p11, %r309, 0;
	mov.u32 	%r310, %r23;
	@%p11 bra 	$L__BB2_15;

	mul.wide.s32 	%rd58, %r23, 4;
	add.s64 	%rd59, %rd58, %rd3;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd96, %rd2, %rd60;
	shl.b32 	%r151, %r23, 4;
	mov.u32 	%r152, shraw;
	add.s32 	%r307, %r152, %r151;
	mov.u32 	%r310, %r23;

$L__BB2_14:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r153, %r154, %r155, %r156}, [%rd96];
	st.shared.v4.u32 	[%r307], {%r153, %r154, %r155, %r156};
	add.s32 	%r310, %r310, 128;
	add.s64 	%rd96, %rd96, 2048;
	add.s32 	%r307, %r307, 2048;
	add.s32 	%r309, %r309, -1;
	setp.ne.s32 	%p12, %r309, 0;
	@%p12 bra 	$L__BB2_14;

$L__BB2_15:
	setp.lt.u32 	%p13, %r24, 384;
	@%p13 bra 	$L__BB2_18;

	shl.b32 	%r161, %r310, 4;
	mov.u32 	%r162, shraw;
	add.s32 	%r163, %r162, %r161;
	add.s32 	%r311, %r163, 4096;
	mul.wide.s32 	%rd61, %r310, 4;
	add.s64 	%rd62, %rd61, %rd3;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd64, %rd2, %rd63;
	add.s64 	%rd97, %rd64, 4096;

$L__BB2_17:
	ld.global.nc.v4.u32 	{%r164, %r165, %r166, %r167}, [%rd97+-4096];
	st.shared.v4.u32 	[%r311+-4096], {%r164, %r165, %r166, %r167};
	ld.global.nc.v4.u32 	{%r172, %r173, %r174, %r175}, [%rd97+-2048];
	st.shared.v4.u32 	[%r311+-2048], {%r172, %r173, %r174, %r175};
	ld.global.nc.v4.u32 	{%r180, %r181, %r182, %r183}, [%rd97];
	st.shared.v4.u32 	[%r311], {%r180, %r181, %r182, %r183};
	ld.global.nc.v4.u32 	{%r188, %r189, %r190, %r191}, [%rd97+2048];
	st.shared.v4.u32 	[%r311+2048], {%r188, %r189, %r190, %r191};
	add.s32 	%r311, %r311, 8192;
	add.s64 	%rd97, %rd97, 8192;
	add.s32 	%r310, %r310, 512;
	setp.lt.s32 	%p14, %r310, %r22;
	@%p14 bra 	$L__BB2_17;

$L__BB2_18:
	setp.ne.s32 	%p15, %r23, 0;
	and.b32  	%r39, %r2, 3;
	setp.eq.s32 	%p16, %r39, 0;
	or.pred  	%p17, %p15, %p16;
	@%p17 bra 	$L__BB2_21;

	and.b32  	%r197, %r2, -4;
	and.b32  	%r199, %r137, -16;
	mov.u32 	%r200, shraw;
	add.s32 	%r313, %r200, %r199;
	cvt.s64.s32 	%rd65, %r197;
	add.s64 	%rd66, %rd65, %rd3;
	shl.b64 	%rd67, %rd66, 2;
	add.s64 	%rd98, %rd2, %rd67;
	mov.u32 	%r314, 0;

$L__BB2_20:
	ld.global.nc.f32 	%f23, [%rd98];
	st.shared.f32 	[%r313], %f23;
	add.s32 	%r313, %r313, 4;
	add.s64 	%rd98, %rd98, 4;
	add.s32 	%r314, %r314, 1;
	setp.lt.u32 	%p18, %r314, %r39;
	@%p18 bra 	$L__BB2_20;

$L__BB2_21:
	bar.sync 	0;
	add.s32 	%r201, %r3, 1;
	sub.s32 	%r45, %r201, %r2;
	setp.gt.s32 	%p19, %r45, -1;
	add.s32 	%r202, %r3, 128;
	setp.le.s32 	%p20, %r202, %r132;
	and.pred  	%p21, %p19, %p20;
	@%p21 bra 	$L__BB2_39;
	bra.uni 	$L__BB2_22;

$L__BB2_39:
	cvt.s64.s32 	%rd25, %r45;
	mul.wide.s32 	%rd71, %r45, 4;
	add.s64 	%rd72, %rd41, %rd71;
	and.b64  	%rd73, %rd72, 15;
	setp.eq.s64 	%p42, %rd73, 0;
	@%p42 bra 	$L__BB2_47;

	mov.u32 	%r338, %tid.x;
	setp.ge.s32 	%p43, %r338, %r4;
	@%p43 bra 	$L__BB2_57;

	add.s32 	%r220, %r2, 126;
	sub.s32 	%r73, %r220, %r338;
	shr.u32 	%r221, %r73, 7;
	add.s32 	%r222, %r221, 1;
	and.b32  	%r326, %r222, 3;
	setp.eq.s32 	%p44, %r326, 0;
	mov.u32 	%r327, %r338;
	@%p44 bra 	$L__BB2_44;

	shl.b32 	%r223, %r338, 2;
	add.s32 	%r224, %r5, %r223;
	mov.u32 	%r225, shraw;
	add.s32 	%r324, %r225, %r224;
	cvt.s64.s32 	%rd74, %r338;
	add.s64 	%rd75, %rd74, %rd25;
	shl.b64 	%rd76, %rd75, 2;
	add.s64 	%rd101, %rd1, %rd76;
	mov.u32 	%r327, %r338;

$L__BB2_43:
	.pragma "nounroll";
	ld.global.nc.f32 	%f29, [%rd101];
	st.shared.f32 	[%r324], %f29;
	add.s32 	%r327, %r327, 128;
	add.s32 	%r324, %r324, 512;
	add.s64 	%rd101, %rd101, 512;
	add.s32 	%r326, %r326, -1;
	setp.ne.s32 	%p45, %r326, 0;
	@%p45 bra 	$L__BB2_43;

$L__BB2_44:
	setp.lt.u32 	%p46, %r73, 384;
	@%p46 bra 	$L__BB2_57;

	shl.b32 	%r226, %r327, 2;
	add.s32 	%r227, %r5, %r226;
	mov.u32 	%r228, shraw;
	add.s32 	%r229, %r228, %r227;
	add.s32 	%r328, %r229, 1024;
	cvt.s64.s32 	%rd77, %r327;
	add.s64 	%rd78, %rd77, %rd25;
	shl.b64 	%rd79, %rd78, 2;
	add.s64 	%rd80, %rd1, %rd79;
	add.s64 	%rd102, %rd80, 1024;

$L__BB2_46:
	ld.global.nc.f32 	%f30, [%rd102+-1024];
	st.shared.f32 	[%r328+-1024], %f30;
	ld.global.nc.f32 	%f31, [%rd102+-512];
	st.shared.f32 	[%r328+-512], %f31;
	ld.global.nc.f32 	%f32, [%rd102];
	st.shared.f32 	[%r328], %f32;
	ld.global.nc.f32 	%f33, [%rd102+512];
	st.shared.f32 	[%r328+512], %f33;
	add.s32 	%r328, %r328, 2048;
	add.s64 	%rd102, %rd102, 2048;
	add.s32 	%r327, %r327, 512;
	setp.lt.s32 	%p47, %r327, %r4;
	@%p47 bra 	$L__BB2_46;
	bra.uni 	$L__BB2_57;

$L__BB2_22:
	mov.u32 	%r338, %tid.x;
	setp.ge.s32 	%p22, %r338, %r4;
	@%p22 bra 	$L__BB2_57;

	add.s32 	%r203, %r2, 126;
	sub.s32 	%r47, %r203, %r338;
	shr.u32 	%r204, %r47, 7;
	add.s32 	%r205, %r204, 1;
	and.b32  	%r318, %r205, 3;
	setp.eq.s32 	%p23, %r318, 0;
	mov.u32 	%r319, %r338;
	@%p23 bra 	$L__BB2_28;

	shl.b32 	%r206, %r338, 2;
	add.s32 	%r207, %r5, %r206;
	mov.u32 	%r208, shraw;
	add.s32 	%r316, %r208, %r207;
	add.s32 	%r209, %r338, %r3;
	add.s32 	%r210, %r209, 1;
	sub.s32 	%r315, %r210, %r2;
	mul.wide.s32 	%rd68, %r315, 4;
	add.s64 	%rd99, %rd1, %rd68;
	mov.u32 	%r319, %r338;

$L__BB2_25:
	.pragma "nounroll";
	setp.ge.s32 	%p24, %r315, %r132;
	setp.lt.s32 	%p25, %r315, 0;
	mov.f32 	%f57, 0f00000000;
	or.pred  	%p26, %p25, %p24;
	@%p26 bra 	$L__BB2_27;

	ld.global.nc.f32 	%f57, [%rd99];

$L__BB2_27:
	st.shared.f32 	[%r316], %f57;
	add.s32 	%r319, %r319, 128;
	add.s32 	%r316, %r316, 512;
	add.s64 	%rd99, %rd99, 512;
	add.s32 	%r315, %r315, 128;
	add.s32 	%r318, %r318, -1;
	setp.ne.s32 	%p27, %r318, 0;
	@%p27 bra 	$L__BB2_25;

$L__BB2_28:
	setp.lt.u32 	%p28, %r47, 384;
	@%p28 bra 	$L__BB2_57;

	add.s32 	%r211, %r319, %r3;
	add.s32 	%r212, %r211, 1;
	sub.s32 	%r321, %r212, %r2;
	mul.wide.s32 	%rd70, %r321, 4;
	add.s64 	%rd100, %rd1, %rd70;
	add.s32 	%r213, %r211, 385;
	sub.s32 	%r320, %r213, %r2;
	shl.b32 	%r214, %r319, 2;
	add.s32 	%r215, %r5, %r214;
	mov.u32 	%r216, shraw;
	add.s32 	%r323, %r216, %r215;

$L__BB2_30:
	setp.ge.s32 	%p29, %r321, %r132;
	setp.lt.s32 	%p30, %r321, 0;
	mov.f32 	%f59, 0f00000000;
	or.pred  	%p31, %p30, %p29;
	mov.f32 	%f58, %f59;
	@%p31 bra 	$L__BB2_32;

	ld.global.nc.f32 	%f58, [%rd100];

$L__BB2_32:
	st.shared.f32 	[%r323], %f58;
	add.s32 	%r217, %r320, -256;
	setp.lt.s32 	%p32, %r217, 0;
	setp.ge.s32 	%p33, %r217, %r132;
	or.pred  	%p34, %p32, %p33;
	@%p34 bra 	$L__BB2_34;

	ld.global.nc.f32 	%f59, [%rd100+512];

$L__BB2_34:
	st.shared.f32 	[%r323+512], %f59;
	add.s32 	%r218, %r320, -128;
	setp.lt.s32 	%p35, %r218, 0;
	setp.ge.s32 	%p36, %r218, %r132;
	mov.f32 	%f61, 0f00000000;
	or.pred  	%p37, %p35, %p36;
	mov.f32 	%f60, %f61;
	@%p37 bra 	$L__BB2_36;

	ld.global.nc.f32 	%f60, [%rd100+1024];

$L__BB2_36:
	st.shared.f32 	[%r323+1024], %f60;
	add.s32 	%r219, %r321, 384;
	setp.lt.s32 	%p38, %r219, 0;
	setp.ge.s32 	%p39, %r219, %r132;
	or.pred  	%p40, %p38, %p39;
	@%p40 bra 	$L__BB2_38;

	ld.global.nc.f32 	%f61, [%rd100+1536];

$L__BB2_38:
	add.s64 	%rd100, %rd100, 2048;
	add.s32 	%r68, %r323, 2048;
	st.shared.f32 	[%r323+1536], %f61;
	add.s32 	%r321, %r321, 512;
	add.s32 	%r320, %r320, 512;
	add.s32 	%r319, %r319, 512;
	setp.lt.s32 	%p41, %r319, %r4;
	mov.u32 	%r323, %r68;
	@%p41 bra 	$L__BB2_30;
	bra.uni 	$L__BB2_57;

$L__BB2_47:
	shr.s32 	%r88, %r4, 2;
	mov.u32 	%r338, %tid.x;
	setp.ge.s32 	%p48, %r338, %r88;
	@%p48 bra 	$L__BB2_54;

	not.b32 	%r230, %r338;
	add.s32 	%r90, %r88, %r230;
	shr.u32 	%r231, %r90, 7;
	add.s32 	%r232, %r231, 1;
	and.b32  	%r332, %r232, 3;
	setp.eq.s32 	%p49, %r332, 0;
	mov.u32 	%r333, %r338;
	@%p49 bra 	$L__BB2_51;

	mul.wide.s32 	%rd81, %r338, 4;
	add.s64 	%rd82, %rd81, %rd25;
	shl.b64 	%rd83, %rd82, 2;
	add.s64 	%rd103, %rd1, %rd83;
	shl.b32 	%r233, %r338, 4;
	add.s32 	%r234, %r5, %r233;
	mov.u32 	%r235, shraw;
	add.s32 	%r330, %r235, %r234;
	mov.u32 	%r333, %r338;

$L__BB2_50:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r236, %r237, %r238, %r239}, [%rd103];
	st.shared.v4.u32 	[%r330], {%r236, %r237, %r238, %r239};
	add.s32 	%r333, %r333, 128;
	add.s64 	%rd103, %rd103, 2048;
	add.s32 	%r330, %r330, 2048;
	add.s32 	%r332, %r332, -1;
	setp.ne.s32 	%p50, %r332, 0;
	@%p50 bra 	$L__BB2_50;

$L__BB2_51:
	setp.lt.u32 	%p51, %r90, 384;
	@%p51 bra 	$L__BB2_54;

	shl.b32 	%r244, %r333, 4;
	add.s32 	%r245, %r5, %r244;
	mov.u32 	%r246, shraw;
	add.s32 	%r247, %r246, %r245;
	add.s32 	%r334, %r247, 4096;
	mul.wide.s32 	%rd84, %r333, 4;
	add.s64 	%rd85, %rd84, %rd25;
	shl.b64 	%rd86, %rd85, 2;
	add.s64 	%rd87, %rd1, %rd86;
	add.s64 	%rd104, %rd87, 4096;

$L__BB2_53:
	ld.global.nc.v4.u32 	{%r248, %r249, %r250, %r251}, [%rd104+-4096];
	st.shared.v4.u32 	[%r334+-4096], {%r248, %r249, %r250, %r251};
	ld.global.nc.v4.u32 	{%r256, %r257, %r258, %r259}, [%rd104+-2048];
	st.shared.v4.u32 	[%r334+-2048], {%r256, %r257, %r258, %r259};
	ld.global.nc.v4.u32 	{%r264, %r265, %r266, %r267}, [%rd104];
	st.shared.v4.u32 	[%r334], {%r264, %r265, %r266, %r267};
	ld.global.nc.v4.u32 	{%r272, %r273, %r274, %r275}, [%rd104+2048];
	st.shared.v4.u32 	[%r334+2048], {%r272, %r273, %r274, %r275};
	add.s32 	%r334, %r334, 8192;
	add.s64 	%rd104, %rd104, 8192;
	add.s32 	%r333, %r333, 512;
	setp.lt.s32 	%p52, %r333, %r88;
	@%p52 bra 	$L__BB2_53;

$L__BB2_54:
	setp.ne.s32 	%p53, %r338, 0;
	and.b32  	%r105, %r4, 3;
	setp.eq.s32 	%p54, %r105, 0;
	or.pred  	%p55, %p53, %p54;
	@%p55 bra 	$L__BB2_57;

	and.b32  	%r281, %r4, -4;
	shl.b32 	%r282, %r4, 2;
	and.b32  	%r283, %r282, -16;
	add.s32 	%r284, %r5, %r283;
	mov.u32 	%r285, shraw;
	add.s32 	%r336, %r285, %r284;
	cvt.s64.s32 	%rd88, %r281;
	add.s64 	%rd89, %rd88, %rd25;
	shl.b64 	%rd90, %rd89, 2;
	add.s64 	%rd105, %rd1, %rd90;
	mov.u32 	%r338, 0;
	mov.u32 	%r337, %r338;

$L__BB2_56:
	ld.global.nc.f32 	%f34, [%rd105];
	st.shared.f32 	[%r336], %f34;
	add.s32 	%r336, %r336, 4;
	add.s64 	%rd105, %rd105, 4;
	add.s32 	%r337, %r337, 1;
	setp.lt.u32 	%p56, %r337, %r105;
	@%p56 bra 	$L__BB2_56;

$L__BB2_57:
	bar.sync 	0;
	add.s32 	%r112, %r338, %r3;
	setp.ge.s32 	%p57, %r112, %r132;
	@%p57 bra 	$L__BB2_67;

	add.s32 	%r287, %r134, %r2;
	add.s32 	%r288, %r287, -1;
	setp.lt.s32 	%p58, %r112, %r288;
	mov.f32 	%f66, 0f7FC00000;
	@%p58 bra 	$L__BB2_66;

	setp.lt.s32 	%p59, %r2, 1;
	mov.f32 	%f66, 0f00000000;
	@%p59 bra 	$L__BB2_66;

	add.s32 	%r290, %r2, -1;
	and.b32  	%r345, %r2, 3;
	setp.lt.u32 	%p60, %r290, 3;
	mov.f32 	%f66, 0f00000000;
	mov.u32 	%r342, 0;
	@%p60 bra 	$L__BB2_63;

	sub.s32 	%r341, %r2, %r345;
	shl.b32 	%r293, %r338, 2;
	add.s32 	%r115, %r5, %r293;
	mov.f32 	%f66, 0f00000000;
	mov.u32 	%r342, 0;
	mov.u32 	%r339, shraw;

$L__BB2_62:
	.pragma "nounroll";
	add.s32 	%r294, %r339, %r115;
	ld.shared.v4.f32 	{%f40, %f41, %f42, %f43}, [%r339];
	ld.shared.f32 	%f48, [%r294];
	fma.rn.ftz.f32 	%f49, %f48, %f40, %f66;
	ld.shared.f32 	%f50, [%r294+4];
	fma.rn.ftz.f32 	%f51, %f50, %f41, %f49;
	ld.shared.f32 	%f52, [%r294+8];
	fma.rn.ftz.f32 	%f53, %f52, %f42, %f51;
	ld.shared.f32 	%f54, [%r294+12];
	fma.rn.ftz.f32 	%f66, %f54, %f43, %f53;
	add.s32 	%r342, %r342, 4;
	add.s32 	%r339, %r339, 16;
	add.s32 	%r341, %r341, -4;
	setp.ne.s32 	%p61, %r341, 0;
	@%p61 bra 	$L__BB2_62;

$L__BB2_63:
	setp.eq.s32 	%p62, %r345, 0;
	@%p62 bra 	$L__BB2_66;

	shl.b32 	%r295, %r342, 2;
	mov.u32 	%r296, shraw;
	add.s32 	%r344, %r296, %r295;
	add.s32 	%r297, %r338, %r342;
	shl.b32 	%r298, %r297, 2;
	add.s32 	%r299, %r5, %r298;
	add.s32 	%r343, %r296, %r299;

$L__BB2_65:
	.pragma "nounroll";
	ld.shared.f32 	%f55, [%r344];
	ld.shared.f32 	%f56, [%r343];
	fma.rn.ftz.f32 	%f66, %f56, %f55, %f66;
	add.s32 	%r344, %r344, 4;
	add.s32 	%r343, %r343, 4;
	add.s32 	%r345, %r345, -1;
	setp.ne.s32 	%p63, %r345, 0;
	@%p63 bra 	$L__BB2_65;

$L__BB2_66:
	mad.lo.s32 	%r300, %r1, %r132, %r112;
	cvta.to.global.u64 	%rd91, %rd44;
	mul.wide.s32 	%rd92, %r300, 4;
	add.s64 	%rd93, %rd91, %rd92;
	st.global.f32 	[%rd93], %f66;

$L__BB2_67:
	ret;

}
	// .globl	alma_batch_tiled_f32_tile256
.visible .entry alma_batch_tiled_f32_tile256(
	.param .u64 alma_batch_tiled_f32_tile256_param_0,
	.param .u64 alma_batch_tiled_f32_tile256_param_1,
	.param .u64 alma_batch_tiled_f32_tile256_param_2,
	.param .u64 alma_batch_tiled_f32_tile256_param_3,
	.param .u32 alma_batch_tiled_f32_tile256_param_4,
	.param .u32 alma_batch_tiled_f32_tile256_param_5,
	.param .u32 alma_batch_tiled_f32_tile256_param_6,
	.param .u32 alma_batch_tiled_f32_tile256_param_7,
	.param .u64 alma_batch_tiled_f32_tile256_param_8
)
{
	.reg .pred 	%p<64>;
	.reg .f32 	%f<67>;
	.reg .b32 	%r<346>;
	.reg .b64 	%rd<106>;


	ld.param.u64 	%rd41, [alma_batch_tiled_f32_tile256_param_0];
	ld.param.u64 	%rd42, [alma_batch_tiled_f32_tile256_param_1];
	ld.param.u64 	%rd43, [alma_batch_tiled_f32_tile256_param_2];
	ld.param.u32 	%r131, [alma_batch_tiled_f32_tile256_param_4];
	ld.param.u32 	%r132, [alma_batch_tiled_f32_tile256_param_5];
	ld.param.u32 	%r133, [alma_batch_tiled_f32_tile256_param_6];
	ld.param.u32 	%r134, [alma_batch_tiled_f32_tile256_param_7];
	ld.param.u64 	%rd44, [alma_batch_tiled_f32_tile256_param_8];
	cvta.to.global.u64 	%rd1, %rd41;
	cvta.to.global.u64 	%rd2, %rd42;
	mov.u32 	%r135, %ntid.x;
	setp.ne.s32 	%p1, %r135, 256;
	@%p1 bra 	$L__BB3_67;

	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p2, %r1, %r133;
	@%p2 bra 	$L__BB3_67;

	cvta.to.global.u64 	%rd45, %rd43;
	mul.wide.s32 	%rd46, %r1, 4;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.nc.u32 	%r2, [%rd47];
	mov.u32 	%r136, %ctaid.x;
	shl.b32 	%r3, %r136, 8;
	setp.ge.s32 	%p3, %r3, %r132;
	@%p3 bra 	$L__BB3_67;

	add.s32 	%r4, %r2, 255;
	shl.b32 	%r137, %r2, 2;
	add.s32 	%r138, %r137, 15;
	and.b32  	%r5, %r138, -16;
	mul.lo.s32 	%r139, %r1, %r131;
	cvt.s64.s32 	%rd3, %r139;
	mul.wide.s32 	%rd48, %r139, 4;
	add.s64 	%rd49, %rd42, %rd48;
	and.b64  	%rd50, %rd49, 15;
	setp.eq.s64 	%p4, %rd50, 0;
	@%p4 bra 	$L__BB3_11;

	mov.u32 	%r304, %tid.x;
	setp.ge.s32 	%p5, %r304, %r2;
	@%p5 bra 	$L__BB3_21;

	not.b32 	%r140, %r304;
	add.s32 	%r7, %r2, %r140;
	shr.u32 	%r141, %r7, 8;
	add.s32 	%r142, %r141, 1;
	and.b32  	%r303, %r142, 3;
	setp.eq.s32 	%p6, %r303, 0;
	@%p6 bra 	$L__BB3_8;

	shl.b32 	%r143, %r304, 2;
	mov.u32 	%r144, shraw;
	add.s32 	%r301, %r144, %r143;
	cvt.s64.s32 	%rd51, %r304;
	add.s64 	%rd52, %rd51, %rd3;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd94, %rd2, %rd53;

$L__BB3_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f18, [%rd94];
	st.shared.f32 	[%r301], %f18;
	add.s32 	%r304, %r304, 256;
	add.s32 	%r301, %r301, 1024;
	add.s64 	%rd94, %rd94, 1024;
	add.s32 	%r303, %r303, -1;
	setp.ne.s32 	%p7, %r303, 0;
	@%p7 bra 	$L__BB3_7;

$L__BB3_8:
	setp.lt.u32 	%p8, %r7, 768;
	@%p8 bra 	$L__BB3_21;

	shl.b32 	%r145, %r304, 2;
	mov.u32 	%r146, shraw;
	add.s32 	%r147, %r146, %r145;
	add.s32 	%r305, %r147, 2048;
	cvt.s64.s32 	%rd54, %r304;
	add.s64 	%rd55, %rd54, %rd3;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd57, %rd2, %rd56;
	add.s64 	%rd95, %rd57, 2048;

$L__BB3_10:
	ld.global.nc.f32 	%f19, [%rd95+-2048];
	st.shared.f32 	[%r305+-2048], %f19;
	ld.global.nc.f32 	%f20, [%rd95+-1024];
	st.shared.f32 	[%r305+-1024], %f20;
	ld.global.nc.f32 	%f21, [%rd95];
	st.shared.f32 	[%r305], %f21;
	ld.global.nc.f32 	%f22, [%rd95+1024];
	st.shared.f32 	[%r305+1024], %f22;
	add.s32 	%r305, %r305, 4096;
	add.s64 	%rd95, %rd95, 4096;
	add.s32 	%r304, %r304, 1024;
	setp.lt.s32 	%p9, %r304, %r2;
	@%p9 bra 	$L__BB3_10;
	bra.uni 	$L__BB3_21;

$L__BB3_11:
	shr.s32 	%r22, %r2, 2;
	mov.u32 	%r23, %tid.x;
	setp.ge.s32 	%p10, %r23, %r22;
	@%p10 bra 	$L__BB3_18;

	not.b32 	%r148, %r23;
	add.s32 	%r24, %r22, %r148;
	shr.u32 	%r149, %r24, 8;
	add.s32 	%r150, %r149, 1;
	and.b32  	%r309, %r150, 3;
	setp.eq.s32 	%p11, %r309, 0;
	mov.u32 	%r310, %r23;
	@%p11 bra 	$L__BB3_15;

	mul.wide.s32 	%rd58, %r23, 4;
	add.s64 	%rd59, %rd58, %rd3;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd96, %rd2, %rd60;
	shl.b32 	%r151, %r23, 4;
	mov.u32 	%r152, shraw;
	add.s32 	%r307, %r152, %r151;
	mov.u32 	%r310, %r23;

$L__BB3_14:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r153, %r154, %r155, %r156}, [%rd96];
	st.shared.v4.u32 	[%r307], {%r153, %r154, %r155, %r156};
	add.s32 	%r310, %r310, 256;
	add.s64 	%rd96, %rd96, 4096;
	add.s32 	%r307, %r307, 4096;
	add.s32 	%r309, %r309, -1;
	setp.ne.s32 	%p12, %r309, 0;
	@%p12 bra 	$L__BB3_14;

$L__BB3_15:
	setp.lt.u32 	%p13, %r24, 768;
	@%p13 bra 	$L__BB3_18;

	shl.b32 	%r161, %r310, 4;
	mov.u32 	%r162, shraw;
	add.s32 	%r163, %r162, %r161;
	add.s32 	%r311, %r163, 8192;
	mul.wide.s32 	%rd61, %r310, 4;
	add.s64 	%rd62, %rd61, %rd3;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd64, %rd2, %rd63;
	add.s64 	%rd97, %rd64, 8192;

$L__BB3_17:
	ld.global.nc.v4.u32 	{%r164, %r165, %r166, %r167}, [%rd97+-8192];
	st.shared.v4.u32 	[%r311+-8192], {%r164, %r165, %r166, %r167};
	ld.global.nc.v4.u32 	{%r172, %r173, %r174, %r175}, [%rd97+-4096];
	st.shared.v4.u32 	[%r311+-4096], {%r172, %r173, %r174, %r175};
	ld.global.nc.v4.u32 	{%r180, %r181, %r182, %r183}, [%rd97];
	st.shared.v4.u32 	[%r311], {%r180, %r181, %r182, %r183};
	ld.global.nc.v4.u32 	{%r188, %r189, %r190, %r191}, [%rd97+4096];
	st.shared.v4.u32 	[%r311+4096], {%r188, %r189, %r190, %r191};
	add.s32 	%r311, %r311, 16384;
	add.s64 	%rd97, %rd97, 16384;
	add.s32 	%r310, %r310, 1024;
	setp.lt.s32 	%p14, %r310, %r22;
	@%p14 bra 	$L__BB3_17;

$L__BB3_18:
	setp.ne.s32 	%p15, %r23, 0;
	and.b32  	%r39, %r2, 3;
	setp.eq.s32 	%p16, %r39, 0;
	or.pred  	%p17, %p15, %p16;
	@%p17 bra 	$L__BB3_21;

	and.b32  	%r197, %r2, -4;
	and.b32  	%r199, %r137, -16;
	mov.u32 	%r200, shraw;
	add.s32 	%r313, %r200, %r199;
	cvt.s64.s32 	%rd65, %r197;
	add.s64 	%rd66, %rd65, %rd3;
	shl.b64 	%rd67, %rd66, 2;
	add.s64 	%rd98, %rd2, %rd67;
	mov.u32 	%r314, 0;

$L__BB3_20:
	ld.global.nc.f32 	%f23, [%rd98];
	st.shared.f32 	[%r313], %f23;
	add.s32 	%r313, %r313, 4;
	add.s64 	%rd98, %rd98, 4;
	add.s32 	%r314, %r314, 1;
	setp.lt.u32 	%p18, %r314, %r39;
	@%p18 bra 	$L__BB3_20;

$L__BB3_21:
	bar.sync 	0;
	add.s32 	%r201, %r3, 1;
	sub.s32 	%r45, %r201, %r2;
	setp.gt.s32 	%p19, %r45, -1;
	add.s32 	%r202, %r3, 256;
	setp.le.s32 	%p20, %r202, %r132;
	and.pred  	%p21, %p19, %p20;
	@%p21 bra 	$L__BB3_39;
	bra.uni 	$L__BB3_22;

$L__BB3_39:
	cvt.s64.s32 	%rd25, %r45;
	mul.wide.s32 	%rd71, %r45, 4;
	add.s64 	%rd72, %rd41, %rd71;
	and.b64  	%rd73, %rd72, 15;
	setp.eq.s64 	%p42, %rd73, 0;
	@%p42 bra 	$L__BB3_47;

	mov.u32 	%r338, %tid.x;
	setp.ge.s32 	%p43, %r338, %r4;
	@%p43 bra 	$L__BB3_57;

	add.s32 	%r220, %r2, 254;
	sub.s32 	%r73, %r220, %r338;
	shr.u32 	%r221, %r73, 8;
	add.s32 	%r222, %r221, 1;
	and.b32  	%r326, %r222, 3;
	setp.eq.s32 	%p44, %r326, 0;
	mov.u32 	%r327, %r338;
	@%p44 bra 	$L__BB3_44;

	shl.b32 	%r223, %r338, 2;
	add.s32 	%r224, %r5, %r223;
	mov.u32 	%r225, shraw;
	add.s32 	%r324, %r225, %r224;
	cvt.s64.s32 	%rd74, %r338;
	add.s64 	%rd75, %rd74, %rd25;
	shl.b64 	%rd76, %rd75, 2;
	add.s64 	%rd101, %rd1, %rd76;
	mov.u32 	%r327, %r338;

$L__BB3_43:
	.pragma "nounroll";
	ld.global.nc.f32 	%f29, [%rd101];
	st.shared.f32 	[%r324], %f29;
	add.s32 	%r327, %r327, 256;
	add.s32 	%r324, %r324, 1024;
	add.s64 	%rd101, %rd101, 1024;
	add.s32 	%r326, %r326, -1;
	setp.ne.s32 	%p45, %r326, 0;
	@%p45 bra 	$L__BB3_43;

$L__BB3_44:
	setp.lt.u32 	%p46, %r73, 768;
	@%p46 bra 	$L__BB3_57;

	shl.b32 	%r226, %r327, 2;
	add.s32 	%r227, %r5, %r226;
	mov.u32 	%r228, shraw;
	add.s32 	%r229, %r228, %r227;
	add.s32 	%r328, %r229, 2048;
	cvt.s64.s32 	%rd77, %r327;
	add.s64 	%rd78, %rd77, %rd25;
	shl.b64 	%rd79, %rd78, 2;
	add.s64 	%rd80, %rd1, %rd79;
	add.s64 	%rd102, %rd80, 2048;

$L__BB3_46:
	ld.global.nc.f32 	%f30, [%rd102+-2048];
	st.shared.f32 	[%r328+-2048], %f30;
	ld.global.nc.f32 	%f31, [%rd102+-1024];
	st.shared.f32 	[%r328+-1024], %f31;
	ld.global.nc.f32 	%f32, [%rd102];
	st.shared.f32 	[%r328], %f32;
	ld.global.nc.f32 	%f33, [%rd102+1024];
	st.shared.f32 	[%r328+1024], %f33;
	add.s32 	%r328, %r328, 4096;
	add.s64 	%rd102, %rd102, 4096;
	add.s32 	%r327, %r327, 1024;
	setp.lt.s32 	%p47, %r327, %r4;
	@%p47 bra 	$L__BB3_46;
	bra.uni 	$L__BB3_57;

$L__BB3_22:
	mov.u32 	%r338, %tid.x;
	setp.ge.s32 	%p22, %r338, %r4;
	@%p22 bra 	$L__BB3_57;

	add.s32 	%r203, %r2, 254;
	sub.s32 	%r47, %r203, %r338;
	shr.u32 	%r204, %r47, 8;
	add.s32 	%r205, %r204, 1;
	and.b32  	%r318, %r205, 3;
	setp.eq.s32 	%p23, %r318, 0;
	mov.u32 	%r319, %r338;
	@%p23 bra 	$L__BB3_28;

	shl.b32 	%r206, %r338, 2;
	add.s32 	%r207, %r5, %r206;
	mov.u32 	%r208, shraw;
	add.s32 	%r316, %r208, %r207;
	add.s32 	%r209, %r338, %r3;
	add.s32 	%r210, %r209, 1;
	sub.s32 	%r315, %r210, %r2;
	mul.wide.s32 	%rd68, %r315, 4;
	add.s64 	%rd99, %rd1, %rd68;
	mov.u32 	%r319, %r338;

$L__BB3_25:
	.pragma "nounroll";
	setp.ge.s32 	%p24, %r315, %r132;
	setp.lt.s32 	%p25, %r315, 0;
	mov.f32 	%f57, 0f00000000;
	or.pred  	%p26, %p25, %p24;
	@%p26 bra 	$L__BB3_27;

	ld.global.nc.f32 	%f57, [%rd99];

$L__BB3_27:
	st.shared.f32 	[%r316], %f57;
	add.s32 	%r319, %r319, 256;
	add.s32 	%r316, %r316, 1024;
	add.s64 	%rd99, %rd99, 1024;
	add.s32 	%r315, %r315, 256;
	add.s32 	%r318, %r318, -1;
	setp.ne.s32 	%p27, %r318, 0;
	@%p27 bra 	$L__BB3_25;

$L__BB3_28:
	setp.lt.u32 	%p28, %r47, 768;
	@%p28 bra 	$L__BB3_57;

	add.s32 	%r211, %r319, %r3;
	add.s32 	%r212, %r211, 1;
	sub.s32 	%r321, %r212, %r2;
	mul.wide.s32 	%rd70, %r321, 4;
	add.s64 	%rd100, %rd1, %rd70;
	add.s32 	%r213, %r211, 769;
	sub.s32 	%r320, %r213, %r2;
	shl.b32 	%r214, %r319, 2;
	add.s32 	%r215, %r5, %r214;
	mov.u32 	%r216, shraw;
	add.s32 	%r323, %r216, %r215;

$L__BB3_30:
	setp.ge.s32 	%p29, %r321, %r132;
	setp.lt.s32 	%p30, %r321, 0;
	mov.f32 	%f59, 0f00000000;
	or.pred  	%p31, %p30, %p29;
	mov.f32 	%f58, %f59;
	@%p31 bra 	$L__BB3_32;

	ld.global.nc.f32 	%f58, [%rd100];

$L__BB3_32:
	st.shared.f32 	[%r323], %f58;
	add.s32 	%r217, %r320, -512;
	setp.lt.s32 	%p32, %r217, 0;
	setp.ge.s32 	%p33, %r217, %r132;
	or.pred  	%p34, %p32, %p33;
	@%p34 bra 	$L__BB3_34;

	ld.global.nc.f32 	%f59, [%rd100+1024];

$L__BB3_34:
	st.shared.f32 	[%r323+1024], %f59;
	add.s32 	%r218, %r320, -256;
	setp.lt.s32 	%p35, %r218, 0;
	setp.ge.s32 	%p36, %r218, %r132;
	mov.f32 	%f61, 0f00000000;
	or.pred  	%p37, %p35, %p36;
	mov.f32 	%f60, %f61;
	@%p37 bra 	$L__BB3_36;

	ld.global.nc.f32 	%f60, [%rd100+2048];

$L__BB3_36:
	st.shared.f32 	[%r323+2048], %f60;
	add.s32 	%r219, %r321, 768;
	setp.lt.s32 	%p38, %r219, 0;
	setp.ge.s32 	%p39, %r219, %r132;
	or.pred  	%p40, %p38, %p39;
	@%p40 bra 	$L__BB3_38;

	ld.global.nc.f32 	%f61, [%rd100+3072];

$L__BB3_38:
	add.s64 	%rd100, %rd100, 4096;
	add.s32 	%r68, %r323, 4096;
	st.shared.f32 	[%r323+3072], %f61;
	add.s32 	%r321, %r321, 1024;
	add.s32 	%r320, %r320, 1024;
	add.s32 	%r319, %r319, 1024;
	setp.lt.s32 	%p41, %r319, %r4;
	mov.u32 	%r323, %r68;
	@%p41 bra 	$L__BB3_30;
	bra.uni 	$L__BB3_57;

$L__BB3_47:
	shr.s32 	%r88, %r4, 2;
	mov.u32 	%r338, %tid.x;
	setp.ge.s32 	%p48, %r338, %r88;
	@%p48 bra 	$L__BB3_54;

	not.b32 	%r230, %r338;
	add.s32 	%r90, %r88, %r230;
	shr.u32 	%r231, %r90, 8;
	add.s32 	%r232, %r231, 1;
	and.b32  	%r332, %r232, 3;
	setp.eq.s32 	%p49, %r332, 0;
	mov.u32 	%r333, %r338;
	@%p49 bra 	$L__BB3_51;

	mul.wide.s32 	%rd81, %r338, 4;
	add.s64 	%rd82, %rd81, %rd25;
	shl.b64 	%rd83, %rd82, 2;
	add.s64 	%rd103, %rd1, %rd83;
	shl.b32 	%r233, %r338, 4;
	add.s32 	%r234, %r5, %r233;
	mov.u32 	%r235, shraw;
	add.s32 	%r330, %r235, %r234;
	mov.u32 	%r333, %r338;

$L__BB3_50:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r236, %r237, %r238, %r239}, [%rd103];
	st.shared.v4.u32 	[%r330], {%r236, %r237, %r238, %r239};
	add.s32 	%r333, %r333, 256;
	add.s64 	%rd103, %rd103, 4096;
	add.s32 	%r330, %r330, 4096;
	add.s32 	%r332, %r332, -1;
	setp.ne.s32 	%p50, %r332, 0;
	@%p50 bra 	$L__BB3_50;

$L__BB3_51:
	setp.lt.u32 	%p51, %r90, 768;
	@%p51 bra 	$L__BB3_54;

	shl.b32 	%r244, %r333, 4;
	add.s32 	%r245, %r5, %r244;
	mov.u32 	%r246, shraw;
	add.s32 	%r247, %r246, %r245;
	add.s32 	%r334, %r247, 8192;
	mul.wide.s32 	%rd84, %r333, 4;
	add.s64 	%rd85, %rd84, %rd25;
	shl.b64 	%rd86, %rd85, 2;
	add.s64 	%rd87, %rd1, %rd86;
	add.s64 	%rd104, %rd87, 8192;

$L__BB3_53:
	ld.global.nc.v4.u32 	{%r248, %r249, %r250, %r251}, [%rd104+-8192];
	st.shared.v4.u32 	[%r334+-8192], {%r248, %r249, %r250, %r251};
	ld.global.nc.v4.u32 	{%r256, %r257, %r258, %r259}, [%rd104+-4096];
	st.shared.v4.u32 	[%r334+-4096], {%r256, %r257, %r258, %r259};
	ld.global.nc.v4.u32 	{%r264, %r265, %r266, %r267}, [%rd104];
	st.shared.v4.u32 	[%r334], {%r264, %r265, %r266, %r267};
	ld.global.nc.v4.u32 	{%r272, %r273, %r274, %r275}, [%rd104+4096];
	st.shared.v4.u32 	[%r334+4096], {%r272, %r273, %r274, %r275};
	add.s32 	%r334, %r334, 16384;
	add.s64 	%rd104, %rd104, 16384;
	add.s32 	%r333, %r333, 1024;
	setp.lt.s32 	%p52, %r333, %r88;
	@%p52 bra 	$L__BB3_53;

$L__BB3_54:
	setp.ne.s32 	%p53, %r338, 0;
	and.b32  	%r105, %r4, 3;
	setp.eq.s32 	%p54, %r105, 0;
	or.pred  	%p55, %p53, %p54;
	@%p55 bra 	$L__BB3_57;

	and.b32  	%r281, %r4, -4;
	shl.b32 	%r282, %r4, 2;
	and.b32  	%r283, %r282, -16;
	add.s32 	%r284, %r5, %r283;
	mov.u32 	%r285, shraw;
	add.s32 	%r336, %r285, %r284;
	cvt.s64.s32 	%rd88, %r281;
	add.s64 	%rd89, %rd88, %rd25;
	shl.b64 	%rd90, %rd89, 2;
	add.s64 	%rd105, %rd1, %rd90;
	mov.u32 	%r338, 0;
	mov.u32 	%r337, %r338;

$L__BB3_56:
	ld.global.nc.f32 	%f34, [%rd105];
	st.shared.f32 	[%r336], %f34;
	add.s32 	%r336, %r336, 4;
	add.s64 	%rd105, %rd105, 4;
	add.s32 	%r337, %r337, 1;
	setp.lt.u32 	%p56, %r337, %r105;
	@%p56 bra 	$L__BB3_56;

$L__BB3_57:
	bar.sync 	0;
	add.s32 	%r112, %r338, %r3;
	setp.ge.s32 	%p57, %r112, %r132;
	@%p57 bra 	$L__BB3_67;

	add.s32 	%r287, %r134, %r2;
	add.s32 	%r288, %r287, -1;
	setp.lt.s32 	%p58, %r112, %r288;
	mov.f32 	%f66, 0f7FC00000;
	@%p58 bra 	$L__BB3_66;

	setp.lt.s32 	%p59, %r2, 1;
	mov.f32 	%f66, 0f00000000;
	@%p59 bra 	$L__BB3_66;

	add.s32 	%r290, %r2, -1;
	and.b32  	%r345, %r2, 3;
	setp.lt.u32 	%p60, %r290, 3;
	mov.f32 	%f66, 0f00000000;
	mov.u32 	%r342, 0;
	@%p60 bra 	$L__BB3_63;

	sub.s32 	%r341, %r2, %r345;
	shl.b32 	%r293, %r338, 2;
	add.s32 	%r115, %r5, %r293;
	mov.f32 	%f66, 0f00000000;
	mov.u32 	%r342, 0;
	mov.u32 	%r339, shraw;

$L__BB3_62:
	.pragma "nounroll";
	add.s32 	%r294, %r339, %r115;
	ld.shared.v4.f32 	{%f40, %f41, %f42, %f43}, [%r339];
	ld.shared.f32 	%f48, [%r294];
	fma.rn.ftz.f32 	%f49, %f48, %f40, %f66;
	ld.shared.f32 	%f50, [%r294+4];
	fma.rn.ftz.f32 	%f51, %f50, %f41, %f49;
	ld.shared.f32 	%f52, [%r294+8];
	fma.rn.ftz.f32 	%f53, %f52, %f42, %f51;
	ld.shared.f32 	%f54, [%r294+12];
	fma.rn.ftz.f32 	%f66, %f54, %f43, %f53;
	add.s32 	%r342, %r342, 4;
	add.s32 	%r339, %r339, 16;
	add.s32 	%r341, %r341, -4;
	setp.ne.s32 	%p61, %r341, 0;
	@%p61 bra 	$L__BB3_62;

$L__BB3_63:
	setp.eq.s32 	%p62, %r345, 0;
	@%p62 bra 	$L__BB3_66;

	shl.b32 	%r295, %r342, 2;
	mov.u32 	%r296, shraw;
	add.s32 	%r344, %r296, %r295;
	add.s32 	%r297, %r338, %r342;
	shl.b32 	%r298, %r297, 2;
	add.s32 	%r299, %r5, %r298;
	add.s32 	%r343, %r296, %r299;

$L__BB3_65:
	.pragma "nounroll";
	ld.shared.f32 	%f55, [%r344];
	ld.shared.f32 	%f56, [%r343];
	fma.rn.ftz.f32 	%f66, %f56, %f55, %f66;
	add.s32 	%r344, %r344, 4;
	add.s32 	%r343, %r343, 4;
	add.s32 	%r345, %r345, -1;
	setp.ne.s32 	%p63, %r345, 0;
	@%p63 bra 	$L__BB3_65;

$L__BB3_66:
	mad.lo.s32 	%r300, %r1, %r132, %r112;
	cvta.to.global.u64 	%rd91, %rd44;
	mul.wide.s32 	%rd92, %r300, 4;
	add.s64 	%rd93, %rd91, %rd92;
	st.global.f32 	[%rd93], %f66;

$L__BB3_67:
	ret;

}
	// .globl	alma_batch_tiled_f32_tile512
.visible .entry alma_batch_tiled_f32_tile512(
	.param .u64 alma_batch_tiled_f32_tile512_param_0,
	.param .u64 alma_batch_tiled_f32_tile512_param_1,
	.param .u64 alma_batch_tiled_f32_tile512_param_2,
	.param .u64 alma_batch_tiled_f32_tile512_param_3,
	.param .u32 alma_batch_tiled_f32_tile512_param_4,
	.param .u32 alma_batch_tiled_f32_tile512_param_5,
	.param .u32 alma_batch_tiled_f32_tile512_param_6,
	.param .u32 alma_batch_tiled_f32_tile512_param_7,
	.param .u64 alma_batch_tiled_f32_tile512_param_8
)
{
	.reg .pred 	%p<64>;
	.reg .f32 	%f<67>;
	.reg .b32 	%r<346>;
	.reg .b64 	%rd<106>;


	ld.param.u64 	%rd41, [alma_batch_tiled_f32_tile512_param_0];
	ld.param.u64 	%rd42, [alma_batch_tiled_f32_tile512_param_1];
	ld.param.u64 	%rd43, [alma_batch_tiled_f32_tile512_param_2];
	ld.param.u32 	%r131, [alma_batch_tiled_f32_tile512_param_4];
	ld.param.u32 	%r132, [alma_batch_tiled_f32_tile512_param_5];
	ld.param.u32 	%r133, [alma_batch_tiled_f32_tile512_param_6];
	ld.param.u32 	%r134, [alma_batch_tiled_f32_tile512_param_7];
	ld.param.u64 	%rd44, [alma_batch_tiled_f32_tile512_param_8];
	cvta.to.global.u64 	%rd1, %rd41;
	cvta.to.global.u64 	%rd2, %rd42;
	mov.u32 	%r135, %ntid.x;
	setp.ne.s32 	%p1, %r135, 512;
	@%p1 bra 	$L__BB4_67;

	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p2, %r1, %r133;
	@%p2 bra 	$L__BB4_67;

	cvta.to.global.u64 	%rd45, %rd43;
	mul.wide.s32 	%rd46, %r1, 4;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.nc.u32 	%r2, [%rd47];
	mov.u32 	%r136, %ctaid.x;
	shl.b32 	%r3, %r136, 9;
	setp.ge.s32 	%p3, %r3, %r132;
	@%p3 bra 	$L__BB4_67;

	add.s32 	%r4, %r2, 511;
	shl.b32 	%r137, %r2, 2;
	add.s32 	%r138, %r137, 15;
	and.b32  	%r5, %r138, -16;
	mul.lo.s32 	%r139, %r1, %r131;
	cvt.s64.s32 	%rd3, %r139;
	mul.wide.s32 	%rd48, %r139, 4;
	add.s64 	%rd49, %rd42, %rd48;
	and.b64  	%rd50, %rd49, 15;
	setp.eq.s64 	%p4, %rd50, 0;
	@%p4 bra 	$L__BB4_11;

	mov.u32 	%r304, %tid.x;
	setp.ge.s32 	%p5, %r304, %r2;
	@%p5 bra 	$L__BB4_21;

	not.b32 	%r140, %r304;
	add.s32 	%r7, %r2, %r140;
	shr.u32 	%r141, %r7, 9;
	add.s32 	%r142, %r141, 1;
	and.b32  	%r303, %r142, 3;
	setp.eq.s32 	%p6, %r303, 0;
	@%p6 bra 	$L__BB4_8;

	shl.b32 	%r143, %r304, 2;
	mov.u32 	%r144, shraw;
	add.s32 	%r301, %r144, %r143;
	cvt.s64.s32 	%rd51, %r304;
	add.s64 	%rd52, %rd51, %rd3;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd94, %rd2, %rd53;

$L__BB4_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f18, [%rd94];
	st.shared.f32 	[%r301], %f18;
	add.s32 	%r304, %r304, 512;
	add.s32 	%r301, %r301, 2048;
	add.s64 	%rd94, %rd94, 2048;
	add.s32 	%r303, %r303, -1;
	setp.ne.s32 	%p7, %r303, 0;
	@%p7 bra 	$L__BB4_7;

$L__BB4_8:
	setp.lt.u32 	%p8, %r7, 1536;
	@%p8 bra 	$L__BB4_21;

	shl.b32 	%r145, %r304, 2;
	mov.u32 	%r146, shraw;
	add.s32 	%r147, %r146, %r145;
	add.s32 	%r305, %r147, 4096;
	cvt.s64.s32 	%rd54, %r304;
	add.s64 	%rd55, %rd54, %rd3;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd57, %rd2, %rd56;
	add.s64 	%rd95, %rd57, 4096;

$L__BB4_10:
	ld.global.nc.f32 	%f19, [%rd95+-4096];
	st.shared.f32 	[%r305+-4096], %f19;
	ld.global.nc.f32 	%f20, [%rd95+-2048];
	st.shared.f32 	[%r305+-2048], %f20;
	ld.global.nc.f32 	%f21, [%rd95];
	st.shared.f32 	[%r305], %f21;
	ld.global.nc.f32 	%f22, [%rd95+2048];
	st.shared.f32 	[%r305+2048], %f22;
	add.s32 	%r305, %r305, 8192;
	add.s64 	%rd95, %rd95, 8192;
	add.s32 	%r304, %r304, 2048;
	setp.lt.s32 	%p9, %r304, %r2;
	@%p9 bra 	$L__BB4_10;
	bra.uni 	$L__BB4_21;

$L__BB4_11:
	shr.s32 	%r22, %r2, 2;
	mov.u32 	%r23, %tid.x;
	setp.ge.s32 	%p10, %r23, %r22;
	@%p10 bra 	$L__BB4_18;

	not.b32 	%r148, %r23;
	add.s32 	%r24, %r22, %r148;
	shr.u32 	%r149, %r24, 9;
	add.s32 	%r150, %r149, 1;
	and.b32  	%r309, %r150, 3;
	setp.eq.s32 	%p11, %r309, 0;
	mov.u32 	%r310, %r23;
	@%p11 bra 	$L__BB4_15;

	mul.wide.s32 	%rd58, %r23, 4;
	add.s64 	%rd59, %rd58, %rd3;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd96, %rd2, %rd60;
	shl.b32 	%r151, %r23, 4;
	mov.u32 	%r152, shraw;
	add.s32 	%r307, %r152, %r151;
	mov.u32 	%r310, %r23;

$L__BB4_14:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r153, %r154, %r155, %r156}, [%rd96];
	st.shared.v4.u32 	[%r307], {%r153, %r154, %r155, %r156};
	add.s32 	%r310, %r310, 512;
	add.s64 	%rd96, %rd96, 8192;
	add.s32 	%r307, %r307, 8192;
	add.s32 	%r309, %r309, -1;
	setp.ne.s32 	%p12, %r309, 0;
	@%p12 bra 	$L__BB4_14;

$L__BB4_15:
	setp.lt.u32 	%p13, %r24, 1536;
	@%p13 bra 	$L__BB4_18;

	shl.b32 	%r161, %r310, 4;
	mov.u32 	%r162, shraw;
	add.s32 	%r163, %r162, %r161;
	add.s32 	%r311, %r163, 16384;
	mul.wide.s32 	%rd61, %r310, 4;
	add.s64 	%rd62, %rd61, %rd3;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd64, %rd2, %rd63;
	add.s64 	%rd97, %rd64, 16384;

$L__BB4_17:
	ld.global.nc.v4.u32 	{%r164, %r165, %r166, %r167}, [%rd97+-16384];
	st.shared.v4.u32 	[%r311+-16384], {%r164, %r165, %r166, %r167};
	ld.global.nc.v4.u32 	{%r172, %r173, %r174, %r175}, [%rd97+-8192];
	st.shared.v4.u32 	[%r311+-8192], {%r172, %r173, %r174, %r175};
	ld.global.nc.v4.u32 	{%r180, %r181, %r182, %r183}, [%rd97];
	st.shared.v4.u32 	[%r311], {%r180, %r181, %r182, %r183};
	ld.global.nc.v4.u32 	{%r188, %r189, %r190, %r191}, [%rd97+8192];
	st.shared.v4.u32 	[%r311+8192], {%r188, %r189, %r190, %r191};
	add.s32 	%r311, %r311, 32768;
	add.s64 	%rd97, %rd97, 32768;
	add.s32 	%r310, %r310, 2048;
	setp.lt.s32 	%p14, %r310, %r22;
	@%p14 bra 	$L__BB4_17;

$L__BB4_18:
	setp.ne.s32 	%p15, %r23, 0;
	and.b32  	%r39, %r2, 3;
	setp.eq.s32 	%p16, %r39, 0;
	or.pred  	%p17, %p15, %p16;
	@%p17 bra 	$L__BB4_21;

	and.b32  	%r197, %r2, -4;
	and.b32  	%r199, %r137, -16;
	mov.u32 	%r200, shraw;
	add.s32 	%r313, %r200, %r199;
	cvt.s64.s32 	%rd65, %r197;
	add.s64 	%rd66, %rd65, %rd3;
	shl.b64 	%rd67, %rd66, 2;
	add.s64 	%rd98, %rd2, %rd67;
	mov.u32 	%r314, 0;

$L__BB4_20:
	ld.global.nc.f32 	%f23, [%rd98];
	st.shared.f32 	[%r313], %f23;
	add.s32 	%r313, %r313, 4;
	add.s64 	%rd98, %rd98, 4;
	add.s32 	%r314, %r314, 1;
	setp.lt.u32 	%p18, %r314, %r39;
	@%p18 bra 	$L__BB4_20;

$L__BB4_21:
	bar.sync 	0;
	add.s32 	%r201, %r3, 1;
	sub.s32 	%r45, %r201, %r2;
	setp.gt.s32 	%p19, %r45, -1;
	add.s32 	%r202, %r3, 512;
	setp.le.s32 	%p20, %r202, %r132;
	and.pred  	%p21, %p19, %p20;
	@%p21 bra 	$L__BB4_39;
	bra.uni 	$L__BB4_22;

$L__BB4_39:
	cvt.s64.s32 	%rd25, %r45;
	mul.wide.s32 	%rd71, %r45, 4;
	add.s64 	%rd72, %rd41, %rd71;
	and.b64  	%rd73, %rd72, 15;
	setp.eq.s64 	%p42, %rd73, 0;
	@%p42 bra 	$L__BB4_47;

	mov.u32 	%r338, %tid.x;
	setp.ge.s32 	%p43, %r338, %r4;
	@%p43 bra 	$L__BB4_57;

	add.s32 	%r220, %r2, 510;
	sub.s32 	%r73, %r220, %r338;
	shr.u32 	%r221, %r73, 9;
	add.s32 	%r222, %r221, 1;
	and.b32  	%r326, %r222, 3;
	setp.eq.s32 	%p44, %r326, 0;
	mov.u32 	%r327, %r338;
	@%p44 bra 	$L__BB4_44;

	shl.b32 	%r223, %r338, 2;
	add.s32 	%r224, %r5, %r223;
	mov.u32 	%r225, shraw;
	add.s32 	%r324, %r225, %r224;
	cvt.s64.s32 	%rd74, %r338;
	add.s64 	%rd75, %rd74, %rd25;
	shl.b64 	%rd76, %rd75, 2;
	add.s64 	%rd101, %rd1, %rd76;
	mov.u32 	%r327, %r338;

$L__BB4_43:
	.pragma "nounroll";
	ld.global.nc.f32 	%f29, [%rd101];
	st.shared.f32 	[%r324], %f29;
	add.s32 	%r327, %r327, 512;
	add.s32 	%r324, %r324, 2048;
	add.s64 	%rd101, %rd101, 2048;
	add.s32 	%r326, %r326, -1;
	setp.ne.s32 	%p45, %r326, 0;
	@%p45 bra 	$L__BB4_43;

$L__BB4_44:
	setp.lt.u32 	%p46, %r73, 1536;
	@%p46 bra 	$L__BB4_57;

	shl.b32 	%r226, %r327, 2;
	add.s32 	%r227, %r5, %r226;
	mov.u32 	%r228, shraw;
	add.s32 	%r229, %r228, %r227;
	add.s32 	%r328, %r229, 4096;
	cvt.s64.s32 	%rd77, %r327;
	add.s64 	%rd78, %rd77, %rd25;
	shl.b64 	%rd79, %rd78, 2;
	add.s64 	%rd80, %rd1, %rd79;
	add.s64 	%rd102, %rd80, 4096;

$L__BB4_46:
	ld.global.nc.f32 	%f30, [%rd102+-4096];
	st.shared.f32 	[%r328+-4096], %f30;
	ld.global.nc.f32 	%f31, [%rd102+-2048];
	st.shared.f32 	[%r328+-2048], %f31;
	ld.global.nc.f32 	%f32, [%rd102];
	st.shared.f32 	[%r328], %f32;
	ld.global.nc.f32 	%f33, [%rd102+2048];
	st.shared.f32 	[%r328+2048], %f33;
	add.s32 	%r328, %r328, 8192;
	add.s64 	%rd102, %rd102, 8192;
	add.s32 	%r327, %r327, 2048;
	setp.lt.s32 	%p47, %r327, %r4;
	@%p47 bra 	$L__BB4_46;
	bra.uni 	$L__BB4_57;

$L__BB4_22:
	mov.u32 	%r338, %tid.x;
	setp.ge.s32 	%p22, %r338, %r4;
	@%p22 bra 	$L__BB4_57;

	add.s32 	%r203, %r2, 510;
	sub.s32 	%r47, %r203, %r338;
	shr.u32 	%r204, %r47, 9;
	add.s32 	%r205, %r204, 1;
	and.b32  	%r318, %r205, 3;
	setp.eq.s32 	%p23, %r318, 0;
	mov.u32 	%r319, %r338;
	@%p23 bra 	$L__BB4_28;

	shl.b32 	%r206, %r338, 2;
	add.s32 	%r207, %r5, %r206;
	mov.u32 	%r208, shraw;
	add.s32 	%r316, %r208, %r207;
	add.s32 	%r209, %r338, %r3;
	add.s32 	%r210, %r209, 1;
	sub.s32 	%r315, %r210, %r2;
	mul.wide.s32 	%rd68, %r315, 4;
	add.s64 	%rd99, %rd1, %rd68;
	mov.u32 	%r319, %r338;

$L__BB4_25:
	.pragma "nounroll";
	setp.ge.s32 	%p24, %r315, %r132;
	setp.lt.s32 	%p25, %r315, 0;
	mov.f32 	%f57, 0f00000000;
	or.pred  	%p26, %p25, %p24;
	@%p26 bra 	$L__BB4_27;

	ld.global.nc.f32 	%f57, [%rd99];

$L__BB4_27:
	st.shared.f32 	[%r316], %f57;
	add.s32 	%r319, %r319, 512;
	add.s32 	%r316, %r316, 2048;
	add.s64 	%rd99, %rd99, 2048;
	add.s32 	%r315, %r315, 512;
	add.s32 	%r318, %r318, -1;
	setp.ne.s32 	%p27, %r318, 0;
	@%p27 bra 	$L__BB4_25;

$L__BB4_28:
	setp.lt.u32 	%p28, %r47, 1536;
	@%p28 bra 	$L__BB4_57;

	add.s32 	%r211, %r319, %r3;
	add.s32 	%r212, %r211, 1;
	sub.s32 	%r321, %r212, %r2;
	mul.wide.s32 	%rd70, %r321, 4;
	add.s64 	%rd100, %rd1, %rd70;
	add.s32 	%r213, %r211, 1537;
	sub.s32 	%r320, %r213, %r2;
	shl.b32 	%r214, %r319, 2;
	add.s32 	%r215, %r5, %r214;
	mov.u32 	%r216, shraw;
	add.s32 	%r323, %r216, %r215;

$L__BB4_30:
	setp.ge.s32 	%p29, %r321, %r132;
	setp.lt.s32 	%p30, %r321, 0;
	mov.f32 	%f59, 0f00000000;
	or.pred  	%p31, %p30, %p29;
	mov.f32 	%f58, %f59;
	@%p31 bra 	$L__BB4_32;

	ld.global.nc.f32 	%f58, [%rd100];

$L__BB4_32:
	st.shared.f32 	[%r323], %f58;
	add.s32 	%r217, %r320, -1024;
	setp.lt.s32 	%p32, %r217, 0;
	setp.ge.s32 	%p33, %r217, %r132;
	or.pred  	%p34, %p32, %p33;
	@%p34 bra 	$L__BB4_34;

	ld.global.nc.f32 	%f59, [%rd100+2048];

$L__BB4_34:
	st.shared.f32 	[%r323+2048], %f59;
	add.s32 	%r218, %r320, -512;
	setp.lt.s32 	%p35, %r218, 0;
	setp.ge.s32 	%p36, %r218, %r132;
	mov.f32 	%f61, 0f00000000;
	or.pred  	%p37, %p35, %p36;
	mov.f32 	%f60, %f61;
	@%p37 bra 	$L__BB4_36;

	ld.global.nc.f32 	%f60, [%rd100+4096];

$L__BB4_36:
	st.shared.f32 	[%r323+4096], %f60;
	add.s32 	%r219, %r321, 1536;
	setp.lt.s32 	%p38, %r219, 0;
	setp.ge.s32 	%p39, %r219, %r132;
	or.pred  	%p40, %p38, %p39;
	@%p40 bra 	$L__BB4_38;

	ld.global.nc.f32 	%f61, [%rd100+6144];

$L__BB4_38:
	add.s64 	%rd100, %rd100, 8192;
	add.s32 	%r68, %r323, 8192;
	st.shared.f32 	[%r323+6144], %f61;
	add.s32 	%r321, %r321, 2048;
	add.s32 	%r320, %r320, 2048;
	add.s32 	%r319, %r319, 2048;
	setp.lt.s32 	%p41, %r319, %r4;
	mov.u32 	%r323, %r68;
	@%p41 bra 	$L__BB4_30;
	bra.uni 	$L__BB4_57;

$L__BB4_47:
	shr.s32 	%r88, %r4, 2;
	mov.u32 	%r338, %tid.x;
	setp.ge.s32 	%p48, %r338, %r88;
	@%p48 bra 	$L__BB4_54;

	not.b32 	%r230, %r338;
	add.s32 	%r90, %r88, %r230;
	shr.u32 	%r231, %r90, 9;
	add.s32 	%r232, %r231, 1;
	and.b32  	%r332, %r232, 3;
	setp.eq.s32 	%p49, %r332, 0;
	mov.u32 	%r333, %r338;
	@%p49 bra 	$L__BB4_51;

	mul.wide.s32 	%rd81, %r338, 4;
	add.s64 	%rd82, %rd81, %rd25;
	shl.b64 	%rd83, %rd82, 2;
	add.s64 	%rd103, %rd1, %rd83;
	shl.b32 	%r233, %r338, 4;
	add.s32 	%r234, %r5, %r233;
	mov.u32 	%r235, shraw;
	add.s32 	%r330, %r235, %r234;
	mov.u32 	%r333, %r338;

$L__BB4_50:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r236, %r237, %r238, %r239}, [%rd103];
	st.shared.v4.u32 	[%r330], {%r236, %r237, %r238, %r239};
	add.s32 	%r333, %r333, 512;
	add.s64 	%rd103, %rd103, 8192;
	add.s32 	%r330, %r330, 8192;
	add.s32 	%r332, %r332, -1;
	setp.ne.s32 	%p50, %r332, 0;
	@%p50 bra 	$L__BB4_50;

$L__BB4_51:
	setp.lt.u32 	%p51, %r90, 1536;
	@%p51 bra 	$L__BB4_54;

	shl.b32 	%r244, %r333, 4;
	add.s32 	%r245, %r5, %r244;
	mov.u32 	%r246, shraw;
	add.s32 	%r247, %r246, %r245;
	add.s32 	%r334, %r247, 16384;
	mul.wide.s32 	%rd84, %r333, 4;
	add.s64 	%rd85, %rd84, %rd25;
	shl.b64 	%rd86, %rd85, 2;
	add.s64 	%rd87, %rd1, %rd86;
	add.s64 	%rd104, %rd87, 16384;

$L__BB4_53:
	ld.global.nc.v4.u32 	{%r248, %r249, %r250, %r251}, [%rd104+-16384];
	st.shared.v4.u32 	[%r334+-16384], {%r248, %r249, %r250, %r251};
	ld.global.nc.v4.u32 	{%r256, %r257, %r258, %r259}, [%rd104+-8192];
	st.shared.v4.u32 	[%r334+-8192], {%r256, %r257, %r258, %r259};
	ld.global.nc.v4.u32 	{%r264, %r265, %r266, %r267}, [%rd104];
	st.shared.v4.u32 	[%r334], {%r264, %r265, %r266, %r267};
	ld.global.nc.v4.u32 	{%r272, %r273, %r274, %r275}, [%rd104+8192];
	st.shared.v4.u32 	[%r334+8192], {%r272, %r273, %r274, %r275};
	add.s32 	%r334, %r334, 32768;
	add.s64 	%rd104, %rd104, 32768;
	add.s32 	%r333, %r333, 2048;
	setp.lt.s32 	%p52, %r333, %r88;
	@%p52 bra 	$L__BB4_53;

$L__BB4_54:
	setp.ne.s32 	%p53, %r338, 0;
	and.b32  	%r105, %r4, 3;
	setp.eq.s32 	%p54, %r105, 0;
	or.pred  	%p55, %p53, %p54;
	@%p55 bra 	$L__BB4_57;

	and.b32  	%r281, %r4, -4;
	shl.b32 	%r282, %r4, 2;
	and.b32  	%r283, %r282, -16;
	add.s32 	%r284, %r5, %r283;
	mov.u32 	%r285, shraw;
	add.s32 	%r336, %r285, %r284;
	cvt.s64.s32 	%rd88, %r281;
	add.s64 	%rd89, %rd88, %rd25;
	shl.b64 	%rd90, %rd89, 2;
	add.s64 	%rd105, %rd1, %rd90;
	mov.u32 	%r338, 0;
	mov.u32 	%r337, %r338;

$L__BB4_56:
	ld.global.nc.f32 	%f34, [%rd105];
	st.shared.f32 	[%r336], %f34;
	add.s32 	%r336, %r336, 4;
	add.s64 	%rd105, %rd105, 4;
	add.s32 	%r337, %r337, 1;
	setp.lt.u32 	%p56, %r337, %r105;
	@%p56 bra 	$L__BB4_56;

$L__BB4_57:
	bar.sync 	0;
	add.s32 	%r112, %r338, %r3;
	setp.ge.s32 	%p57, %r112, %r132;
	@%p57 bra 	$L__BB4_67;

	add.s32 	%r287, %r134, %r2;
	add.s32 	%r288, %r287, -1;
	setp.lt.s32 	%p58, %r112, %r288;
	mov.f32 	%f66, 0f7FC00000;
	@%p58 bra 	$L__BB4_66;

	setp.lt.s32 	%p59, %r2, 1;
	mov.f32 	%f66, 0f00000000;
	@%p59 bra 	$L__BB4_66;

	add.s32 	%r290, %r2, -1;
	and.b32  	%r345, %r2, 3;
	setp.lt.u32 	%p60, %r290, 3;
	mov.f32 	%f66, 0f00000000;
	mov.u32 	%r342, 0;
	@%p60 bra 	$L__BB4_63;

	sub.s32 	%r341, %r2, %r345;
	shl.b32 	%r293, %r338, 2;
	add.s32 	%r115, %r5, %r293;
	mov.f32 	%f66, 0f00000000;
	mov.u32 	%r342, 0;
	mov.u32 	%r339, shraw;

$L__BB4_62:
	.pragma "nounroll";
	add.s32 	%r294, %r339, %r115;
	ld.shared.v4.f32 	{%f40, %f41, %f42, %f43}, [%r339];
	ld.shared.f32 	%f48, [%r294];
	fma.rn.ftz.f32 	%f49, %f48, %f40, %f66;
	ld.shared.f32 	%f50, [%r294+4];
	fma.rn.ftz.f32 	%f51, %f50, %f41, %f49;
	ld.shared.f32 	%f52, [%r294+8];
	fma.rn.ftz.f32 	%f53, %f52, %f42, %f51;
	ld.shared.f32 	%f54, [%r294+12];
	fma.rn.ftz.f32 	%f66, %f54, %f43, %f53;
	add.s32 	%r342, %r342, 4;
	add.s32 	%r339, %r339, 16;
	add.s32 	%r341, %r341, -4;
	setp.ne.s32 	%p61, %r341, 0;
	@%p61 bra 	$L__BB4_62;

$L__BB4_63:
	setp.eq.s32 	%p62, %r345, 0;
	@%p62 bra 	$L__BB4_66;

	shl.b32 	%r295, %r342, 2;
	mov.u32 	%r296, shraw;
	add.s32 	%r344, %r296, %r295;
	add.s32 	%r297, %r338, %r342;
	shl.b32 	%r298, %r297, 2;
	add.s32 	%r299, %r5, %r298;
	add.s32 	%r343, %r296, %r299;

$L__BB4_65:
	.pragma "nounroll";
	ld.shared.f32 	%f55, [%r344];
	ld.shared.f32 	%f56, [%r343];
	fma.rn.ftz.f32 	%f66, %f56, %f55, %f66;
	add.s32 	%r344, %r344, 4;
	add.s32 	%r343, %r343, 4;
	add.s32 	%r345, %r345, -1;
	setp.ne.s32 	%p63, %r345, 0;
	@%p63 bra 	$L__BB4_65;

$L__BB4_66:
	mad.lo.s32 	%r300, %r1, %r132, %r112;
	cvta.to.global.u64 	%rd91, %rd44;
	mul.wide.s32 	%rd92, %r300, 4;
	add.s64 	%rd93, %rd91, %rd92;
	st.global.f32 	[%rd93], %f66;

$L__BB4_67:
	ret;

}
	// .globl	alma_batch_tiled_f32_2x_tile128
.visible .entry alma_batch_tiled_f32_2x_tile128(
	.param .u64 alma_batch_tiled_f32_2x_tile128_param_0,
	.param .u64 alma_batch_tiled_f32_2x_tile128_param_1,
	.param .u64 alma_batch_tiled_f32_2x_tile128_param_2,
	.param .u64 alma_batch_tiled_f32_2x_tile128_param_3,
	.param .u32 alma_batch_tiled_f32_2x_tile128_param_4,
	.param .u32 alma_batch_tiled_f32_2x_tile128_param_5,
	.param .u32 alma_batch_tiled_f32_2x_tile128_param_6,
	.param .u32 alma_batch_tiled_f32_2x_tile128_param_7,
	.param .u64 alma_batch_tiled_f32_2x_tile128_param_8
)
{
	.reg .pred 	%p<81>;
	.reg .f32 	%f<173>;
	.reg .b32 	%r<435>;
	.reg .b64 	%rd<114>;


	ld.param.u64 	%rd41, [alma_batch_tiled_f32_2x_tile128_param_0];
	ld.param.u64 	%rd42, [alma_batch_tiled_f32_2x_tile128_param_1];
	ld.param.u64 	%rd43, [alma_batch_tiled_f32_2x_tile128_param_2];
	ld.param.u32 	%r168, [alma_batch_tiled_f32_2x_tile128_param_4];
	ld.param.u32 	%r169, [alma_batch_tiled_f32_2x_tile128_param_5];
	ld.param.u32 	%r170, [alma_batch_tiled_f32_2x_tile128_param_6];
	ld.param.u32 	%r171, [alma_batch_tiled_f32_2x_tile128_param_7];
	ld.param.u64 	%rd44, [alma_batch_tiled_f32_2x_tile128_param_8];
	cvta.to.global.u64 	%rd1, %rd42;
	mov.u32 	%r172, %ntid.x;
	setp.ne.s32 	%p2, %r172, 64;
	@%p2 bra 	$L__BB5_84;

	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p3, %r1, %r170;
	@%p3 bra 	$L__BB5_84;

	cvta.to.global.u64 	%rd45, %rd43;
	mul.wide.s32 	%rd46, %r1, 4;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.nc.u32 	%r2, [%rd47];
	mov.u32 	%r173, %ctaid.x;
	shl.b32 	%r174, %r173, 7;
	setp.ge.s32 	%p4, %r174, %r169;
	@%p4 bra 	$L__BB5_84;

	add.s32 	%r3, %r2, 127;
	shl.b32 	%r175, %r2, 2;
	add.s32 	%r176, %r175, 15;
	and.b32  	%r4, %r176, -16;
	mul.lo.s32 	%r177, %r1, %r168;
	cvt.s64.s32 	%rd2, %r177;
	mul.wide.s32 	%rd48, %r177, 4;
	add.s64 	%rd49, %rd42, %rd48;
	and.b64  	%rd50, %rd49, 15;
	setp.eq.s64 	%p5, %rd50, 0;
	@%p5 bra 	$L__BB5_11;

	mov.u32 	%r380, %tid.x;
	setp.ge.s32 	%p6, %r380, %r2;
	@%p6 bra 	$L__BB5_21;

	not.b32 	%r178, %r380;
	add.s32 	%r6, %r2, %r178;
	shr.u32 	%r179, %r6, 6;
	add.s32 	%r180, %r179, 1;
	and.b32  	%r379, %r180, 3;
	setp.eq.s32 	%p7, %r379, 0;
	@%p7 bra 	$L__BB5_8;

	shl.b32 	%r181, %r380, 2;
	mov.u32 	%r182, shraw;
	add.s32 	%r377, %r182, %r181;
	cvt.s64.s32 	%rd51, %r380;
	add.s64 	%rd52, %rd51, %rd2;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd102, %rd1, %rd53;

$L__BB5_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f37, [%rd102];
	st.shared.f32 	[%r377], %f37;
	add.s32 	%r380, %r380, 64;
	add.s32 	%r377, %r377, 256;
	add.s64 	%rd102, %rd102, 256;
	add.s32 	%r379, %r379, -1;
	setp.ne.s32 	%p8, %r379, 0;
	@%p8 bra 	$L__BB5_7;

$L__BB5_8:
	setp.lt.u32 	%p9, %r6, 192;
	@%p9 bra 	$L__BB5_21;

	shl.b32 	%r183, %r380, 2;
	mov.u32 	%r184, shraw;
	add.s32 	%r185, %r184, %r183;
	add.s32 	%r381, %r185, 512;
	cvt.s64.s32 	%rd54, %r380;
	add.s64 	%rd55, %rd54, %rd2;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd57, %rd1, %rd56;
	add.s64 	%rd103, %rd57, 512;

$L__BB5_10:
	ld.global.nc.f32 	%f38, [%rd103+-512];
	st.shared.f32 	[%r381+-512], %f38;
	ld.global.nc.f32 	%f39, [%rd103+-256];
	st.shared.f32 	[%r381+-256], %f39;
	ld.global.nc.f32 	%f40, [%rd103];
	st.shared.f32 	[%r381], %f40;
	ld.global.nc.f32 	%f41, [%rd103+256];
	st.shared.f32 	[%r381+256], %f41;
	add.s32 	%r381, %r381, 1024;
	add.s64 	%rd103, %rd103, 1024;
	add.s32 	%r380, %r380, 256;
	setp.lt.s32 	%p10, %r380, %r2;
	@%p10 bra 	$L__BB5_10;
	bra.uni 	$L__BB5_21;

$L__BB5_11:
	shr.s32 	%r21, %r2, 2;
	mov.u32 	%r186, %tid.x;
	setp.ge.s32 	%p11, %r186, %r21;
	@%p11 bra 	$L__BB5_18;

	not.b32 	%r187, %r186;
	add.s32 	%r23, %r21, %r187;
	shr.u32 	%r188, %r23, 6;
	add.s32 	%r189, %r188, 1;
	and.b32  	%r385, %r189, 3;
	setp.eq.s32 	%p12, %r385, 0;
	mov.u32 	%r386, %r186;
	@%p12 bra 	$L__BB5_15;

	mov.u32 	%r386, %tid.x;
	mul.wide.s32 	%rd58, %r386, 4;
	add.s64 	%rd59, %rd58, %rd2;
	shl.b64 	%rd61, %rd59, 2;
	add.s64 	%rd104, %rd1, %rd61;
	shl.b32 	%r190, %r386, 4;
	mov.u32 	%r191, shraw;
	add.s32 	%r383, %r191, %r190;

$L__BB5_14:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r192, %r193, %r194, %r195}, [%rd104];
	st.shared.v4.u32 	[%r383], {%r192, %r193, %r194, %r195};
	add.s32 	%r386, %r386, 64;
	add.s64 	%rd104, %rd104, 1024;
	add.s32 	%r383, %r383, 1024;
	add.s32 	%r385, %r385, -1;
	setp.ne.s32 	%p13, %r385, 0;
	@%p13 bra 	$L__BB5_14;

$L__BB5_15:
	setp.lt.u32 	%p14, %r23, 192;
	@%p14 bra 	$L__BB5_18;

	shl.b32 	%r200, %r386, 4;
	mov.u32 	%r201, shraw;
	add.s32 	%r202, %r201, %r200;
	add.s32 	%r387, %r202, 2048;
	mul.wide.s32 	%rd62, %r386, 4;
	add.s64 	%rd64, %rd62, %rd2;
	shl.b64 	%rd65, %rd64, 2;
	add.s64 	%rd66, %rd1, %rd65;
	add.s64 	%rd105, %rd66, 2048;

$L__BB5_17:
	ld.global.nc.v4.u32 	{%r203, %r204, %r205, %r206}, [%rd105+-2048];
	st.shared.v4.u32 	[%r387+-2048], {%r203, %r204, %r205, %r206};
	ld.global.nc.v4.u32 	{%r211, %r212, %r213, %r214}, [%rd105+-1024];
	st.shared.v4.u32 	[%r387+-1024], {%r211, %r212, %r213, %r214};
	ld.global.nc.v4.u32 	{%r219, %r220, %r221, %r222}, [%rd105];
	st.shared.v4.u32 	[%r387], {%r219, %r220, %r221, %r222};
	ld.global.nc.v4.u32 	{%r227, %r228, %r229, %r230}, [%rd105+1024];
	st.shared.v4.u32 	[%r387+1024], {%r227, %r228, %r229, %r230};
	add.s32 	%r387, %r387, 4096;
	add.s64 	%rd105, %rd105, 4096;
	add.s32 	%r386, %r386, 256;
	setp.lt.s32 	%p15, %r386, %r21;
	@%p15 bra 	$L__BB5_17;

$L__BB5_18:
	setp.ne.s32 	%p16, %r186, 0;
	and.b32  	%r40, %r2, 3;
	setp.eq.s32 	%p17, %r40, 0;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	$L__BB5_21;

	and.b32  	%r237, %r2, -4;
	and.b32  	%r239, %r175, -16;
	mov.u32 	%r240, shraw;
	add.s32 	%r389, %r240, %r239;
	cvt.s64.s32 	%rd67, %r237;
	add.s64 	%rd68, %rd67, %rd2;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd106, %rd1, %rd70;
	mov.u32 	%r390, 0;

$L__BB5_20:
	ld.global.nc.f32 	%f42, [%rd106];
	st.shared.f32 	[%r389], %f42;
	add.s32 	%r389, %r389, 4;
	add.s64 	%rd106, %rd106, 4;
	add.s32 	%r390, %r390, 1;
	setp.lt.u32 	%p19, %r390, %r40;
	@%p19 bra 	$L__BB5_20;

$L__BB5_21:
	bar.sync 	0;
	or.b32  	%r243, %r174, 1;
	sub.s32 	%r46, %r243, %r2;
	setp.gt.s32 	%p20, %r46, -1;
	add.s32 	%r244, %r174, 128;
	setp.le.s32 	%p21, %r244, %r169;
	and.pred  	%p22, %p20, %p21;
	@%p22 bra 	$L__BB5_39;
	bra.uni 	$L__BB5_22;

$L__BB5_39:
	cvt.s64.s32 	%rd24, %r46;
	mul.wide.s32 	%rd75, %r46, 4;
	add.s64 	%rd76, %rd41, %rd75;
	and.b64  	%rd77, %rd76, 15;
	setp.eq.s64 	%p43, %rd77, 0;
	@%p43 bra 	$L__BB5_47;

	mov.u32 	%r414, %tid.x;
	setp.ge.s32 	%p44, %r414, %r3;
	@%p44 bra 	$L__BB5_57;

	add.s32 	%r269, %r2, 126;
	sub.s32 	%r74, %r269, %r414;
	shr.u32 	%r270, %r74, 6;
	add.s32 	%r271, %r270, 1;
	and.b32  	%r402, %r271, 3;
	setp.eq.s32 	%p45, %r402, 0;
	mov.u32 	%r403, %r414;
	@%p45 bra 	$L__BB5_44;

	shl.b32 	%r272, %r414, 2;
	add.s32 	%r273, %r4, %r272;
	mov.u32 	%r274, shraw;
	add.s32 	%r400, %r274, %r273;
	cvt.s64.s32 	%rd78, %r414;
	add.s64 	%rd79, %rd78, %rd24;
	cvta.to.global.u64 	%rd80, %rd41;
	shl.b64 	%rd81, %rd79, 2;
	add.s64 	%rd109, %rd80, %rd81;
	mov.u32 	%r403, %r414;

$L__BB5_43:
	.pragma "nounroll";
	ld.global.nc.f32 	%f48, [%rd109];
	st.shared.f32 	[%r400], %f48;
	add.s32 	%r403, %r403, 64;
	add.s32 	%r400, %r400, 256;
	add.s64 	%rd109, %rd109, 256;
	add.s32 	%r402, %r402, -1;
	setp.ne.s32 	%p46, %r402, 0;
	@%p46 bra 	$L__BB5_43;

$L__BB5_44:
	setp.lt.u32 	%p47, %r74, 192;
	@%p47 bra 	$L__BB5_57;

	shl.b32 	%r275, %r403, 2;
	add.s32 	%r276, %r4, %r275;
	mov.u32 	%r277, shraw;
	add.s32 	%r278, %r277, %r276;
	add.s32 	%r404, %r278, 512;
	cvt.s64.s32 	%rd82, %r403;
	cvta.to.global.u64 	%rd83, %rd41;
	add.s64 	%rd84, %rd82, %rd24;
	shl.b64 	%rd85, %rd84, 2;
	add.s64 	%rd86, %rd83, %rd85;
	add.s64 	%rd110, %rd86, 512;

$L__BB5_46:
	ld.global.nc.f32 	%f49, [%rd110+-512];
	st.shared.f32 	[%r404+-512], %f49;
	ld.global.nc.f32 	%f50, [%rd110+-256];
	st.shared.f32 	[%r404+-256], %f50;
	ld.global.nc.f32 	%f51, [%rd110];
	st.shared.f32 	[%r404], %f51;
	ld.global.nc.f32 	%f52, [%rd110+256];
	st.shared.f32 	[%r404+256], %f52;
	add.s32 	%r404, %r404, 1024;
	add.s64 	%rd110, %rd110, 1024;
	add.s32 	%r403, %r403, 256;
	setp.lt.s32 	%p48, %r403, %r3;
	@%p48 bra 	$L__BB5_46;
	bra.uni 	$L__BB5_57;

$L__BB5_22:
	mov.u32 	%r414, %tid.x;
	setp.ge.s32 	%p23, %r414, %r3;
	@%p23 bra 	$L__BB5_57;

	add.s32 	%r245, %r2, 126;
	sub.s32 	%r246, %r245, %r414;
	shr.u32 	%r247, %r246, 6;
	add.s32 	%r248, %r247, 1;
	and.b32  	%r394, %r248, 3;
	setp.eq.s32 	%p24, %r394, 0;
	mov.u32 	%r395, %r414;
	@%p24 bra 	$L__BB5_28;

	shl.b32 	%r249, %r414, 2;
	add.s32 	%r250, %r4, %r249;
	mov.u32 	%r251, shraw;
	add.s32 	%r392, %r251, %r250;
	add.s32 	%r254, %r414, %r174;
	add.s32 	%r255, %r254, 1;
	sub.s32 	%r391, %r255, %r2;
	cvta.to.global.u64 	%rd71, %rd41;
	mul.wide.s32 	%rd72, %r391, 4;
	add.s64 	%rd107, %rd71, %rd72;
	mov.u32 	%r395, %r414;

$L__BB5_25:
	.pragma "nounroll";
	setp.ge.s32 	%p25, %r391, %r169;
	setp.lt.s32 	%p26, %r391, 0;
	mov.f32 	%f150, 0f00000000;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB5_27;

	ld.global.nc.f32 	%f150, [%rd107];

$L__BB5_27:
	st.shared.f32 	[%r392], %f150;
	add.s32 	%r395, %r395, 64;
	add.s32 	%r392, %r392, 256;
	add.s64 	%rd107, %rd107, 256;
	add.s32 	%r391, %r391, 64;
	add.s32 	%r394, %r394, -1;
	setp.ne.s32 	%p28, %r394, 0;
	@%p28 bra 	$L__BB5_25;

$L__BB5_28:
	setp.lt.u32 	%p29, %r246, 192;
	@%p29 bra 	$L__BB5_57;

	add.s32 	%r260, %r395, %r174;
	add.s32 	%r261, %r260, 1;
	sub.s32 	%r397, %r261, %r2;
	cvta.to.global.u64 	%rd73, %rd41;
	mul.wide.s32 	%rd74, %r397, 4;
	add.s64 	%rd108, %rd73, %rd74;
	add.s32 	%r262, %r260, 193;
	sub.s32 	%r396, %r262, %r2;
	shl.b32 	%r263, %r395, 2;
	add.s32 	%r264, %r4, %r263;
	mov.u32 	%r265, shraw;
	add.s32 	%r399, %r265, %r264;

$L__BB5_30:
	setp.ge.s32 	%p30, %r397, %r169;
	setp.lt.s32 	%p31, %r397, 0;
	mov.f32 	%f152, 0f00000000;
	or.pred  	%p32, %p31, %p30;
	mov.f32 	%f151, %f152;
	@%p32 bra 	$L__BB5_32;

	ld.global.nc.f32 	%f151, [%rd108];

$L__BB5_32:
	st.shared.f32 	[%r399], %f151;
	add.s32 	%r266, %r396, -128;
	setp.lt.s32 	%p33, %r266, 0;
	setp.ge.s32 	%p34, %r266, %r169;
	or.pred  	%p35, %p33, %p34;
	@%p35 bra 	$L__BB5_34;

	ld.global.nc.f32 	%f152, [%rd108+256];

$L__BB5_34:
	st.shared.f32 	[%r399+256], %f152;
	add.s32 	%r267, %r396, -64;
	setp.lt.s32 	%p36, %r267, 0;
	setp.ge.s32 	%p37, %r267, %r169;
	mov.f32 	%f154, 0f00000000;
	or.pred  	%p38, %p36, %p37;
	mov.f32 	%f153, %f154;
	@%p38 bra 	$L__BB5_36;

	ld.global.nc.f32 	%f153, [%rd108+512];

$L__BB5_36:
	st.shared.f32 	[%r399+512], %f153;
	add.s32 	%r268, %r397, 192;
	setp.lt.s32 	%p39, %r268, 0;
	setp.ge.s32 	%p40, %r268, %r169;
	or.pred  	%p41, %p39, %p40;
	@%p41 bra 	$L__BB5_38;

	ld.global.nc.f32 	%f154, [%rd108+768];

$L__BB5_38:
	add.s64 	%rd108, %rd108, 1024;
	add.s32 	%r69, %r399, 1024;
	st.shared.f32 	[%r399+768], %f154;
	add.s32 	%r397, %r397, 256;
	add.s32 	%r396, %r396, 256;
	add.s32 	%r395, %r395, 256;
	setp.lt.s32 	%p42, %r395, %r3;
	mov.u32 	%r399, %r69;
	@%p42 bra 	$L__BB5_30;
	bra.uni 	$L__BB5_57;

$L__BB5_47:
	shr.s32 	%r89, %r3, 2;
	mov.u32 	%r414, %tid.x;
	setp.ge.s32 	%p49, %r414, %r89;
	@%p49 bra 	$L__BB5_54;

	not.b32 	%r279, %r414;
	add.s32 	%r91, %r89, %r279;
	shr.u32 	%r280, %r91, 6;
	add.s32 	%r281, %r280, 1;
	and.b32  	%r408, %r281, 3;
	setp.eq.s32 	%p50, %r408, 0;
	mov.u32 	%r409, %r414;
	@%p50 bra 	$L__BB5_51;

	mul.wide.s32 	%rd87, %r414, 4;
	add.s64 	%rd88, %rd87, %rd24;
	cvta.to.global.u64 	%rd89, %rd41;
	shl.b64 	%rd90, %rd88, 2;
	add.s64 	%rd111, %rd89, %rd90;
	shl.b32 	%r282, %r414, 4;
	add.s32 	%r283, %r4, %r282;
	mov.u32 	%r284, shraw;
	add.s32 	%r406, %r284, %r283;
	mov.u32 	%r409, %r414;

$L__BB5_50:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r285, %r286, %r287, %r288}, [%rd111];
	st.shared.v4.u32 	[%r406], {%r285, %r286, %r287, %r288};
	add.s32 	%r409, %r409, 64;
	add.s64 	%rd111, %rd111, 1024;
	add.s32 	%r406, %r406, 1024;
	add.s32 	%r408, %r408, -1;
	setp.ne.s32 	%p51, %r408, 0;
	@%p51 bra 	$L__BB5_50;

$L__BB5_51:
	setp.lt.u32 	%p52, %r91, 192;
	@%p52 bra 	$L__BB5_54;

	shl.b32 	%r293, %r409, 4;
	add.s32 	%r294, %r4, %r293;
	mov.u32 	%r295, shraw;
	add.s32 	%r296, %r295, %r294;
	add.s32 	%r410, %r296, 2048;
	mul.wide.s32 	%rd91, %r409, 4;
	cvta.to.global.u64 	%rd92, %rd41;
	add.s64 	%rd93, %rd91, %rd24;
	shl.b64 	%rd94, %rd93, 2;
	add.s64 	%rd95, %rd92, %rd94;
	add.s64 	%rd112, %rd95, 2048;

$L__BB5_53:
	ld.global.nc.v4.u32 	{%r297, %r298, %r299, %r300}, [%rd112+-2048];
	st.shared.v4.u32 	[%r410+-2048], {%r297, %r298, %r299, %r300};
	ld.global.nc.v4.u32 	{%r305, %r306, %r307, %r308}, [%rd112+-1024];
	st.shared.v4.u32 	[%r410+-1024], {%r305, %r306, %r307, %r308};
	ld.global.nc.v4.u32 	{%r313, %r314, %r315, %r316}, [%rd112];
	st.shared.v4.u32 	[%r410], {%r313, %r314, %r315, %r316};
	ld.global.nc.v4.u32 	{%r321, %r322, %r323, %r324}, [%rd112+1024];
	st.shared.v4.u32 	[%r410+1024], {%r321, %r322, %r323, %r324};
	add.s32 	%r410, %r410, 4096;
	add.s64 	%rd112, %rd112, 4096;
	add.s32 	%r409, %r409, 256;
	setp.lt.s32 	%p53, %r409, %r89;
	@%p53 bra 	$L__BB5_53;

$L__BB5_54:
	setp.ne.s32 	%p54, %r414, 0;
	and.b32  	%r106, %r3, 3;
	setp.eq.s32 	%p55, %r106, 0;
	or.pred  	%p56, %p54, %p55;
	@%p56 bra 	$L__BB5_57;

	and.b32  	%r330, %r3, -4;
	shl.b32 	%r331, %r3, 2;
	and.b32  	%r332, %r331, -16;
	add.s32 	%r333, %r4, %r332;
	mov.u32 	%r334, shraw;
	add.s32 	%r412, %r334, %r333;
	cvt.s64.s32 	%rd96, %r330;
	add.s64 	%rd97, %rd96, %rd24;
	cvta.to.global.u64 	%rd98, %rd41;
	shl.b64 	%rd99, %rd97, 2;
	add.s64 	%rd113, %rd98, %rd99;
	mov.u32 	%r414, 0;
	mov.u32 	%r413, %r414;

$L__BB5_56:
	ld.global.nc.f32 	%f53, [%rd113];
	st.shared.f32 	[%r412], %f53;
	add.s32 	%r412, %r412, 4;
	add.s64 	%rd113, %rd113, 4;
	add.s32 	%r413, %r413, 1;
	setp.lt.u32 	%p57, %r413, %r106;
	@%p57 bra 	$L__BB5_56;

$L__BB5_57:
	bar.sync 	0;
	add.s32 	%r336, %r171, %r2;
	add.s32 	%r113, %r336, -1;
	shl.b32 	%r339, %r414, 1;
	add.s32 	%r114, %r339, %r174;
	setp.ge.s32 	%p58, %r114, %r169;
	@%p58 bra 	$L__BB5_84;

	setp.ge.s32 	%p59, %r114, %r113;
	add.s32 	%r115, %r114, 1;
	setp.lt.s32 	%p60, %r115, %r169;
	setp.ge.s32 	%p61, %r115, %r113;
	and.pred  	%p1, %p60, %p61;
	and.pred  	%p62, %p59, %p1;
	mov.f32 	%f147, 0f00000000;
	mov.f32 	%f149, 0f00000000;
	@%p62 bra 	$L__BB5_75;
	bra.uni 	$L__BB5_59;

$L__BB5_75:
	setp.lt.s32 	%p75, %r2, 1;
	mov.f32 	%f171, %f147;
	mov.f32 	%f172, %f149;
	@%p75 bra 	$L__BB5_82;

	add.s32 	%r363, %r2, -1;
	and.b32  	%r434, %r2, 3;
	setp.lt.u32 	%p76, %r363, 3;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r431, 0;
	mov.f32 	%f171, %f172;
	@%p76 bra 	$L__BB5_79;

	sub.s32 	%r430, %r2, %r434;
	shl.b32 	%r366, %r414, 3;
	add.s32 	%r152, %r4, %r366;
	mov.u32 	%r367, shraw;
	add.s32 	%r428, %r367, 8;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r431, 0;

$L__BB5_78:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f117, %f118, %f119, %f120}, [%r428+-8];
	add.s32 	%r368, %r428, %r152;
	ld.shared.v2.f32 	{%f125, %f126}, [%r368+-8];
	fma.rn.ftz.f32 	%f129, %f125, %f117, %f171;
	fma.rn.ftz.f32 	%f130, %f126, %f117, %f172;
	ld.shared.f32 	%f131, [%r368+-4];
	fma.rn.ftz.f32 	%f132, %f131, %f118, %f129;
	ld.shared.v2.f32 	{%f133, %f134}, [%r368];
	fma.rn.ftz.f32 	%f137, %f133, %f118, %f130;
	ld.shared.f32 	%f138, [%r368];
	fma.rn.ftz.f32 	%f139, %f138, %f119, %f132;
	fma.rn.ftz.f32 	%f140, %f134, %f119, %f137;
	ld.shared.f32 	%f141, [%r368+4];
	fma.rn.ftz.f32 	%f171, %f141, %f120, %f139;
	ld.shared.f32 	%f142, [%r368+8];
	fma.rn.ftz.f32 	%f172, %f142, %f120, %f140;
	add.s32 	%r431, %r431, 4;
	add.s32 	%r428, %r428, 16;
	add.s32 	%r430, %r430, -4;
	setp.ne.s32 	%p77, %r430, 0;
	@%p77 bra 	$L__BB5_78;

$L__BB5_79:
	setp.eq.s32 	%p78, %r434, 0;
	@%p78 bra 	$L__BB5_82;

	shl.b32 	%r369, %r414, 3;
	add.s32 	%r370, %r4, %r369;
	shl.b32 	%r371, %r431, 2;
	add.s32 	%r372, %r370, %r371;
	mov.u32 	%r373, shraw;
	add.s32 	%r374, %r373, %r372;
	add.s32 	%r433, %r374, 4;
	add.s32 	%r432, %r373, %r371;

$L__BB5_81:
	.pragma "nounroll";
	ld.shared.f32 	%f143, [%r433+-4];
	ld.shared.f32 	%f144, [%r432];
	fma.rn.ftz.f32 	%f171, %f143, %f144, %f171;
	ld.shared.f32 	%f145, [%r433];
	fma.rn.ftz.f32 	%f172, %f145, %f144, %f172;
	add.s32 	%r433, %r433, 4;
	add.s32 	%r432, %r432, 4;
	add.s32 	%r434, %r434, -1;
	setp.ne.s32 	%p79, %r434, 0;
	@%p79 bra 	$L__BB5_81;
	bra.uni 	$L__BB5_82;

$L__BB5_59:
	mov.f32 	%f171, 0f7FC00000;
	mov.f32 	%f172, 0f7FC00000;
	@%p59 bra 	$L__BB5_68;
	bra.uni 	$L__BB5_60;

$L__BB5_68:
	setp.lt.s32 	%p70, %r2, 1;
	mov.f32 	%f171, %f147;
	@%p70 bra 	$L__BB5_82;

	add.s32 	%r353, %r2, -1;
	and.b32  	%r427, %r2, 3;
	setp.lt.u32 	%p71, %r353, 3;
	mov.f32 	%f171, 0f00000000;
	mov.u32 	%r425, 0;
	@%p71 bra 	$L__BB5_72;

	sub.s32 	%r424, %r2, %r427;
	shl.b32 	%r356, %r414, 3;
	add.s32 	%r136, %r4, %r356;
	mov.f32 	%f171, 0f00000000;
	mov.u32 	%r425, 0;
	mov.u32 	%r422, shraw;

$L__BB5_71:
	.pragma "nounroll";
	add.s32 	%r357, %r422, %r136;
	ld.shared.v2.f32 	{%f87, %f88}, [%r357];
	ld.shared.v4.f32 	{%f91, %f92, %f93, %f94}, [%r422];
	fma.rn.ftz.f32 	%f99, %f87, %f91, %f171;
	fma.rn.ftz.f32 	%f100, %f88, %f92, %f99;
	ld.shared.v2.f32 	{%f101, %f102}, [%r357+8];
	fma.rn.ftz.f32 	%f105, %f101, %f93, %f100;
	fma.rn.ftz.f32 	%f171, %f102, %f94, %f105;
	add.s32 	%r425, %r425, 4;
	add.s32 	%r422, %r422, 16;
	add.s32 	%r424, %r424, -4;
	setp.ne.s32 	%p72, %r424, 0;
	@%p72 bra 	$L__BB5_71;

$L__BB5_72:
	setp.eq.s32 	%p73, %r427, 0;
	@%p73 bra 	$L__BB5_82;

	shl.b32 	%r358, %r425, 2;
	mov.u32 	%r359, shraw;
	add.s32 	%r426, %r359, %r358;
	shl.b32 	%r360, %r414, 3;
	add.s32 	%r145, %r4, %r360;

$L__BB5_74:
	.pragma "nounroll";
	add.s32 	%r361, %r426, %r145;
	ld.shared.f32 	%f108, [%r426];
	ld.shared.f32 	%f109, [%r361];
	fma.rn.ftz.f32 	%f171, %f109, %f108, %f171;
	add.s32 	%r426, %r426, 4;
	add.s32 	%r427, %r427, -1;
	setp.eq.s32 	%p74, %r427, 0;
	@%p74 bra 	$L__BB5_82;
	bra.uni 	$L__BB5_74;

$L__BB5_60:
	not.pred 	%p64, %p1;
	@%p64 bra 	$L__BB5_82;

	setp.lt.s32 	%p65, %r2, 1;
	mov.f32 	%f172, %f149;
	@%p65 bra 	$L__BB5_82;

	add.s32 	%r341, %r2, -1;
	and.b32  	%r421, %r2, 3;
	setp.lt.u32 	%p66, %r341, 3;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r419, 0;
	@%p66 bra 	$L__BB5_65;

	sub.s32 	%r418, %r2, %r421;
	shl.b32 	%r344, %r414, 3;
	add.s32 	%r345, %r4, %r344;
	mov.u32 	%r415, shraw;
	add.s32 	%r346, %r415, %r345;
	add.s32 	%r416, %r346, 8;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r419, 0;

$L__BB5_64:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f61, %f62, %f63, %f64}, [%r415];
	ld.shared.f32 	%f69, [%r416+-4];
	fma.rn.ftz.f32 	%f70, %f69, %f61, %f172;
	ld.shared.v2.f32 	{%f71, %f72}, [%r416];
	fma.rn.ftz.f32 	%f75, %f71, %f62, %f70;
	fma.rn.ftz.f32 	%f76, %f72, %f63, %f75;
	ld.shared.f32 	%f77, [%r416+8];
	fma.rn.ftz.f32 	%f172, %f77, %f64, %f76;
	add.s32 	%r419, %r419, 4;
	add.s32 	%r416, %r416, 16;
	add.s32 	%r415, %r415, 16;
	add.s32 	%r418, %r418, -4;
	setp.ne.s32 	%p67, %r418, 0;
	@%p67 bra 	$L__BB5_64;

$L__BB5_65:
	setp.eq.s32 	%p68, %r421, 0;
	@%p68 bra 	$L__BB5_82;

	shl.b32 	%r347, %r419, 2;
	mov.u32 	%r348, shraw;
	add.s32 	%r420, %r348, %r347;
	shl.b32 	%r349, %r414, 3;
	add.s32 	%r350, %r4, %r349;
	add.s32 	%r129, %r350, 4;

$L__BB5_67:
	.pragma "nounroll";
	add.s32 	%r351, %r420, %r129;
	ld.shared.f32 	%f80, [%r420];
	ld.shared.f32 	%f81, [%r351];
	fma.rn.ftz.f32 	%f172, %f81, %f80, %f172;
	add.s32 	%r420, %r420, 4;
	add.s32 	%r421, %r421, -1;
	setp.eq.s32 	%p69, %r421, 0;
	@%p69 bra 	$L__BB5_82;
	bra.uni 	$L__BB5_67;

$L__BB5_82:
	mad.lo.s32 	%r376, %r1, %r169, %r114;
	cvta.to.global.u64 	%rd100, %rd44;
	mul.wide.s32 	%rd101, %r376, 4;
	add.s64 	%rd40, %rd100, %rd101;
	st.global.f32 	[%rd40], %f171;
	setp.ge.s32 	%p80, %r115, %r169;
	@%p80 bra 	$L__BB5_84;

	st.global.f32 	[%rd40+4], %f172;

$L__BB5_84:
	ret;

}
	// .globl	alma_batch_tiled_f32_2x_tile256
.visible .entry alma_batch_tiled_f32_2x_tile256(
	.param .u64 alma_batch_tiled_f32_2x_tile256_param_0,
	.param .u64 alma_batch_tiled_f32_2x_tile256_param_1,
	.param .u64 alma_batch_tiled_f32_2x_tile256_param_2,
	.param .u64 alma_batch_tiled_f32_2x_tile256_param_3,
	.param .u32 alma_batch_tiled_f32_2x_tile256_param_4,
	.param .u32 alma_batch_tiled_f32_2x_tile256_param_5,
	.param .u32 alma_batch_tiled_f32_2x_tile256_param_6,
	.param .u32 alma_batch_tiled_f32_2x_tile256_param_7,
	.param .u64 alma_batch_tiled_f32_2x_tile256_param_8
)
{
	.reg .pred 	%p<81>;
	.reg .f32 	%f<173>;
	.reg .b32 	%r<435>;
	.reg .b64 	%rd<114>;


	ld.param.u64 	%rd41, [alma_batch_tiled_f32_2x_tile256_param_0];
	ld.param.u64 	%rd42, [alma_batch_tiled_f32_2x_tile256_param_1];
	ld.param.u64 	%rd43, [alma_batch_tiled_f32_2x_tile256_param_2];
	ld.param.u32 	%r168, [alma_batch_tiled_f32_2x_tile256_param_4];
	ld.param.u32 	%r169, [alma_batch_tiled_f32_2x_tile256_param_5];
	ld.param.u32 	%r170, [alma_batch_tiled_f32_2x_tile256_param_6];
	ld.param.u32 	%r171, [alma_batch_tiled_f32_2x_tile256_param_7];
	ld.param.u64 	%rd44, [alma_batch_tiled_f32_2x_tile256_param_8];
	cvta.to.global.u64 	%rd1, %rd42;
	mov.u32 	%r172, %ntid.x;
	setp.ne.s32 	%p2, %r172, 128;
	@%p2 bra 	$L__BB6_84;

	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p3, %r1, %r170;
	@%p3 bra 	$L__BB6_84;

	cvta.to.global.u64 	%rd45, %rd43;
	mul.wide.s32 	%rd46, %r1, 4;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.nc.u32 	%r2, [%rd47];
	mov.u32 	%r173, %ctaid.x;
	shl.b32 	%r174, %r173, 8;
	setp.ge.s32 	%p4, %r174, %r169;
	@%p4 bra 	$L__BB6_84;

	add.s32 	%r3, %r2, 255;
	shl.b32 	%r175, %r2, 2;
	add.s32 	%r176, %r175, 15;
	and.b32  	%r4, %r176, -16;
	mul.lo.s32 	%r177, %r1, %r168;
	cvt.s64.s32 	%rd2, %r177;
	mul.wide.s32 	%rd48, %r177, 4;
	add.s64 	%rd49, %rd42, %rd48;
	and.b64  	%rd50, %rd49, 15;
	setp.eq.s64 	%p5, %rd50, 0;
	@%p5 bra 	$L__BB6_11;

	mov.u32 	%r380, %tid.x;
	setp.ge.s32 	%p6, %r380, %r2;
	@%p6 bra 	$L__BB6_21;

	not.b32 	%r178, %r380;
	add.s32 	%r6, %r2, %r178;
	shr.u32 	%r179, %r6, 7;
	add.s32 	%r180, %r179, 1;
	and.b32  	%r379, %r180, 3;
	setp.eq.s32 	%p7, %r379, 0;
	@%p7 bra 	$L__BB6_8;

	shl.b32 	%r181, %r380, 2;
	mov.u32 	%r182, shraw;
	add.s32 	%r377, %r182, %r181;
	cvt.s64.s32 	%rd51, %r380;
	add.s64 	%rd52, %rd51, %rd2;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd102, %rd1, %rd53;

$L__BB6_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f37, [%rd102];
	st.shared.f32 	[%r377], %f37;
	add.s32 	%r380, %r380, 128;
	add.s32 	%r377, %r377, 512;
	add.s64 	%rd102, %rd102, 512;
	add.s32 	%r379, %r379, -1;
	setp.ne.s32 	%p8, %r379, 0;
	@%p8 bra 	$L__BB6_7;

$L__BB6_8:
	setp.lt.u32 	%p9, %r6, 384;
	@%p9 bra 	$L__BB6_21;

	shl.b32 	%r183, %r380, 2;
	mov.u32 	%r184, shraw;
	add.s32 	%r185, %r184, %r183;
	add.s32 	%r381, %r185, 1024;
	cvt.s64.s32 	%rd54, %r380;
	add.s64 	%rd55, %rd54, %rd2;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd57, %rd1, %rd56;
	add.s64 	%rd103, %rd57, 1024;

$L__BB6_10:
	ld.global.nc.f32 	%f38, [%rd103+-1024];
	st.shared.f32 	[%r381+-1024], %f38;
	ld.global.nc.f32 	%f39, [%rd103+-512];
	st.shared.f32 	[%r381+-512], %f39;
	ld.global.nc.f32 	%f40, [%rd103];
	st.shared.f32 	[%r381], %f40;
	ld.global.nc.f32 	%f41, [%rd103+512];
	st.shared.f32 	[%r381+512], %f41;
	add.s32 	%r381, %r381, 2048;
	add.s64 	%rd103, %rd103, 2048;
	add.s32 	%r380, %r380, 512;
	setp.lt.s32 	%p10, %r380, %r2;
	@%p10 bra 	$L__BB6_10;
	bra.uni 	$L__BB6_21;

$L__BB6_11:
	shr.s32 	%r21, %r2, 2;
	mov.u32 	%r186, %tid.x;
	setp.ge.s32 	%p11, %r186, %r21;
	@%p11 bra 	$L__BB6_18;

	not.b32 	%r187, %r186;
	add.s32 	%r23, %r21, %r187;
	shr.u32 	%r188, %r23, 7;
	add.s32 	%r189, %r188, 1;
	and.b32  	%r385, %r189, 3;
	setp.eq.s32 	%p12, %r385, 0;
	mov.u32 	%r386, %r186;
	@%p12 bra 	$L__BB6_15;

	mov.u32 	%r386, %tid.x;
	mul.wide.s32 	%rd58, %r386, 4;
	add.s64 	%rd59, %rd58, %rd2;
	shl.b64 	%rd61, %rd59, 2;
	add.s64 	%rd104, %rd1, %rd61;
	shl.b32 	%r190, %r386, 4;
	mov.u32 	%r191, shraw;
	add.s32 	%r383, %r191, %r190;

$L__BB6_14:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r192, %r193, %r194, %r195}, [%rd104];
	st.shared.v4.u32 	[%r383], {%r192, %r193, %r194, %r195};
	add.s32 	%r386, %r386, 128;
	add.s64 	%rd104, %rd104, 2048;
	add.s32 	%r383, %r383, 2048;
	add.s32 	%r385, %r385, -1;
	setp.ne.s32 	%p13, %r385, 0;
	@%p13 bra 	$L__BB6_14;

$L__BB6_15:
	setp.lt.u32 	%p14, %r23, 384;
	@%p14 bra 	$L__BB6_18;

	shl.b32 	%r200, %r386, 4;
	mov.u32 	%r201, shraw;
	add.s32 	%r202, %r201, %r200;
	add.s32 	%r387, %r202, 4096;
	mul.wide.s32 	%rd62, %r386, 4;
	add.s64 	%rd64, %rd62, %rd2;
	shl.b64 	%rd65, %rd64, 2;
	add.s64 	%rd66, %rd1, %rd65;
	add.s64 	%rd105, %rd66, 4096;

$L__BB6_17:
	ld.global.nc.v4.u32 	{%r203, %r204, %r205, %r206}, [%rd105+-4096];
	st.shared.v4.u32 	[%r387+-4096], {%r203, %r204, %r205, %r206};
	ld.global.nc.v4.u32 	{%r211, %r212, %r213, %r214}, [%rd105+-2048];
	st.shared.v4.u32 	[%r387+-2048], {%r211, %r212, %r213, %r214};
	ld.global.nc.v4.u32 	{%r219, %r220, %r221, %r222}, [%rd105];
	st.shared.v4.u32 	[%r387], {%r219, %r220, %r221, %r222};
	ld.global.nc.v4.u32 	{%r227, %r228, %r229, %r230}, [%rd105+2048];
	st.shared.v4.u32 	[%r387+2048], {%r227, %r228, %r229, %r230};
	add.s32 	%r387, %r387, 8192;
	add.s64 	%rd105, %rd105, 8192;
	add.s32 	%r386, %r386, 512;
	setp.lt.s32 	%p15, %r386, %r21;
	@%p15 bra 	$L__BB6_17;

$L__BB6_18:
	setp.ne.s32 	%p16, %r186, 0;
	and.b32  	%r40, %r2, 3;
	setp.eq.s32 	%p17, %r40, 0;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	$L__BB6_21;

	and.b32  	%r237, %r2, -4;
	and.b32  	%r239, %r175, -16;
	mov.u32 	%r240, shraw;
	add.s32 	%r389, %r240, %r239;
	cvt.s64.s32 	%rd67, %r237;
	add.s64 	%rd68, %rd67, %rd2;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd106, %rd1, %rd70;
	mov.u32 	%r390, 0;

$L__BB6_20:
	ld.global.nc.f32 	%f42, [%rd106];
	st.shared.f32 	[%r389], %f42;
	add.s32 	%r389, %r389, 4;
	add.s64 	%rd106, %rd106, 4;
	add.s32 	%r390, %r390, 1;
	setp.lt.u32 	%p19, %r390, %r40;
	@%p19 bra 	$L__BB6_20;

$L__BB6_21:
	bar.sync 	0;
	or.b32  	%r243, %r174, 1;
	sub.s32 	%r46, %r243, %r2;
	setp.gt.s32 	%p20, %r46, -1;
	add.s32 	%r244, %r174, 256;
	setp.le.s32 	%p21, %r244, %r169;
	and.pred  	%p22, %p20, %p21;
	@%p22 bra 	$L__BB6_39;
	bra.uni 	$L__BB6_22;

$L__BB6_39:
	cvt.s64.s32 	%rd24, %r46;
	mul.wide.s32 	%rd75, %r46, 4;
	add.s64 	%rd76, %rd41, %rd75;
	and.b64  	%rd77, %rd76, 15;
	setp.eq.s64 	%p43, %rd77, 0;
	@%p43 bra 	$L__BB6_47;

	mov.u32 	%r414, %tid.x;
	setp.ge.s32 	%p44, %r414, %r3;
	@%p44 bra 	$L__BB6_57;

	add.s32 	%r269, %r2, 254;
	sub.s32 	%r74, %r269, %r414;
	shr.u32 	%r270, %r74, 7;
	add.s32 	%r271, %r270, 1;
	and.b32  	%r402, %r271, 3;
	setp.eq.s32 	%p45, %r402, 0;
	mov.u32 	%r403, %r414;
	@%p45 bra 	$L__BB6_44;

	shl.b32 	%r272, %r414, 2;
	add.s32 	%r273, %r4, %r272;
	mov.u32 	%r274, shraw;
	add.s32 	%r400, %r274, %r273;
	cvt.s64.s32 	%rd78, %r414;
	add.s64 	%rd79, %rd78, %rd24;
	cvta.to.global.u64 	%rd80, %rd41;
	shl.b64 	%rd81, %rd79, 2;
	add.s64 	%rd109, %rd80, %rd81;
	mov.u32 	%r403, %r414;

$L__BB6_43:
	.pragma "nounroll";
	ld.global.nc.f32 	%f48, [%rd109];
	st.shared.f32 	[%r400], %f48;
	add.s32 	%r403, %r403, 128;
	add.s32 	%r400, %r400, 512;
	add.s64 	%rd109, %rd109, 512;
	add.s32 	%r402, %r402, -1;
	setp.ne.s32 	%p46, %r402, 0;
	@%p46 bra 	$L__BB6_43;

$L__BB6_44:
	setp.lt.u32 	%p47, %r74, 384;
	@%p47 bra 	$L__BB6_57;

	shl.b32 	%r275, %r403, 2;
	add.s32 	%r276, %r4, %r275;
	mov.u32 	%r277, shraw;
	add.s32 	%r278, %r277, %r276;
	add.s32 	%r404, %r278, 1024;
	cvt.s64.s32 	%rd82, %r403;
	cvta.to.global.u64 	%rd83, %rd41;
	add.s64 	%rd84, %rd82, %rd24;
	shl.b64 	%rd85, %rd84, 2;
	add.s64 	%rd86, %rd83, %rd85;
	add.s64 	%rd110, %rd86, 1024;

$L__BB6_46:
	ld.global.nc.f32 	%f49, [%rd110+-1024];
	st.shared.f32 	[%r404+-1024], %f49;
	ld.global.nc.f32 	%f50, [%rd110+-512];
	st.shared.f32 	[%r404+-512], %f50;
	ld.global.nc.f32 	%f51, [%rd110];
	st.shared.f32 	[%r404], %f51;
	ld.global.nc.f32 	%f52, [%rd110+512];
	st.shared.f32 	[%r404+512], %f52;
	add.s32 	%r404, %r404, 2048;
	add.s64 	%rd110, %rd110, 2048;
	add.s32 	%r403, %r403, 512;
	setp.lt.s32 	%p48, %r403, %r3;
	@%p48 bra 	$L__BB6_46;
	bra.uni 	$L__BB6_57;

$L__BB6_22:
	mov.u32 	%r414, %tid.x;
	setp.ge.s32 	%p23, %r414, %r3;
	@%p23 bra 	$L__BB6_57;

	add.s32 	%r245, %r2, 254;
	sub.s32 	%r246, %r245, %r414;
	shr.u32 	%r247, %r246, 7;
	add.s32 	%r248, %r247, 1;
	and.b32  	%r394, %r248, 3;
	setp.eq.s32 	%p24, %r394, 0;
	mov.u32 	%r395, %r414;
	@%p24 bra 	$L__BB6_28;

	shl.b32 	%r249, %r414, 2;
	add.s32 	%r250, %r4, %r249;
	mov.u32 	%r251, shraw;
	add.s32 	%r392, %r251, %r250;
	add.s32 	%r254, %r414, %r174;
	add.s32 	%r255, %r254, 1;
	sub.s32 	%r391, %r255, %r2;
	cvta.to.global.u64 	%rd71, %rd41;
	mul.wide.s32 	%rd72, %r391, 4;
	add.s64 	%rd107, %rd71, %rd72;
	mov.u32 	%r395, %r414;

$L__BB6_25:
	.pragma "nounroll";
	setp.ge.s32 	%p25, %r391, %r169;
	setp.lt.s32 	%p26, %r391, 0;
	mov.f32 	%f150, 0f00000000;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB6_27;

	ld.global.nc.f32 	%f150, [%rd107];

$L__BB6_27:
	st.shared.f32 	[%r392], %f150;
	add.s32 	%r395, %r395, 128;
	add.s32 	%r392, %r392, 512;
	add.s64 	%rd107, %rd107, 512;
	add.s32 	%r391, %r391, 128;
	add.s32 	%r394, %r394, -1;
	setp.ne.s32 	%p28, %r394, 0;
	@%p28 bra 	$L__BB6_25;

$L__BB6_28:
	setp.lt.u32 	%p29, %r246, 384;
	@%p29 bra 	$L__BB6_57;

	add.s32 	%r260, %r395, %r174;
	add.s32 	%r261, %r260, 1;
	sub.s32 	%r397, %r261, %r2;
	cvta.to.global.u64 	%rd73, %rd41;
	mul.wide.s32 	%rd74, %r397, 4;
	add.s64 	%rd108, %rd73, %rd74;
	add.s32 	%r262, %r260, 385;
	sub.s32 	%r396, %r262, %r2;
	shl.b32 	%r263, %r395, 2;
	add.s32 	%r264, %r4, %r263;
	mov.u32 	%r265, shraw;
	add.s32 	%r399, %r265, %r264;

$L__BB6_30:
	setp.ge.s32 	%p30, %r397, %r169;
	setp.lt.s32 	%p31, %r397, 0;
	mov.f32 	%f152, 0f00000000;
	or.pred  	%p32, %p31, %p30;
	mov.f32 	%f151, %f152;
	@%p32 bra 	$L__BB6_32;

	ld.global.nc.f32 	%f151, [%rd108];

$L__BB6_32:
	st.shared.f32 	[%r399], %f151;
	add.s32 	%r266, %r396, -256;
	setp.lt.s32 	%p33, %r266, 0;
	setp.ge.s32 	%p34, %r266, %r169;
	or.pred  	%p35, %p33, %p34;
	@%p35 bra 	$L__BB6_34;

	ld.global.nc.f32 	%f152, [%rd108+512];

$L__BB6_34:
	st.shared.f32 	[%r399+512], %f152;
	add.s32 	%r267, %r396, -128;
	setp.lt.s32 	%p36, %r267, 0;
	setp.ge.s32 	%p37, %r267, %r169;
	mov.f32 	%f154, 0f00000000;
	or.pred  	%p38, %p36, %p37;
	mov.f32 	%f153, %f154;
	@%p38 bra 	$L__BB6_36;

	ld.global.nc.f32 	%f153, [%rd108+1024];

$L__BB6_36:
	st.shared.f32 	[%r399+1024], %f153;
	add.s32 	%r268, %r397, 384;
	setp.lt.s32 	%p39, %r268, 0;
	setp.ge.s32 	%p40, %r268, %r169;
	or.pred  	%p41, %p39, %p40;
	@%p41 bra 	$L__BB6_38;

	ld.global.nc.f32 	%f154, [%rd108+1536];

$L__BB6_38:
	add.s64 	%rd108, %rd108, 2048;
	add.s32 	%r69, %r399, 2048;
	st.shared.f32 	[%r399+1536], %f154;
	add.s32 	%r397, %r397, 512;
	add.s32 	%r396, %r396, 512;
	add.s32 	%r395, %r395, 512;
	setp.lt.s32 	%p42, %r395, %r3;
	mov.u32 	%r399, %r69;
	@%p42 bra 	$L__BB6_30;
	bra.uni 	$L__BB6_57;

$L__BB6_47:
	shr.s32 	%r89, %r3, 2;
	mov.u32 	%r414, %tid.x;
	setp.ge.s32 	%p49, %r414, %r89;
	@%p49 bra 	$L__BB6_54;

	not.b32 	%r279, %r414;
	add.s32 	%r91, %r89, %r279;
	shr.u32 	%r280, %r91, 7;
	add.s32 	%r281, %r280, 1;
	and.b32  	%r408, %r281, 3;
	setp.eq.s32 	%p50, %r408, 0;
	mov.u32 	%r409, %r414;
	@%p50 bra 	$L__BB6_51;

	mul.wide.s32 	%rd87, %r414, 4;
	add.s64 	%rd88, %rd87, %rd24;
	cvta.to.global.u64 	%rd89, %rd41;
	shl.b64 	%rd90, %rd88, 2;
	add.s64 	%rd111, %rd89, %rd90;
	shl.b32 	%r282, %r414, 4;
	add.s32 	%r283, %r4, %r282;
	mov.u32 	%r284, shraw;
	add.s32 	%r406, %r284, %r283;
	mov.u32 	%r409, %r414;

$L__BB6_50:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r285, %r286, %r287, %r288}, [%rd111];
	st.shared.v4.u32 	[%r406], {%r285, %r286, %r287, %r288};
	add.s32 	%r409, %r409, 128;
	add.s64 	%rd111, %rd111, 2048;
	add.s32 	%r406, %r406, 2048;
	add.s32 	%r408, %r408, -1;
	setp.ne.s32 	%p51, %r408, 0;
	@%p51 bra 	$L__BB6_50;

$L__BB6_51:
	setp.lt.u32 	%p52, %r91, 384;
	@%p52 bra 	$L__BB6_54;

	shl.b32 	%r293, %r409, 4;
	add.s32 	%r294, %r4, %r293;
	mov.u32 	%r295, shraw;
	add.s32 	%r296, %r295, %r294;
	add.s32 	%r410, %r296, 4096;
	mul.wide.s32 	%rd91, %r409, 4;
	cvta.to.global.u64 	%rd92, %rd41;
	add.s64 	%rd93, %rd91, %rd24;
	shl.b64 	%rd94, %rd93, 2;
	add.s64 	%rd95, %rd92, %rd94;
	add.s64 	%rd112, %rd95, 4096;

$L__BB6_53:
	ld.global.nc.v4.u32 	{%r297, %r298, %r299, %r300}, [%rd112+-4096];
	st.shared.v4.u32 	[%r410+-4096], {%r297, %r298, %r299, %r300};
	ld.global.nc.v4.u32 	{%r305, %r306, %r307, %r308}, [%rd112+-2048];
	st.shared.v4.u32 	[%r410+-2048], {%r305, %r306, %r307, %r308};
	ld.global.nc.v4.u32 	{%r313, %r314, %r315, %r316}, [%rd112];
	st.shared.v4.u32 	[%r410], {%r313, %r314, %r315, %r316};
	ld.global.nc.v4.u32 	{%r321, %r322, %r323, %r324}, [%rd112+2048];
	st.shared.v4.u32 	[%r410+2048], {%r321, %r322, %r323, %r324};
	add.s32 	%r410, %r410, 8192;
	add.s64 	%rd112, %rd112, 8192;
	add.s32 	%r409, %r409, 512;
	setp.lt.s32 	%p53, %r409, %r89;
	@%p53 bra 	$L__BB6_53;

$L__BB6_54:
	setp.ne.s32 	%p54, %r414, 0;
	and.b32  	%r106, %r3, 3;
	setp.eq.s32 	%p55, %r106, 0;
	or.pred  	%p56, %p54, %p55;
	@%p56 bra 	$L__BB6_57;

	and.b32  	%r330, %r3, -4;
	shl.b32 	%r331, %r3, 2;
	and.b32  	%r332, %r331, -16;
	add.s32 	%r333, %r4, %r332;
	mov.u32 	%r334, shraw;
	add.s32 	%r412, %r334, %r333;
	cvt.s64.s32 	%rd96, %r330;
	add.s64 	%rd97, %rd96, %rd24;
	cvta.to.global.u64 	%rd98, %rd41;
	shl.b64 	%rd99, %rd97, 2;
	add.s64 	%rd113, %rd98, %rd99;
	mov.u32 	%r414, 0;
	mov.u32 	%r413, %r414;

$L__BB6_56:
	ld.global.nc.f32 	%f53, [%rd113];
	st.shared.f32 	[%r412], %f53;
	add.s32 	%r412, %r412, 4;
	add.s64 	%rd113, %rd113, 4;
	add.s32 	%r413, %r413, 1;
	setp.lt.u32 	%p57, %r413, %r106;
	@%p57 bra 	$L__BB6_56;

$L__BB6_57:
	bar.sync 	0;
	add.s32 	%r336, %r171, %r2;
	add.s32 	%r113, %r336, -1;
	shl.b32 	%r339, %r414, 1;
	add.s32 	%r114, %r339, %r174;
	setp.ge.s32 	%p58, %r114, %r169;
	@%p58 bra 	$L__BB6_84;

	setp.ge.s32 	%p59, %r114, %r113;
	add.s32 	%r115, %r114, 1;
	setp.lt.s32 	%p60, %r115, %r169;
	setp.ge.s32 	%p61, %r115, %r113;
	and.pred  	%p1, %p60, %p61;
	and.pred  	%p62, %p59, %p1;
	mov.f32 	%f147, 0f00000000;
	mov.f32 	%f149, 0f00000000;
	@%p62 bra 	$L__BB6_75;
	bra.uni 	$L__BB6_59;

$L__BB6_75:
	setp.lt.s32 	%p75, %r2, 1;
	mov.f32 	%f171, %f147;
	mov.f32 	%f172, %f149;
	@%p75 bra 	$L__BB6_82;

	add.s32 	%r363, %r2, -1;
	and.b32  	%r434, %r2, 3;
	setp.lt.u32 	%p76, %r363, 3;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r431, 0;
	mov.f32 	%f171, %f172;
	@%p76 bra 	$L__BB6_79;

	sub.s32 	%r430, %r2, %r434;
	shl.b32 	%r366, %r414, 3;
	add.s32 	%r152, %r4, %r366;
	mov.u32 	%r367, shraw;
	add.s32 	%r428, %r367, 8;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r431, 0;

$L__BB6_78:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f117, %f118, %f119, %f120}, [%r428+-8];
	add.s32 	%r368, %r428, %r152;
	ld.shared.v2.f32 	{%f125, %f126}, [%r368+-8];
	fma.rn.ftz.f32 	%f129, %f125, %f117, %f171;
	fma.rn.ftz.f32 	%f130, %f126, %f117, %f172;
	ld.shared.f32 	%f131, [%r368+-4];
	fma.rn.ftz.f32 	%f132, %f131, %f118, %f129;
	ld.shared.v2.f32 	{%f133, %f134}, [%r368];
	fma.rn.ftz.f32 	%f137, %f133, %f118, %f130;
	ld.shared.f32 	%f138, [%r368];
	fma.rn.ftz.f32 	%f139, %f138, %f119, %f132;
	fma.rn.ftz.f32 	%f140, %f134, %f119, %f137;
	ld.shared.f32 	%f141, [%r368+4];
	fma.rn.ftz.f32 	%f171, %f141, %f120, %f139;
	ld.shared.f32 	%f142, [%r368+8];
	fma.rn.ftz.f32 	%f172, %f142, %f120, %f140;
	add.s32 	%r431, %r431, 4;
	add.s32 	%r428, %r428, 16;
	add.s32 	%r430, %r430, -4;
	setp.ne.s32 	%p77, %r430, 0;
	@%p77 bra 	$L__BB6_78;

$L__BB6_79:
	setp.eq.s32 	%p78, %r434, 0;
	@%p78 bra 	$L__BB6_82;

	shl.b32 	%r369, %r414, 3;
	add.s32 	%r370, %r4, %r369;
	shl.b32 	%r371, %r431, 2;
	add.s32 	%r372, %r370, %r371;
	mov.u32 	%r373, shraw;
	add.s32 	%r374, %r373, %r372;
	add.s32 	%r433, %r374, 4;
	add.s32 	%r432, %r373, %r371;

$L__BB6_81:
	.pragma "nounroll";
	ld.shared.f32 	%f143, [%r433+-4];
	ld.shared.f32 	%f144, [%r432];
	fma.rn.ftz.f32 	%f171, %f143, %f144, %f171;
	ld.shared.f32 	%f145, [%r433];
	fma.rn.ftz.f32 	%f172, %f145, %f144, %f172;
	add.s32 	%r433, %r433, 4;
	add.s32 	%r432, %r432, 4;
	add.s32 	%r434, %r434, -1;
	setp.ne.s32 	%p79, %r434, 0;
	@%p79 bra 	$L__BB6_81;
	bra.uni 	$L__BB6_82;

$L__BB6_59:
	mov.f32 	%f171, 0f7FC00000;
	mov.f32 	%f172, 0f7FC00000;
	@%p59 bra 	$L__BB6_68;
	bra.uni 	$L__BB6_60;

$L__BB6_68:
	setp.lt.s32 	%p70, %r2, 1;
	mov.f32 	%f171, %f147;
	@%p70 bra 	$L__BB6_82;

	add.s32 	%r353, %r2, -1;
	and.b32  	%r427, %r2, 3;
	setp.lt.u32 	%p71, %r353, 3;
	mov.f32 	%f171, 0f00000000;
	mov.u32 	%r425, 0;
	@%p71 bra 	$L__BB6_72;

	sub.s32 	%r424, %r2, %r427;
	shl.b32 	%r356, %r414, 3;
	add.s32 	%r136, %r4, %r356;
	mov.f32 	%f171, 0f00000000;
	mov.u32 	%r425, 0;
	mov.u32 	%r422, shraw;

$L__BB6_71:
	.pragma "nounroll";
	add.s32 	%r357, %r422, %r136;
	ld.shared.v2.f32 	{%f87, %f88}, [%r357];
	ld.shared.v4.f32 	{%f91, %f92, %f93, %f94}, [%r422];
	fma.rn.ftz.f32 	%f99, %f87, %f91, %f171;
	fma.rn.ftz.f32 	%f100, %f88, %f92, %f99;
	ld.shared.v2.f32 	{%f101, %f102}, [%r357+8];
	fma.rn.ftz.f32 	%f105, %f101, %f93, %f100;
	fma.rn.ftz.f32 	%f171, %f102, %f94, %f105;
	add.s32 	%r425, %r425, 4;
	add.s32 	%r422, %r422, 16;
	add.s32 	%r424, %r424, -4;
	setp.ne.s32 	%p72, %r424, 0;
	@%p72 bra 	$L__BB6_71;

$L__BB6_72:
	setp.eq.s32 	%p73, %r427, 0;
	@%p73 bra 	$L__BB6_82;

	shl.b32 	%r358, %r425, 2;
	mov.u32 	%r359, shraw;
	add.s32 	%r426, %r359, %r358;
	shl.b32 	%r360, %r414, 3;
	add.s32 	%r145, %r4, %r360;

$L__BB6_74:
	.pragma "nounroll";
	add.s32 	%r361, %r426, %r145;
	ld.shared.f32 	%f108, [%r426];
	ld.shared.f32 	%f109, [%r361];
	fma.rn.ftz.f32 	%f171, %f109, %f108, %f171;
	add.s32 	%r426, %r426, 4;
	add.s32 	%r427, %r427, -1;
	setp.eq.s32 	%p74, %r427, 0;
	@%p74 bra 	$L__BB6_82;
	bra.uni 	$L__BB6_74;

$L__BB6_60:
	not.pred 	%p64, %p1;
	@%p64 bra 	$L__BB6_82;

	setp.lt.s32 	%p65, %r2, 1;
	mov.f32 	%f172, %f149;
	@%p65 bra 	$L__BB6_82;

	add.s32 	%r341, %r2, -1;
	and.b32  	%r421, %r2, 3;
	setp.lt.u32 	%p66, %r341, 3;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r419, 0;
	@%p66 bra 	$L__BB6_65;

	sub.s32 	%r418, %r2, %r421;
	shl.b32 	%r344, %r414, 3;
	add.s32 	%r345, %r4, %r344;
	mov.u32 	%r415, shraw;
	add.s32 	%r346, %r415, %r345;
	add.s32 	%r416, %r346, 8;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r419, 0;

$L__BB6_64:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f61, %f62, %f63, %f64}, [%r415];
	ld.shared.f32 	%f69, [%r416+-4];
	fma.rn.ftz.f32 	%f70, %f69, %f61, %f172;
	ld.shared.v2.f32 	{%f71, %f72}, [%r416];
	fma.rn.ftz.f32 	%f75, %f71, %f62, %f70;
	fma.rn.ftz.f32 	%f76, %f72, %f63, %f75;
	ld.shared.f32 	%f77, [%r416+8];
	fma.rn.ftz.f32 	%f172, %f77, %f64, %f76;
	add.s32 	%r419, %r419, 4;
	add.s32 	%r416, %r416, 16;
	add.s32 	%r415, %r415, 16;
	add.s32 	%r418, %r418, -4;
	setp.ne.s32 	%p67, %r418, 0;
	@%p67 bra 	$L__BB6_64;

$L__BB6_65:
	setp.eq.s32 	%p68, %r421, 0;
	@%p68 bra 	$L__BB6_82;

	shl.b32 	%r347, %r419, 2;
	mov.u32 	%r348, shraw;
	add.s32 	%r420, %r348, %r347;
	shl.b32 	%r349, %r414, 3;
	add.s32 	%r350, %r4, %r349;
	add.s32 	%r129, %r350, 4;

$L__BB6_67:
	.pragma "nounroll";
	add.s32 	%r351, %r420, %r129;
	ld.shared.f32 	%f80, [%r420];
	ld.shared.f32 	%f81, [%r351];
	fma.rn.ftz.f32 	%f172, %f81, %f80, %f172;
	add.s32 	%r420, %r420, 4;
	add.s32 	%r421, %r421, -1;
	setp.eq.s32 	%p69, %r421, 0;
	@%p69 bra 	$L__BB6_82;
	bra.uni 	$L__BB6_67;

$L__BB6_82:
	mad.lo.s32 	%r376, %r1, %r169, %r114;
	cvta.to.global.u64 	%rd100, %rd44;
	mul.wide.s32 	%rd101, %r376, 4;
	add.s64 	%rd40, %rd100, %rd101;
	st.global.f32 	[%rd40], %f171;
	setp.ge.s32 	%p80, %r115, %r169;
	@%p80 bra 	$L__BB6_84;

	st.global.f32 	[%rd40+4], %f172;

$L__BB6_84:
	ret;

}
	// .globl	alma_batch_tiled_f32_2x_tile512
.visible .entry alma_batch_tiled_f32_2x_tile512(
	.param .u64 alma_batch_tiled_f32_2x_tile512_param_0,
	.param .u64 alma_batch_tiled_f32_2x_tile512_param_1,
	.param .u64 alma_batch_tiled_f32_2x_tile512_param_2,
	.param .u64 alma_batch_tiled_f32_2x_tile512_param_3,
	.param .u32 alma_batch_tiled_f32_2x_tile512_param_4,
	.param .u32 alma_batch_tiled_f32_2x_tile512_param_5,
	.param .u32 alma_batch_tiled_f32_2x_tile512_param_6,
	.param .u32 alma_batch_tiled_f32_2x_tile512_param_7,
	.param .u64 alma_batch_tiled_f32_2x_tile512_param_8
)
{
	.reg .pred 	%p<81>;
	.reg .f32 	%f<173>;
	.reg .b32 	%r<435>;
	.reg .b64 	%rd<114>;


	ld.param.u64 	%rd41, [alma_batch_tiled_f32_2x_tile512_param_0];
	ld.param.u64 	%rd42, [alma_batch_tiled_f32_2x_tile512_param_1];
	ld.param.u64 	%rd43, [alma_batch_tiled_f32_2x_tile512_param_2];
	ld.param.u32 	%r168, [alma_batch_tiled_f32_2x_tile512_param_4];
	ld.param.u32 	%r169, [alma_batch_tiled_f32_2x_tile512_param_5];
	ld.param.u32 	%r170, [alma_batch_tiled_f32_2x_tile512_param_6];
	ld.param.u32 	%r171, [alma_batch_tiled_f32_2x_tile512_param_7];
	ld.param.u64 	%rd44, [alma_batch_tiled_f32_2x_tile512_param_8];
	cvta.to.global.u64 	%rd1, %rd42;
	mov.u32 	%r172, %ntid.x;
	setp.ne.s32 	%p2, %r172, 256;
	@%p2 bra 	$L__BB7_84;

	mov.u32 	%r1, %ctaid.y;
	setp.ge.s32 	%p3, %r1, %r170;
	@%p3 bra 	$L__BB7_84;

	cvta.to.global.u64 	%rd45, %rd43;
	mul.wide.s32 	%rd46, %r1, 4;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.nc.u32 	%r2, [%rd47];
	mov.u32 	%r173, %ctaid.x;
	shl.b32 	%r174, %r173, 9;
	setp.ge.s32 	%p4, %r174, %r169;
	@%p4 bra 	$L__BB7_84;

	add.s32 	%r3, %r2, 511;
	shl.b32 	%r175, %r2, 2;
	add.s32 	%r176, %r175, 15;
	and.b32  	%r4, %r176, -16;
	mul.lo.s32 	%r177, %r1, %r168;
	cvt.s64.s32 	%rd2, %r177;
	mul.wide.s32 	%rd48, %r177, 4;
	add.s64 	%rd49, %rd42, %rd48;
	and.b64  	%rd50, %rd49, 15;
	setp.eq.s64 	%p5, %rd50, 0;
	@%p5 bra 	$L__BB7_11;

	mov.u32 	%r380, %tid.x;
	setp.ge.s32 	%p6, %r380, %r2;
	@%p6 bra 	$L__BB7_21;

	not.b32 	%r178, %r380;
	add.s32 	%r6, %r2, %r178;
	shr.u32 	%r179, %r6, 8;
	add.s32 	%r180, %r179, 1;
	and.b32  	%r379, %r180, 3;
	setp.eq.s32 	%p7, %r379, 0;
	@%p7 bra 	$L__BB7_8;

	shl.b32 	%r181, %r380, 2;
	mov.u32 	%r182, shraw;
	add.s32 	%r377, %r182, %r181;
	cvt.s64.s32 	%rd51, %r380;
	add.s64 	%rd52, %rd51, %rd2;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd102, %rd1, %rd53;

$L__BB7_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f37, [%rd102];
	st.shared.f32 	[%r377], %f37;
	add.s32 	%r380, %r380, 256;
	add.s32 	%r377, %r377, 1024;
	add.s64 	%rd102, %rd102, 1024;
	add.s32 	%r379, %r379, -1;
	setp.ne.s32 	%p8, %r379, 0;
	@%p8 bra 	$L__BB7_7;

$L__BB7_8:
	setp.lt.u32 	%p9, %r6, 768;
	@%p9 bra 	$L__BB7_21;

	shl.b32 	%r183, %r380, 2;
	mov.u32 	%r184, shraw;
	add.s32 	%r185, %r184, %r183;
	add.s32 	%r381, %r185, 2048;
	cvt.s64.s32 	%rd54, %r380;
	add.s64 	%rd55, %rd54, %rd2;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd57, %rd1, %rd56;
	add.s64 	%rd103, %rd57, 2048;

$L__BB7_10:
	ld.global.nc.f32 	%f38, [%rd103+-2048];
	st.shared.f32 	[%r381+-2048], %f38;
	ld.global.nc.f32 	%f39, [%rd103+-1024];
	st.shared.f32 	[%r381+-1024], %f39;
	ld.global.nc.f32 	%f40, [%rd103];
	st.shared.f32 	[%r381], %f40;
	ld.global.nc.f32 	%f41, [%rd103+1024];
	st.shared.f32 	[%r381+1024], %f41;
	add.s32 	%r381, %r381, 4096;
	add.s64 	%rd103, %rd103, 4096;
	add.s32 	%r380, %r380, 1024;
	setp.lt.s32 	%p10, %r380, %r2;
	@%p10 bra 	$L__BB7_10;
	bra.uni 	$L__BB7_21;

$L__BB7_11:
	shr.s32 	%r21, %r2, 2;
	mov.u32 	%r186, %tid.x;
	setp.ge.s32 	%p11, %r186, %r21;
	@%p11 bra 	$L__BB7_18;

	not.b32 	%r187, %r186;
	add.s32 	%r23, %r21, %r187;
	shr.u32 	%r188, %r23, 8;
	add.s32 	%r189, %r188, 1;
	and.b32  	%r385, %r189, 3;
	setp.eq.s32 	%p12, %r385, 0;
	mov.u32 	%r386, %r186;
	@%p12 bra 	$L__BB7_15;

	mov.u32 	%r386, %tid.x;
	mul.wide.s32 	%rd58, %r386, 4;
	add.s64 	%rd59, %rd58, %rd2;
	shl.b64 	%rd61, %rd59, 2;
	add.s64 	%rd104, %rd1, %rd61;
	shl.b32 	%r190, %r386, 4;
	mov.u32 	%r191, shraw;
	add.s32 	%r383, %r191, %r190;

$L__BB7_14:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r192, %r193, %r194, %r195}, [%rd104];
	st.shared.v4.u32 	[%r383], {%r192, %r193, %r194, %r195};
	add.s32 	%r386, %r386, 256;
	add.s64 	%rd104, %rd104, 4096;
	add.s32 	%r383, %r383, 4096;
	add.s32 	%r385, %r385, -1;
	setp.ne.s32 	%p13, %r385, 0;
	@%p13 bra 	$L__BB7_14;

$L__BB7_15:
	setp.lt.u32 	%p14, %r23, 768;
	@%p14 bra 	$L__BB7_18;

	shl.b32 	%r200, %r386, 4;
	mov.u32 	%r201, shraw;
	add.s32 	%r202, %r201, %r200;
	add.s32 	%r387, %r202, 8192;
	mul.wide.s32 	%rd62, %r386, 4;
	add.s64 	%rd64, %rd62, %rd2;
	shl.b64 	%rd65, %rd64, 2;
	add.s64 	%rd66, %rd1, %rd65;
	add.s64 	%rd105, %rd66, 8192;

$L__BB7_17:
	ld.global.nc.v4.u32 	{%r203, %r204, %r205, %r206}, [%rd105+-8192];
	st.shared.v4.u32 	[%r387+-8192], {%r203, %r204, %r205, %r206};
	ld.global.nc.v4.u32 	{%r211, %r212, %r213, %r214}, [%rd105+-4096];
	st.shared.v4.u32 	[%r387+-4096], {%r211, %r212, %r213, %r214};
	ld.global.nc.v4.u32 	{%r219, %r220, %r221, %r222}, [%rd105];
	st.shared.v4.u32 	[%r387], {%r219, %r220, %r221, %r222};
	ld.global.nc.v4.u32 	{%r227, %r228, %r229, %r230}, [%rd105+4096];
	st.shared.v4.u32 	[%r387+4096], {%r227, %r228, %r229, %r230};
	add.s32 	%r387, %r387, 16384;
	add.s64 	%rd105, %rd105, 16384;
	add.s32 	%r386, %r386, 1024;
	setp.lt.s32 	%p15, %r386, %r21;
	@%p15 bra 	$L__BB7_17;

$L__BB7_18:
	setp.ne.s32 	%p16, %r186, 0;
	and.b32  	%r40, %r2, 3;
	setp.eq.s32 	%p17, %r40, 0;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	$L__BB7_21;

	and.b32  	%r237, %r2, -4;
	and.b32  	%r239, %r175, -16;
	mov.u32 	%r240, shraw;
	add.s32 	%r389, %r240, %r239;
	cvt.s64.s32 	%rd67, %r237;
	add.s64 	%rd68, %rd67, %rd2;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd106, %rd1, %rd70;
	mov.u32 	%r390, 0;

$L__BB7_20:
	ld.global.nc.f32 	%f42, [%rd106];
	st.shared.f32 	[%r389], %f42;
	add.s32 	%r389, %r389, 4;
	add.s64 	%rd106, %rd106, 4;
	add.s32 	%r390, %r390, 1;
	setp.lt.u32 	%p19, %r390, %r40;
	@%p19 bra 	$L__BB7_20;

$L__BB7_21:
	bar.sync 	0;
	or.b32  	%r243, %r174, 1;
	sub.s32 	%r46, %r243, %r2;
	setp.gt.s32 	%p20, %r46, -1;
	add.s32 	%r244, %r174, 512;
	setp.le.s32 	%p21, %r244, %r169;
	and.pred  	%p22, %p20, %p21;
	@%p22 bra 	$L__BB7_39;
	bra.uni 	$L__BB7_22;

$L__BB7_39:
	cvt.s64.s32 	%rd24, %r46;
	mul.wide.s32 	%rd75, %r46, 4;
	add.s64 	%rd76, %rd41, %rd75;
	and.b64  	%rd77, %rd76, 15;
	setp.eq.s64 	%p43, %rd77, 0;
	@%p43 bra 	$L__BB7_47;

	mov.u32 	%r414, %tid.x;
	setp.ge.s32 	%p44, %r414, %r3;
	@%p44 bra 	$L__BB7_57;

	add.s32 	%r269, %r2, 510;
	sub.s32 	%r74, %r269, %r414;
	shr.u32 	%r270, %r74, 8;
	add.s32 	%r271, %r270, 1;
	and.b32  	%r402, %r271, 3;
	setp.eq.s32 	%p45, %r402, 0;
	mov.u32 	%r403, %r414;
	@%p45 bra 	$L__BB7_44;

	shl.b32 	%r272, %r414, 2;
	add.s32 	%r273, %r4, %r272;
	mov.u32 	%r274, shraw;
	add.s32 	%r400, %r274, %r273;
	cvt.s64.s32 	%rd78, %r414;
	add.s64 	%rd79, %rd78, %rd24;
	cvta.to.global.u64 	%rd80, %rd41;
	shl.b64 	%rd81, %rd79, 2;
	add.s64 	%rd109, %rd80, %rd81;
	mov.u32 	%r403, %r414;

$L__BB7_43:
	.pragma "nounroll";
	ld.global.nc.f32 	%f48, [%rd109];
	st.shared.f32 	[%r400], %f48;
	add.s32 	%r403, %r403, 256;
	add.s32 	%r400, %r400, 1024;
	add.s64 	%rd109, %rd109, 1024;
	add.s32 	%r402, %r402, -1;
	setp.ne.s32 	%p46, %r402, 0;
	@%p46 bra 	$L__BB7_43;

$L__BB7_44:
	setp.lt.u32 	%p47, %r74, 768;
	@%p47 bra 	$L__BB7_57;

	shl.b32 	%r275, %r403, 2;
	add.s32 	%r276, %r4, %r275;
	mov.u32 	%r277, shraw;
	add.s32 	%r278, %r277, %r276;
	add.s32 	%r404, %r278, 2048;
	cvt.s64.s32 	%rd82, %r403;
	cvta.to.global.u64 	%rd83, %rd41;
	add.s64 	%rd84, %rd82, %rd24;
	shl.b64 	%rd85, %rd84, 2;
	add.s64 	%rd86, %rd83, %rd85;
	add.s64 	%rd110, %rd86, 2048;

$L__BB7_46:
	ld.global.nc.f32 	%f49, [%rd110+-2048];
	st.shared.f32 	[%r404+-2048], %f49;
	ld.global.nc.f32 	%f50, [%rd110+-1024];
	st.shared.f32 	[%r404+-1024], %f50;
	ld.global.nc.f32 	%f51, [%rd110];
	st.shared.f32 	[%r404], %f51;
	ld.global.nc.f32 	%f52, [%rd110+1024];
	st.shared.f32 	[%r404+1024], %f52;
	add.s32 	%r404, %r404, 4096;
	add.s64 	%rd110, %rd110, 4096;
	add.s32 	%r403, %r403, 1024;
	setp.lt.s32 	%p48, %r403, %r3;
	@%p48 bra 	$L__BB7_46;
	bra.uni 	$L__BB7_57;

$L__BB7_22:
	mov.u32 	%r414, %tid.x;
	setp.ge.s32 	%p23, %r414, %r3;
	@%p23 bra 	$L__BB7_57;

	add.s32 	%r245, %r2, 510;
	sub.s32 	%r246, %r245, %r414;
	shr.u32 	%r247, %r246, 8;
	add.s32 	%r248, %r247, 1;
	and.b32  	%r394, %r248, 3;
	setp.eq.s32 	%p24, %r394, 0;
	mov.u32 	%r395, %r414;
	@%p24 bra 	$L__BB7_28;

	shl.b32 	%r249, %r414, 2;
	add.s32 	%r250, %r4, %r249;
	mov.u32 	%r251, shraw;
	add.s32 	%r392, %r251, %r250;
	add.s32 	%r254, %r414, %r174;
	add.s32 	%r255, %r254, 1;
	sub.s32 	%r391, %r255, %r2;
	cvta.to.global.u64 	%rd71, %rd41;
	mul.wide.s32 	%rd72, %r391, 4;
	add.s64 	%rd107, %rd71, %rd72;
	mov.u32 	%r395, %r414;

$L__BB7_25:
	.pragma "nounroll";
	setp.ge.s32 	%p25, %r391, %r169;
	setp.lt.s32 	%p26, %r391, 0;
	mov.f32 	%f150, 0f00000000;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB7_27;

	ld.global.nc.f32 	%f150, [%rd107];

$L__BB7_27:
	st.shared.f32 	[%r392], %f150;
	add.s32 	%r395, %r395, 256;
	add.s32 	%r392, %r392, 1024;
	add.s64 	%rd107, %rd107, 1024;
	add.s32 	%r391, %r391, 256;
	add.s32 	%r394, %r394, -1;
	setp.ne.s32 	%p28, %r394, 0;
	@%p28 bra 	$L__BB7_25;

$L__BB7_28:
	setp.lt.u32 	%p29, %r246, 768;
	@%p29 bra 	$L__BB7_57;

	add.s32 	%r260, %r395, %r174;
	add.s32 	%r261, %r260, 1;
	sub.s32 	%r397, %r261, %r2;
	cvta.to.global.u64 	%rd73, %rd41;
	mul.wide.s32 	%rd74, %r397, 4;
	add.s64 	%rd108, %rd73, %rd74;
	add.s32 	%r262, %r260, 769;
	sub.s32 	%r396, %r262, %r2;
	shl.b32 	%r263, %r395, 2;
	add.s32 	%r264, %r4, %r263;
	mov.u32 	%r265, shraw;
	add.s32 	%r399, %r265, %r264;

$L__BB7_30:
	setp.ge.s32 	%p30, %r397, %r169;
	setp.lt.s32 	%p31, %r397, 0;
	mov.f32 	%f152, 0f00000000;
	or.pred  	%p32, %p31, %p30;
	mov.f32 	%f151, %f152;
	@%p32 bra 	$L__BB7_32;

	ld.global.nc.f32 	%f151, [%rd108];

$L__BB7_32:
	st.shared.f32 	[%r399], %f151;
	add.s32 	%r266, %r396, -512;
	setp.lt.s32 	%p33, %r266, 0;
	setp.ge.s32 	%p34, %r266, %r169;
	or.pred  	%p35, %p33, %p34;
	@%p35 bra 	$L__BB7_34;

	ld.global.nc.f32 	%f152, [%rd108+1024];

$L__BB7_34:
	st.shared.f32 	[%r399+1024], %f152;
	add.s32 	%r267, %r396, -256;
	setp.lt.s32 	%p36, %r267, 0;
	setp.ge.s32 	%p37, %r267, %r169;
	mov.f32 	%f154, 0f00000000;
	or.pred  	%p38, %p36, %p37;
	mov.f32 	%f153, %f154;
	@%p38 bra 	$L__BB7_36;

	ld.global.nc.f32 	%f153, [%rd108+2048];

$L__BB7_36:
	st.shared.f32 	[%r399+2048], %f153;
	add.s32 	%r268, %r397, 768;
	setp.lt.s32 	%p39, %r268, 0;
	setp.ge.s32 	%p40, %r268, %r169;
	or.pred  	%p41, %p39, %p40;
	@%p41 bra 	$L__BB7_38;

	ld.global.nc.f32 	%f154, [%rd108+3072];

$L__BB7_38:
	add.s64 	%rd108, %rd108, 4096;
	add.s32 	%r69, %r399, 4096;
	st.shared.f32 	[%r399+3072], %f154;
	add.s32 	%r397, %r397, 1024;
	add.s32 	%r396, %r396, 1024;
	add.s32 	%r395, %r395, 1024;
	setp.lt.s32 	%p42, %r395, %r3;
	mov.u32 	%r399, %r69;
	@%p42 bra 	$L__BB7_30;
	bra.uni 	$L__BB7_57;

$L__BB7_47:
	shr.s32 	%r89, %r3, 2;
	mov.u32 	%r414, %tid.x;
	setp.ge.s32 	%p49, %r414, %r89;
	@%p49 bra 	$L__BB7_54;

	not.b32 	%r279, %r414;
	add.s32 	%r91, %r89, %r279;
	shr.u32 	%r280, %r91, 8;
	add.s32 	%r281, %r280, 1;
	and.b32  	%r408, %r281, 3;
	setp.eq.s32 	%p50, %r408, 0;
	mov.u32 	%r409, %r414;
	@%p50 bra 	$L__BB7_51;

	mul.wide.s32 	%rd87, %r414, 4;
	add.s64 	%rd88, %rd87, %rd24;
	cvta.to.global.u64 	%rd89, %rd41;
	shl.b64 	%rd90, %rd88, 2;
	add.s64 	%rd111, %rd89, %rd90;
	shl.b32 	%r282, %r414, 4;
	add.s32 	%r283, %r4, %r282;
	mov.u32 	%r284, shraw;
	add.s32 	%r406, %r284, %r283;
	mov.u32 	%r409, %r414;

$L__BB7_50:
	.pragma "nounroll";
	ld.global.nc.v4.u32 	{%r285, %r286, %r287, %r288}, [%rd111];
	st.shared.v4.u32 	[%r406], {%r285, %r286, %r287, %r288};
	add.s32 	%r409, %r409, 256;
	add.s64 	%rd111, %rd111, 4096;
	add.s32 	%r406, %r406, 4096;
	add.s32 	%r408, %r408, -1;
	setp.ne.s32 	%p51, %r408, 0;
	@%p51 bra 	$L__BB7_50;

$L__BB7_51:
	setp.lt.u32 	%p52, %r91, 768;
	@%p52 bra 	$L__BB7_54;

	shl.b32 	%r293, %r409, 4;
	add.s32 	%r294, %r4, %r293;
	mov.u32 	%r295, shraw;
	add.s32 	%r296, %r295, %r294;
	add.s32 	%r410, %r296, 8192;
	mul.wide.s32 	%rd91, %r409, 4;
	cvta.to.global.u64 	%rd92, %rd41;
	add.s64 	%rd93, %rd91, %rd24;
	shl.b64 	%rd94, %rd93, 2;
	add.s64 	%rd95, %rd92, %rd94;
	add.s64 	%rd112, %rd95, 8192;

$L__BB7_53:
	ld.global.nc.v4.u32 	{%r297, %r298, %r299, %r300}, [%rd112+-8192];
	st.shared.v4.u32 	[%r410+-8192], {%r297, %r298, %r299, %r300};
	ld.global.nc.v4.u32 	{%r305, %r306, %r307, %r308}, [%rd112+-4096];
	st.shared.v4.u32 	[%r410+-4096], {%r305, %r306, %r307, %r308};
	ld.global.nc.v4.u32 	{%r313, %r314, %r315, %r316}, [%rd112];
	st.shared.v4.u32 	[%r410], {%r313, %r314, %r315, %r316};
	ld.global.nc.v4.u32 	{%r321, %r322, %r323, %r324}, [%rd112+4096];
	st.shared.v4.u32 	[%r410+4096], {%r321, %r322, %r323, %r324};
	add.s32 	%r410, %r410, 16384;
	add.s64 	%rd112, %rd112, 16384;
	add.s32 	%r409, %r409, 1024;
	setp.lt.s32 	%p53, %r409, %r89;
	@%p53 bra 	$L__BB7_53;

$L__BB7_54:
	setp.ne.s32 	%p54, %r414, 0;
	and.b32  	%r106, %r3, 3;
	setp.eq.s32 	%p55, %r106, 0;
	or.pred  	%p56, %p54, %p55;
	@%p56 bra 	$L__BB7_57;

	and.b32  	%r330, %r3, -4;
	shl.b32 	%r331, %r3, 2;
	and.b32  	%r332, %r331, -16;
	add.s32 	%r333, %r4, %r332;
	mov.u32 	%r334, shraw;
	add.s32 	%r412, %r334, %r333;
	cvt.s64.s32 	%rd96, %r330;
	add.s64 	%rd97, %rd96, %rd24;
	cvta.to.global.u64 	%rd98, %rd41;
	shl.b64 	%rd99, %rd97, 2;
	add.s64 	%rd113, %rd98, %rd99;
	mov.u32 	%r414, 0;
	mov.u32 	%r413, %r414;

$L__BB7_56:
	ld.global.nc.f32 	%f53, [%rd113];
	st.shared.f32 	[%r412], %f53;
	add.s32 	%r412, %r412, 4;
	add.s64 	%rd113, %rd113, 4;
	add.s32 	%r413, %r413, 1;
	setp.lt.u32 	%p57, %r413, %r106;
	@%p57 bra 	$L__BB7_56;

$L__BB7_57:
	bar.sync 	0;
	add.s32 	%r336, %r171, %r2;
	add.s32 	%r113, %r336, -1;
	shl.b32 	%r339, %r414, 1;
	add.s32 	%r114, %r339, %r174;
	setp.ge.s32 	%p58, %r114, %r169;
	@%p58 bra 	$L__BB7_84;

	setp.ge.s32 	%p59, %r114, %r113;
	add.s32 	%r115, %r114, 1;
	setp.lt.s32 	%p60, %r115, %r169;
	setp.ge.s32 	%p61, %r115, %r113;
	and.pred  	%p1, %p60, %p61;
	and.pred  	%p62, %p59, %p1;
	mov.f32 	%f147, 0f00000000;
	mov.f32 	%f149, 0f00000000;
	@%p62 bra 	$L__BB7_75;
	bra.uni 	$L__BB7_59;

$L__BB7_75:
	setp.lt.s32 	%p75, %r2, 1;
	mov.f32 	%f171, %f147;
	mov.f32 	%f172, %f149;
	@%p75 bra 	$L__BB7_82;

	add.s32 	%r363, %r2, -1;
	and.b32  	%r434, %r2, 3;
	setp.lt.u32 	%p76, %r363, 3;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r431, 0;
	mov.f32 	%f171, %f172;
	@%p76 bra 	$L__BB7_79;

	sub.s32 	%r430, %r2, %r434;
	shl.b32 	%r366, %r414, 3;
	add.s32 	%r152, %r4, %r366;
	mov.u32 	%r367, shraw;
	add.s32 	%r428, %r367, 8;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r431, 0;

$L__BB7_78:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f117, %f118, %f119, %f120}, [%r428+-8];
	add.s32 	%r368, %r428, %r152;
	ld.shared.v2.f32 	{%f125, %f126}, [%r368+-8];
	fma.rn.ftz.f32 	%f129, %f125, %f117, %f171;
	fma.rn.ftz.f32 	%f130, %f126, %f117, %f172;
	ld.shared.f32 	%f131, [%r368+-4];
	fma.rn.ftz.f32 	%f132, %f131, %f118, %f129;
	ld.shared.v2.f32 	{%f133, %f134}, [%r368];
	fma.rn.ftz.f32 	%f137, %f133, %f118, %f130;
	ld.shared.f32 	%f138, [%r368];
	fma.rn.ftz.f32 	%f139, %f138, %f119, %f132;
	fma.rn.ftz.f32 	%f140, %f134, %f119, %f137;
	ld.shared.f32 	%f141, [%r368+4];
	fma.rn.ftz.f32 	%f171, %f141, %f120, %f139;
	ld.shared.f32 	%f142, [%r368+8];
	fma.rn.ftz.f32 	%f172, %f142, %f120, %f140;
	add.s32 	%r431, %r431, 4;
	add.s32 	%r428, %r428, 16;
	add.s32 	%r430, %r430, -4;
	setp.ne.s32 	%p77, %r430, 0;
	@%p77 bra 	$L__BB7_78;

$L__BB7_79:
	setp.eq.s32 	%p78, %r434, 0;
	@%p78 bra 	$L__BB7_82;

	shl.b32 	%r369, %r414, 3;
	add.s32 	%r370, %r4, %r369;
	shl.b32 	%r371, %r431, 2;
	add.s32 	%r372, %r370, %r371;
	mov.u32 	%r373, shraw;
	add.s32 	%r374, %r373, %r372;
	add.s32 	%r433, %r374, 4;
	add.s32 	%r432, %r373, %r371;

$L__BB7_81:
	.pragma "nounroll";
	ld.shared.f32 	%f143, [%r433+-4];
	ld.shared.f32 	%f144, [%r432];
	fma.rn.ftz.f32 	%f171, %f143, %f144, %f171;
	ld.shared.f32 	%f145, [%r433];
	fma.rn.ftz.f32 	%f172, %f145, %f144, %f172;
	add.s32 	%r433, %r433, 4;
	add.s32 	%r432, %r432, 4;
	add.s32 	%r434, %r434, -1;
	setp.ne.s32 	%p79, %r434, 0;
	@%p79 bra 	$L__BB7_81;
	bra.uni 	$L__BB7_82;

$L__BB7_59:
	mov.f32 	%f171, 0f7FC00000;
	mov.f32 	%f172, 0f7FC00000;
	@%p59 bra 	$L__BB7_68;
	bra.uni 	$L__BB7_60;

$L__BB7_68:
	setp.lt.s32 	%p70, %r2, 1;
	mov.f32 	%f171, %f147;
	@%p70 bra 	$L__BB7_82;

	add.s32 	%r353, %r2, -1;
	and.b32  	%r427, %r2, 3;
	setp.lt.u32 	%p71, %r353, 3;
	mov.f32 	%f171, 0f00000000;
	mov.u32 	%r425, 0;
	@%p71 bra 	$L__BB7_72;

	sub.s32 	%r424, %r2, %r427;
	shl.b32 	%r356, %r414, 3;
	add.s32 	%r136, %r4, %r356;
	mov.f32 	%f171, 0f00000000;
	mov.u32 	%r425, 0;
	mov.u32 	%r422, shraw;

$L__BB7_71:
	.pragma "nounroll";
	add.s32 	%r357, %r422, %r136;
	ld.shared.v2.f32 	{%f87, %f88}, [%r357];
	ld.shared.v4.f32 	{%f91, %f92, %f93, %f94}, [%r422];
	fma.rn.ftz.f32 	%f99, %f87, %f91, %f171;
	fma.rn.ftz.f32 	%f100, %f88, %f92, %f99;
	ld.shared.v2.f32 	{%f101, %f102}, [%r357+8];
	fma.rn.ftz.f32 	%f105, %f101, %f93, %f100;
	fma.rn.ftz.f32 	%f171, %f102, %f94, %f105;
	add.s32 	%r425, %r425, 4;
	add.s32 	%r422, %r422, 16;
	add.s32 	%r424, %r424, -4;
	setp.ne.s32 	%p72, %r424, 0;
	@%p72 bra 	$L__BB7_71;

$L__BB7_72:
	setp.eq.s32 	%p73, %r427, 0;
	@%p73 bra 	$L__BB7_82;

	shl.b32 	%r358, %r425, 2;
	mov.u32 	%r359, shraw;
	add.s32 	%r426, %r359, %r358;
	shl.b32 	%r360, %r414, 3;
	add.s32 	%r145, %r4, %r360;

$L__BB7_74:
	.pragma "nounroll";
	add.s32 	%r361, %r426, %r145;
	ld.shared.f32 	%f108, [%r426];
	ld.shared.f32 	%f109, [%r361];
	fma.rn.ftz.f32 	%f171, %f109, %f108, %f171;
	add.s32 	%r426, %r426, 4;
	add.s32 	%r427, %r427, -1;
	setp.eq.s32 	%p74, %r427, 0;
	@%p74 bra 	$L__BB7_82;
	bra.uni 	$L__BB7_74;

$L__BB7_60:
	not.pred 	%p64, %p1;
	@%p64 bra 	$L__BB7_82;

	setp.lt.s32 	%p65, %r2, 1;
	mov.f32 	%f172, %f149;
	@%p65 bra 	$L__BB7_82;

	add.s32 	%r341, %r2, -1;
	and.b32  	%r421, %r2, 3;
	setp.lt.u32 	%p66, %r341, 3;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r419, 0;
	@%p66 bra 	$L__BB7_65;

	sub.s32 	%r418, %r2, %r421;
	shl.b32 	%r344, %r414, 3;
	add.s32 	%r345, %r4, %r344;
	mov.u32 	%r415, shraw;
	add.s32 	%r346, %r415, %r345;
	add.s32 	%r416, %r346, 8;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r419, 0;

$L__BB7_64:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f61, %f62, %f63, %f64}, [%r415];
	ld.shared.f32 	%f69, [%r416+-4];
	fma.rn.ftz.f32 	%f70, %f69, %f61, %f172;
	ld.shared.v2.f32 	{%f71, %f72}, [%r416];
	fma.rn.ftz.f32 	%f75, %f71, %f62, %f70;
	fma.rn.ftz.f32 	%f76, %f72, %f63, %f75;
	ld.shared.f32 	%f77, [%r416+8];
	fma.rn.ftz.f32 	%f172, %f77, %f64, %f76;
	add.s32 	%r419, %r419, 4;
	add.s32 	%r416, %r416, 16;
	add.s32 	%r415, %r415, 16;
	add.s32 	%r418, %r418, -4;
	setp.ne.s32 	%p67, %r418, 0;
	@%p67 bra 	$L__BB7_64;

$L__BB7_65:
	setp.eq.s32 	%p68, %r421, 0;
	@%p68 bra 	$L__BB7_82;

	shl.b32 	%r347, %r419, 2;
	mov.u32 	%r348, shraw;
	add.s32 	%r420, %r348, %r347;
	shl.b32 	%r349, %r414, 3;
	add.s32 	%r350, %r4, %r349;
	add.s32 	%r129, %r350, 4;

$L__BB7_67:
	.pragma "nounroll";
	add.s32 	%r351, %r420, %r129;
	ld.shared.f32 	%f80, [%r420];
	ld.shared.f32 	%f81, [%r351];
	fma.rn.ftz.f32 	%f172, %f81, %f80, %f172;
	add.s32 	%r420, %r420, 4;
	add.s32 	%r421, %r421, -1;
	setp.eq.s32 	%p69, %r421, 0;
	@%p69 bra 	$L__BB7_82;
	bra.uni 	$L__BB7_67;

$L__BB7_82:
	mad.lo.s32 	%r376, %r1, %r169, %r114;
	cvta.to.global.u64 	%rd100, %rd44;
	mul.wide.s32 	%rd101, %r376, 4;
	add.s64 	%rd40, %rd100, %rd101;
	st.global.f32 	[%rd40], %f171;
	setp.ge.s32 	%p80, %r115, %r169;
	@%p80 bra 	$L__BB7_84;

	st.global.f32 	[%rd40+4], %f172;

$L__BB7_84:
	ret;

}
	// .globl	alma_multi_series_one_param_f32
.visible .entry alma_multi_series_one_param_f32(
	.param .u64 alma_multi_series_one_param_f32_param_0,
	.param .u64 alma_multi_series_one_param_f32_param_1,
	.param .u32 alma_multi_series_one_param_f32_param_2,
	.param .f32 alma_multi_series_one_param_f32_param_3,
	.param .u32 alma_multi_series_one_param_f32_param_4,
	.param .u32 alma_multi_series_one_param_f32_param_5,
	.param .u64 alma_multi_series_one_param_f32_param_6,
	.param .u64 alma_multi_series_one_param_f32_param_7
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<87>;
	.reg .b64 	%rd<43>;


	ld.param.u64 	%rd20, [alma_multi_series_one_param_f32_param_0];
	ld.param.u64 	%rd21, [alma_multi_series_one_param_f32_param_1];
	ld.param.u32 	%r41, [alma_multi_series_one_param_f32_param_2];
	ld.param.u32 	%r42, [alma_multi_series_one_param_f32_param_4];
	ld.param.u32 	%r43, [alma_multi_series_one_param_f32_param_5];
	ld.param.u64 	%rd18, [alma_multi_series_one_param_f32_param_6];
	ld.param.u64 	%rd19, [alma_multi_series_one_param_f32_param_7];
	cvta.to.global.u64 	%rd1, %rd21;
	cvta.to.global.u64 	%rd2, %rd20;
	mov.u32 	%r44, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r3, %r44, %r1, %r2;
	mov.u32 	%r45, %ctaid.y;
	mov.u32 	%r4, %ntid.y;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r45, %r4, %r5;
	setp.ge.s32 	%p1, %r6, %r42;
	setp.ge.s32 	%p2, %r3, %r43;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB8_18;

	mad.lo.s32 	%r78, %r5, %r1, %r2;
	setp.ge.s32 	%p4, %r78, %r41;
	@%p4 bra 	$L__BB8_8;

	mul.lo.s32 	%r8, %r1, %r4;
	add.s32 	%r46, %r8, %r41;
	add.s32 	%r47, %r78, %r8;
	not.b32 	%r48, %r47;
	add.s32 	%r49, %r46, %r48;
	div.u32 	%r9, %r49, %r8;
	add.s32 	%r50, %r9, 1;
	and.b32  	%r77, %r50, 3;
	setp.eq.s32 	%p5, %r77, 0;
	@%p5 bra 	$L__BB8_5;

	shl.b32 	%r51, %r78, 2;
	mov.u32 	%r52, sh;
	add.s32 	%r75, %r52, %r51;
	shl.b32 	%r12, %r8, 2;
	mul.wide.s32 	%rd22, %r78, 4;
	add.s64 	%rd40, %rd1, %rd22;
	mul.wide.s32 	%rd4, %r8, 4;

$L__BB8_4:
	.pragma "nounroll";
	ld.global.nc.f32 	%f8, [%rd40];
	st.shared.f32 	[%r75], %f8;
	add.s32 	%r78, %r78, %r8;
	add.s32 	%r75, %r75, %r12;
	add.s64 	%rd40, %rd40, %rd4;
	add.s32 	%r77, %r77, -1;
	setp.ne.s32 	%p6, %r77, 0;
	@%p6 bra 	$L__BB8_4;

$L__BB8_5:
	setp.lt.u32 	%p7, %r9, 3;
	@%p7 bra 	$L__BB8_8;

	mul.wide.s32 	%rd7, %r8, 4;
	shl.b32 	%r53, %r78, 2;
	mov.u32 	%r54, sh;
	add.s32 	%r80, %r54, %r53;
	mul.lo.s32 	%r55, %r4, %r1;
	shl.b32 	%r21, %r55, 2;

$L__BB8_7:
	mul.wide.s32 	%rd23, %r78, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.nc.f32 	%f9, [%rd24];
	st.shared.f32 	[%r80], %f9;
	add.s64 	%rd25, %rd24, %rd7;
	ld.global.nc.f32 	%f10, [%rd25];
	add.s32 	%r56, %r80, %r21;
	st.shared.f32 	[%r56], %f10;
	add.s32 	%r57, %r78, %r8;
	add.s32 	%r58, %r57, %r8;
	add.s64 	%rd26, %rd25, %rd7;
	ld.global.nc.f32 	%f11, [%rd26];
	add.s32 	%r59, %r56, %r21;
	st.shared.f32 	[%r59], %f11;
	add.s32 	%r60, %r58, %r8;
	add.s64 	%rd27, %rd26, %rd7;
	ld.global.nc.f32 	%f12, [%rd27];
	add.s32 	%r61, %r59, %r21;
	add.s32 	%r80, %r61, %r21;
	st.shared.f32 	[%r61], %f12;
	add.s32 	%r78, %r60, %r8;
	setp.lt.s32 	%p8, %r78, %r41;
	@%p8 bra 	$L__BB8_7;

$L__BB8_8:
	bar.sync 	0;
	cvta.to.global.u64 	%rd28, %rd18;
	mul.wide.s32 	%rd29, %r6, 4;
	add.s64 	%rd30, %rd28, %rd29;
	add.s32 	%r26, %r41, -1;
	ld.global.nc.u32 	%r62, [%rd30];
	add.s32 	%r63, %r26, %r62;
	mad.lo.s32 	%r64, %r3, %r42, %r6;
	setp.lt.s32 	%p9, %r3, %r63;
	cvta.to.global.u64 	%rd31, %rd19;
	mul.wide.s32 	%rd32, %r64, 4;
	add.s64 	%rd8, %rd31, %rd32;
	@%p9 bra 	$L__BB8_17;
	bra.uni 	$L__BB8_9;

$L__BB8_17:
	mov.u32 	%r74, 2143289344;
	st.global.u32 	[%rd8], %r74;
	bra.uni 	$L__BB8_18;

$L__BB8_9:
	add.s32 	%r65, %r3, 1;
	sub.s32 	%r66, %r65, %r41;
	mad.lo.s32 	%r67, %r66, %r42, %r6;
	cvt.s64.s32 	%rd9, %r67;
	setp.lt.s32 	%p10, %r41, 1;
	mov.f32 	%f38, 0f00000000;
	@%p10 bra 	$L__BB8_16;

	and.b32  	%r86, %r41, 3;
	setp.lt.u32 	%p11, %r26, 3;
	mov.f32 	%f38, 0f00000000;
	mov.u32 	%r84, 0;
	@%p11 bra 	$L__BB8_13;

	sub.s32 	%r83, %r41, %r86;
	shl.b64 	%rd33, %rd9, 2;
	add.s64 	%rd41, %rd2, %rd33;
	mul.wide.s32 	%rd11, %r42, 4;
	mov.f32 	%f38, 0f00000000;
	mov.u32 	%r84, 0;
	mov.u32 	%r81, sh;

$L__BB8_12:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f17, %f18, %f19, %f20}, [%r81];
	ld.global.nc.f32 	%f25, [%rd41];
	fma.rn.ftz.f32 	%f26, %f25, %f17, %f38;
	add.s64 	%rd34, %rd41, %rd11;
	ld.global.nc.f32 	%f27, [%rd34];
	fma.rn.ftz.f32 	%f28, %f27, %f18, %f26;
	add.s64 	%rd35, %rd34, %rd11;
	ld.global.nc.f32 	%f29, [%rd35];
	fma.rn.ftz.f32 	%f30, %f29, %f19, %f28;
	add.s64 	%rd36, %rd35, %rd11;
	add.s64 	%rd41, %rd36, %rd11;
	ld.global.nc.f32 	%f31, [%rd36];
	fma.rn.ftz.f32 	%f38, %f31, %f20, %f30;
	add.s32 	%r84, %r84, 4;
	add.s32 	%r81, %r81, 16;
	add.s32 	%r83, %r83, -4;
	setp.ne.s32 	%p12, %r83, 0;
	@%p12 bra 	$L__BB8_12;

$L__BB8_13:
	setp.eq.s32 	%p13, %r86, 0;
	@%p13 bra 	$L__BB8_16;

	shl.b32 	%r71, %r84, 2;
	mov.u32 	%r72, sh;
	add.s32 	%r85, %r72, %r71;
	mul.lo.s32 	%r73, %r84, %r42;
	cvt.s64.s32 	%rd37, %r73;
	add.s64 	%rd38, %rd37, %rd9;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd42, %rd2, %rd39;
	mul.wide.s32 	%rd15, %r42, 4;

$L__BB8_15:
	.pragma "nounroll";
	ld.shared.f32 	%f32, [%r85];
	ld.global.nc.f32 	%f33, [%rd42];
	fma.rn.ftz.f32 	%f38, %f33, %f32, %f38;
	add.s32 	%r85, %r85, 4;
	add.s64 	%rd42, %rd42, %rd15;
	add.s32 	%r86, %r86, -1;
	setp.ne.s32 	%p14, %r86, 0;
	@%p14 bra 	$L__BB8_15;

$L__BB8_16:
	st.global.f32 	[%rd8], %f38;

$L__BB8_18:
	ret;

}
	// .globl	alma_precompute_weights_f32
.visible .entry alma_precompute_weights_f32(
	.param .u64 alma_precompute_weights_f32_param_0,
	.param .u64 alma_precompute_weights_f32_param_1,
	.param .u64 alma_precompute_weights_f32_param_2,
	.param .u32 alma_precompute_weights_f32_param_3,
	.param .u32 alma_precompute_weights_f32_param_4,
	.param .u64 alma_precompute_weights_f32_param_5,
	.param .u64 alma_precompute_weights_f32_param_6
)
{
	.reg .pred 	%p<21>;
	.reg .f32 	%f<58>;
	.reg .b32 	%r<70>;
	.reg .b64 	%rd<21>;
	// demoted variable
	.shared .align 4 .f32 _ZZ27alma_precompute_weights_f32E10inv_norm_s;

	ld.param.u64 	%rd3, [alma_precompute_weights_f32_param_0];
	ld.param.u64 	%rd4, [alma_precompute_weights_f32_param_1];
	ld.param.u64 	%rd5, [alma_precompute_weights_f32_param_2];
	ld.param.u32 	%r14, [alma_precompute_weights_f32_param_3];
	ld.param.u32 	%r13, [alma_precompute_weights_f32_param_4];
	ld.param.u64 	%rd6, [alma_precompute_weights_f32_param_5];
	ld.param.u64 	%rd7, [alma_precompute_weights_f32_param_6];
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r14;
	@%p1 bra 	$L__BB9_17;

	cvta.to.global.u64 	%rd8, %rd3;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.u32 	%r2, [%rd10];
	mov.u32 	%r69, %tid.x;
	setp.ge.s32 	%p2, %r69, %r2;
	mov.f32 	%f55, 0f00000000;
	@%p2 bra 	$L__BB9_4;

	cvta.to.global.u64 	%rd11, %rd4;
	shl.b64 	%rd12, %rd1, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.nc.f32 	%f14, [%rd13];
	cvta.to.global.u64 	%rd14, %rd5;
	add.s64 	%rd15, %rd14, %rd12;
	ld.global.nc.f32 	%f15, [%rd15];
	mov.f32 	%f16, 0f358637BD;
	max.ftz.f32 	%f17, %f15, %f16;
	cvt.rn.f32.s32 	%f18, %r2;
	div.approx.ftz.f32 	%f19, %f18, %f17;
	mov.u32 	%r4, %ntid.x;
	add.ftz.f32 	%f20, %f19, %f19;
	mul.ftz.f32 	%f1, %f19, %f20;
	add.s32 	%r15, %r2, -1;
	cvt.rn.f32.s32 	%f21, %r15;
	mul.ftz.f32 	%f2, %f14, %f21;
	mov.f32 	%f55, 0f00000000;
	mov.u32 	%r68, %r69;

$L__BB9_3:
	cvt.rn.f32.s32 	%f22, %r68;
	sub.ftz.f32 	%f23, %f22, %f2;
	mul.ftz.f32 	%f24, %f23, %f23;
	neg.ftz.f32 	%f25, %f24;
	div.approx.ftz.f32 	%f26, %f25, %f1;
	mul.ftz.f32 	%f27, %f26, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f28, %f27;
	shl.b32 	%r16, %r68, 2;
	mov.u32 	%r17, sh;
	add.s32 	%r18, %r17, %r16;
	st.shared.f32 	[%r18], %f28;
	add.ftz.f32 	%f55, %f55, %f28;
	add.s32 	%r68, %r68, %r4;
	setp.lt.s32 	%p3, %r68, %r2;
	@%p3 bra 	$L__BB9_3;

$L__BB9_4:
	shr.u32 	%r7, %r69, 5;
	mov.b32 	%r19, %f55;
	mov.u32 	%r20, 2;
	mov.u32 	%r21, 31;
	mov.u32 	%r22, 16;
	mov.u32 	%r23, -1;
	shfl.sync.down.b32 	%r24|%p4, %r19, %r22, %r21, %r23;
	mov.b32 	%f29, %r24;
	add.ftz.f32 	%f30, %f55, %f29;
	mov.b32 	%r25, %f30;
	mov.u32 	%r26, 8;
	shfl.sync.down.b32 	%r27|%p5, %r25, %r26, %r21, %r23;
	mov.b32 	%f31, %r27;
	add.ftz.f32 	%f32, %f30, %f31;
	mov.b32 	%r28, %f32;
	mov.u32 	%r29, 4;
	shfl.sync.down.b32 	%r30|%p6, %r28, %r29, %r21, %r23;
	mov.b32 	%f33, %r30;
	add.ftz.f32 	%f34, %f32, %f33;
	mov.b32 	%r31, %f34;
	shfl.sync.down.b32 	%r32|%p7, %r31, %r20, %r21, %r23;
	mov.b32 	%f35, %r32;
	add.ftz.f32 	%f36, %f34, %f35;
	mov.b32 	%r33, %f36;
	mov.u32 	%r34, 1;
	shfl.sync.down.b32 	%r35|%p8, %r33, %r34, %r21, %r23;
	mov.b32 	%f37, %r35;
	add.ftz.f32 	%f6, %f36, %f37;
	and.b32  	%r8, %r69, 31;
	setp.ne.s32 	%p9, %r8, 0;
	@%p9 bra 	$L__BB9_6;

	shl.b32 	%r36, %r7, 2;
	mov.u32 	%r37, _ZZ14alma_block_sumfE8warp_buf;
	add.s32 	%r38, %r37, %r36;
	st.shared.f32 	[%r38], %f6;

$L__BB9_6:
	bar.sync 	0;
	setp.ne.s32 	%p10, %r7, 0;
	mov.f32 	%f57, 0f00000000;
	@%p10 bra 	$L__BB9_10;

	mov.u32 	%r39, %ntid.x;
	add.s32 	%r40, %r39, 31;
	shr.u32 	%r41, %r40, 5;
	setp.ge.u32 	%p11, %r8, %r41;
	mov.f32 	%f56, 0f00000000;
	@%p11 bra 	$L__BB9_9;

	shl.b32 	%r42, %r8, 2;
	mov.u32 	%r43, _ZZ14alma_block_sumfE8warp_buf;
	add.s32 	%r44, %r43, %r42;
	ld.shared.f32 	%f56, [%r44];

$L__BB9_9:
	mov.b32 	%r45, %f56;
	mov.u32 	%r46, 2;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.down.b32 	%r50|%p12, %r45, %r48, %r47, %r49;
	mov.b32 	%f40, %r50;
	add.ftz.f32 	%f41, %f56, %f40;
	mov.b32 	%r51, %f41;
	mov.u32 	%r52, 8;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r47, %r49;
	mov.b32 	%f42, %r53;
	add.ftz.f32 	%f43, %f41, %f42;
	mov.b32 	%r54, %f43;
	mov.u32 	%r55, 4;
	shfl.sync.down.b32 	%r56|%p14, %r54, %r55, %r47, %r49;
	mov.b32 	%f44, %r56;
	add.ftz.f32 	%f45, %f43, %f44;
	mov.b32 	%r57, %f45;
	shfl.sync.down.b32 	%r58|%p15, %r57, %r46, %r47, %r49;
	mov.b32 	%f46, %r58;
	add.ftz.f32 	%f47, %f45, %f46;
	mov.b32 	%r59, %f47;
	mov.u32 	%r60, 1;
	shfl.sync.down.b32 	%r61|%p16, %r59, %r60, %r47, %r49;
	mov.b32 	%f48, %r61;
	add.ftz.f32 	%f57, %f47, %f48;

$L__BB9_10:
	setp.ne.s32 	%p17, %r69, 0;
	@%p17 bra 	$L__BB9_12;

	mov.f32 	%f49, 0f1E3CE508;
	max.ftz.f32 	%f50, %f57, %f49;
	rcp.approx.ftz.f32 	%f51, %f50;
	st.shared.f32 	[_ZZ27alma_precompute_weights_f32E10inv_norm_s], %f51;

$L__BB9_12:
	bar.sync 	0;
	@%p2 bra 	$L__BB9_15;

	ld.shared.f32 	%f11, [_ZZ27alma_precompute_weights_f32E10inv_norm_s];
	mul.lo.s32 	%r9, %r1, %r13;
	mov.u32 	%r10, %ntid.x;
	cvta.to.global.u64 	%rd2, %rd6;

$L__BB9_14:
	shl.b32 	%r63, %r69, 2;
	mov.u32 	%r64, sh;
	add.s32 	%r65, %r64, %r63;
	ld.shared.f32 	%f52, [%r65];
	mul.ftz.f32 	%f53, %f52, %f11;
	add.s32 	%r66, %r69, %r9;
	mul.wide.s32 	%rd16, %r66, 4;
	add.s64 	%rd17, %rd2, %rd16;
	st.global.f32 	[%rd17], %f53;
	add.s32 	%r69, %r69, %r10;
	setp.lt.s32 	%p19, %r69, %r2;
	@%p19 bra 	$L__BB9_14;

$L__BB9_15:
	@%p17 bra 	$L__BB9_17;

	cvta.to.global.u64 	%rd18, %rd7;
	shl.b64 	%rd19, %rd1, 2;
	add.s64 	%rd20, %rd18, %rd19;
	mov.u32 	%r67, 1065353216;
	st.global.u32 	[%rd20], %r67;

$L__BB9_17:
	ret;

}
	// .globl	alma_ms1p_tiled_f32_tx128_ty2
.visible .entry alma_ms1p_tiled_f32_tx128_ty2(
	.param .u64 alma_ms1p_tiled_f32_tx128_ty2_param_0,
	.param .u64 alma_ms1p_tiled_f32_tx128_ty2_param_1,
	.param .u32 alma_ms1p_tiled_f32_tx128_ty2_param_2,
	.param .f32 alma_ms1p_tiled_f32_tx128_ty2_param_3,
	.param .u32 alma_ms1p_tiled_f32_tx128_ty2_param_4,
	.param .u32 alma_ms1p_tiled_f32_tx128_ty2_param_5,
	.param .u64 alma_ms1p_tiled_f32_tx128_ty2_param_6,
	.param .u64 alma_ms1p_tiled_f32_tx128_ty2_param_7
)
{
	.reg .pred 	%p<29>;
	.reg .f32 	%f<40>;
	.reg .b32 	%r<132>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd7, [alma_ms1p_tiled_f32_tx128_ty2_param_0];
	ld.param.u64 	%rd8, [alma_ms1p_tiled_f32_tx128_ty2_param_1];
	ld.param.u32 	%r59, [alma_ms1p_tiled_f32_tx128_ty2_param_2];
	ld.param.u32 	%r60, [alma_ms1p_tiled_f32_tx128_ty2_param_4];
	ld.param.u32 	%r61, [alma_ms1p_tiled_f32_tx128_ty2_param_5];
	ld.param.u64 	%rd9, [alma_ms1p_tiled_f32_tx128_ty2_param_6];
	ld.param.u64 	%rd10, [alma_ms1p_tiled_f32_tx128_ty2_param_7];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r62, %ctaid.x;
	shl.b32 	%r1, %r62, 7;
	mov.u32 	%r63, %ctaid.y;
	shl.b32 	%r2, %r63, 1;
	setp.ge.s32 	%p1, %r1, %r61;
	setp.ge.s32 	%p2, %r2, %r60;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB10_31;

	add.s32 	%r3, %r59, 127;
	shl.b32 	%r64, %r59, 2;
	add.s32 	%r65, %r64, 15;
	and.b32  	%r4, %r65, -16;
	and.b64  	%rd11, %rd8, 15;
	setp.eq.s64 	%p4, %rd11, 0;
	@%p4 bra 	$L__BB10_5;

	mov.u32 	%r66, %tid.y;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r67, %tid.x;
	mad.lo.s32 	%r119, %r66, %r5, %r67;
	setp.ge.s32 	%p5, %r119, %r59;
	@%p5 bra 	$L__BB10_11;

	mov.u32 	%r68, %ntid.y;
	mul.lo.s32 	%r7, %r5, %r68;

$L__BB10_4:
	mul.wide.s32 	%rd12, %r119, 4;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.f32 	%f10, [%rd13];
	shl.b32 	%r69, %r119, 2;
	mov.u32 	%r70, shraw;
	add.s32 	%r71, %r70, %r69;
	st.shared.f32 	[%r71], %f10;
	add.s32 	%r119, %r119, %r7;
	setp.lt.s32 	%p6, %r119, %r59;
	@%p6 bra 	$L__BB10_4;
	bra.uni 	$L__BB10_11;

$L__BB10_5:
	shr.s32 	%r10, %r59, 2;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.y;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r120, %r12, %r11, %r13;
	setp.ge.s32 	%p7, %r120, %r10;
	@%p7 bra 	$L__BB10_8;

	mov.u32 	%r72, %ntid.y;
	mul.lo.s32 	%r15, %r11, %r72;

$L__BB10_7:
	mul.wide.s32 	%rd14, %r120, 16;
	add.s64 	%rd15, %rd1, %rd14;
	shl.b32 	%r73, %r120, 4;
	mov.u32 	%r74, shraw;
	add.s32 	%r75, %r74, %r73;
	ld.global.nc.v4.u32 	{%r76, %r77, %r78, %r79}, [%rd15];
	st.shared.v4.u32 	[%r75], {%r76, %r77, %r78, %r79};
	add.s32 	%r120, %r120, %r15;
	setp.lt.s32 	%p8, %r120, %r10;
	@%p8 bra 	$L__BB10_7;

$L__BB10_8:
	or.b32  	%r84, %r13, %r12;
	setp.ne.s32 	%p9, %r84, 0;
	and.b32  	%r18, %r59, 3;
	setp.eq.s32 	%p10, %r18, 0;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB10_11;

	and.b32  	%r87, %r64, -16;
	mov.u32 	%r88, shraw;
	add.s32 	%r121, %r88, %r87;
	mul.wide.s32 	%rd16, %r59, 4;
	and.b64  	%rd17, %rd16, -16;
	add.s64 	%rd25, %rd1, %rd17;
	mov.u32 	%r122, 0;

$L__BB10_10:
	ld.global.nc.f32 	%f11, [%rd25];
	st.shared.f32 	[%r121], %f11;
	add.s32 	%r121, %r121, 4;
	add.s64 	%rd25, %rd25, 4;
	add.s32 	%r122, %r122, 1;
	setp.lt.u32 	%p12, %r122, %r18;
	@%p12 bra 	$L__BB10_10;

$L__BB10_11:
	mov.u32 	%r24, %tid.x;
	bar.sync 	0;
	setp.lt.s32 	%p13, %r24, %r3;
	@%p13 bra 	$L__BB10_12;
	bra.uni 	$L__BB10_20;

$L__BB10_12:
	add.s32 	%r89, %r1, 1;
	sub.s32 	%r25, %r89, %r59;
	mov.u32 	%r26, %tid.y;
	add.s32 	%r27, %r2, %r26;
	mov.u32 	%r28, %ntid.x;
	shl.b32 	%r29, %r3, 1;
	mov.u32 	%r90, shraw;
	add.s32 	%r30, %r90, %r4;
	cvta.to.global.u64 	%rd5, %rd7;
	mov.u32 	%r123, %r24;

$L__BB10_13:
	add.s32 	%r32, %r25, %r123;
	setp.gt.s32 	%p14, %r32, -1;
	setp.lt.s32 	%p15, %r32, %r61;
	and.pred  	%p16, %p14, %p15;
	shl.b32 	%r91, %r123, 1;
	add.s32 	%r92, %r91, %r26;
	shl.b32 	%r93, %r92, 2;
	add.s32 	%r33, %r30, %r93;
	@%p16 bra 	$L__BB10_16;
	bra.uni 	$L__BB10_14;

$L__BB10_16:
	setp.ge.s32 	%p18, %r27, %r60;
	mov.f32 	%f34, 0f00000000;
	@%p18 bra 	$L__BB10_18;

	mad.lo.s32 	%r97, %r32, %r60, %r27;
	mul.wide.s32 	%rd18, %r97, 4;
	add.s64 	%rd19, %rd5, %rd18;
	ld.global.nc.f32 	%f34, [%rd19];

$L__BB10_18:
	st.shared.f32 	[%r33], %f34;
	bra.uni 	$L__BB10_19;

$L__BB10_14:
	setp.ge.s32 	%p17, %r92, %r29;
	@%p17 bra 	$L__BB10_19;

	mov.u32 	%r96, 0;
	st.shared.u32 	[%r33], %r96;

$L__BB10_19:
	add.s32 	%r123, %r123, %r28;
	setp.lt.s32 	%p19, %r123, %r3;
	@%p19 bra 	$L__BB10_13;

$L__BB10_20:
	mov.u32 	%r35, %tid.y;
	add.s32 	%r36, %r2, %r35;
	bar.sync 	0;
	add.s32 	%r37, %r1, %r24;
	setp.ge.s32 	%p20, %r36, %r60;
	setp.ge.s32 	%p21, %r37, %r61;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	$L__BB10_31;

	cvta.to.global.u64 	%rd20, %rd9;
	mul.wide.s32 	%rd21, %r36, 4;
	add.s64 	%rd22, %rd20, %rd21;
	add.s32 	%r38, %r59, -1;
	ld.global.nc.u32 	%r100, [%rd22];
	add.s32 	%r101, %r38, %r100;
	mad.lo.s32 	%r102, %r37, %r60, %r36;
	setp.lt.s32 	%p23, %r37, %r101;
	cvta.to.global.u64 	%rd23, %rd10;
	mul.wide.s32 	%rd24, %r102, 4;
	add.s64 	%rd6, %rd23, %rd24;
	@%p23 bra 	$L__BB10_30;
	bra.uni 	$L__BB10_22;

$L__BB10_30:
	mov.u32 	%r118, 2143289344;
	st.global.u32 	[%rd6], %r118;
	bra.uni 	$L__BB10_31;

$L__BB10_22:
	setp.lt.s32 	%p24, %r59, 1;
	mov.f32 	%f39, 0f00000000;
	@%p24 bra 	$L__BB10_29;

	and.b32  	%r131, %r59, 3;
	setp.lt.u32 	%p25, %r38, 3;
	mov.f32 	%f39, 0f00000000;
	mov.u32 	%r128, 0;
	@%p25 bra 	$L__BB10_26;

	sub.s32 	%r127, %r59, %r131;
	shl.b32 	%r106, %r24, 3;
	add.s32 	%r107, %r4, %r106;
	shl.b32 	%r108, %r35, 2;
	add.s32 	%r109, %r107, %r108;
	mov.u32 	%r125, shraw;
	add.s32 	%r110, %r125, %r109;
	add.s32 	%r124, %r110, 16;
	mov.f32 	%f39, 0f00000000;
	mov.u32 	%r128, 0;

$L__BB10_25:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f17, %f18, %f19, %f20}, [%r125];
	ld.shared.f32 	%f25, [%r124+-16];
	fma.rn.ftz.f32 	%f26, %f25, %f17, %f39;
	ld.shared.f32 	%f27, [%r124+-8];
	fma.rn.ftz.f32 	%f28, %f27, %f18, %f26;
	ld.shared.f32 	%f29, [%r124];
	fma.rn.ftz.f32 	%f30, %f29, %f19, %f28;
	ld.shared.f32 	%f31, [%r124+8];
	fma.rn.ftz.f32 	%f39, %f31, %f20, %f30;
	add.s32 	%r128, %r128, 4;
	add.s32 	%r125, %r125, 16;
	add.s32 	%r124, %r124, 32;
	add.s32 	%r127, %r127, -4;
	setp.ne.s32 	%p26, %r127, 0;
	@%p26 bra 	$L__BB10_25;

$L__BB10_26:
	setp.eq.s32 	%p27, %r131, 0;
	@%p27 bra 	$L__BB10_29;

	shl.b32 	%r111, %r128, 2;
	mov.u32 	%r112, shraw;
	add.s32 	%r130, %r112, %r111;
	add.s32 	%r113, %r24, %r128;
	shl.b32 	%r114, %r113, 3;
	add.s32 	%r115, %r4, %r114;
	shl.b32 	%r116, %r35, 2;
	add.s32 	%r117, %r115, %r116;
	add.s32 	%r129, %r112, %r117;

$L__BB10_28:
	.pragma "nounroll";
	ld.shared.f32 	%f32, [%r130];
	ld.shared.f32 	%f33, [%r129];
	fma.rn.ftz.f32 	%f39, %f33, %f32, %f39;
	add.s32 	%r130, %r130, 4;
	add.s32 	%r129, %r129, 8;
	add.s32 	%r131, %r131, -1;
	setp.ne.s32 	%p28, %r131, 0;
	@%p28 bra 	$L__BB10_28;

$L__BB10_29:
	st.global.f32 	[%rd6], %f39;

$L__BB10_31:
	ret;

}
	// .globl	alma_ms1p_tiled_f32_tx128_ty4
.visible .entry alma_ms1p_tiled_f32_tx128_ty4(
	.param .u64 alma_ms1p_tiled_f32_tx128_ty4_param_0,
	.param .u64 alma_ms1p_tiled_f32_tx128_ty4_param_1,
	.param .u32 alma_ms1p_tiled_f32_tx128_ty4_param_2,
	.param .f32 alma_ms1p_tiled_f32_tx128_ty4_param_3,
	.param .u32 alma_ms1p_tiled_f32_tx128_ty4_param_4,
	.param .u32 alma_ms1p_tiled_f32_tx128_ty4_param_5,
	.param .u64 alma_ms1p_tiled_f32_tx128_ty4_param_6,
	.param .u64 alma_ms1p_tiled_f32_tx128_ty4_param_7
)
{
	.reg .pred 	%p<30>;
	.reg .f32 	%f<48>;
	.reg .b32 	%r<134>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd10, [alma_ms1p_tiled_f32_tx128_ty4_param_0];
	ld.param.u64 	%rd7, [alma_ms1p_tiled_f32_tx128_ty4_param_1];
	ld.param.u32 	%r60, [alma_ms1p_tiled_f32_tx128_ty4_param_2];
	ld.param.u32 	%r61, [alma_ms1p_tiled_f32_tx128_ty4_param_4];
	ld.param.u32 	%r62, [alma_ms1p_tiled_f32_tx128_ty4_param_5];
	ld.param.u64 	%rd8, [alma_ms1p_tiled_f32_tx128_ty4_param_6];
	ld.param.u64 	%rd9, [alma_ms1p_tiled_f32_tx128_ty4_param_7];
	cvta.to.global.u64 	%rd1, %rd10;
	cvta.to.global.u64 	%rd2, %rd7;
	mov.u32 	%r63, %ctaid.x;
	shl.b32 	%r1, %r63, 7;
	mov.u32 	%r64, %ctaid.y;
	shl.b32 	%r2, %r64, 2;
	setp.ge.s32 	%p1, %r1, %r62;
	setp.ge.s32 	%p2, %r2, %r61;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB11_33;

	add.s32 	%r3, %r60, 127;
	shl.b32 	%r65, %r60, 2;
	add.s32 	%r66, %r65, 15;
	and.b32  	%r4, %r66, -16;
	and.b64  	%rd11, %rd7, 15;
	setp.eq.s64 	%p4, %rd11, 0;
	@%p4 bra 	$L__BB11_5;

	mov.u32 	%r67, %tid.y;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r68, %tid.x;
	mad.lo.s32 	%r121, %r67, %r5, %r68;
	setp.ge.s32 	%p5, %r121, %r60;
	@%p5 bra 	$L__BB11_11;

	mov.u32 	%r69, %ntid.y;
	mul.lo.s32 	%r7, %r5, %r69;

$L__BB11_4:
	mul.wide.s32 	%rd12, %r121, 4;
	add.s64 	%rd13, %rd2, %rd12;
	ld.global.nc.f32 	%f10, [%rd13];
	shl.b32 	%r70, %r121, 2;
	mov.u32 	%r71, shraw;
	add.s32 	%r72, %r71, %r70;
	st.shared.f32 	[%r72], %f10;
	add.s32 	%r121, %r121, %r7;
	setp.lt.s32 	%p6, %r121, %r60;
	@%p6 bra 	$L__BB11_4;
	bra.uni 	$L__BB11_11;

$L__BB11_5:
	shr.s32 	%r10, %r60, 2;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.y;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r122, %r12, %r11, %r13;
	setp.ge.s32 	%p7, %r122, %r10;
	@%p7 bra 	$L__BB11_8;

	mov.u32 	%r73, %ntid.y;
	mul.lo.s32 	%r15, %r11, %r73;

$L__BB11_7:
	mul.wide.s32 	%rd14, %r122, 16;
	add.s64 	%rd15, %rd2, %rd14;
	shl.b32 	%r74, %r122, 4;
	mov.u32 	%r75, shraw;
	add.s32 	%r76, %r75, %r74;
	ld.global.nc.v4.u32 	{%r77, %r78, %r79, %r80}, [%rd15];
	st.shared.v4.u32 	[%r76], {%r77, %r78, %r79, %r80};
	add.s32 	%r122, %r122, %r15;
	setp.lt.s32 	%p8, %r122, %r10;
	@%p8 bra 	$L__BB11_7;

$L__BB11_8:
	or.b32  	%r85, %r13, %r12;
	setp.ne.s32 	%p9, %r85, 0;
	and.b32  	%r18, %r60, 3;
	setp.eq.s32 	%p10, %r18, 0;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB11_11;

	and.b32  	%r88, %r65, -16;
	mov.u32 	%r89, shraw;
	add.s32 	%r123, %r89, %r88;
	mul.wide.s32 	%rd16, %r60, 4;
	and.b64  	%rd17, %rd16, -16;
	add.s64 	%rd27, %rd2, %rd17;
	mov.u32 	%r124, 0;

$L__BB11_10:
	ld.global.nc.f32 	%f11, [%rd27];
	st.shared.f32 	[%r123], %f11;
	add.s32 	%r123, %r123, 4;
	add.s64 	%rd27, %rd27, 4;
	add.s32 	%r124, %r124, 1;
	setp.lt.u32 	%p12, %r124, %r18;
	@%p12 bra 	$L__BB11_10;

$L__BB11_11:
	mov.u32 	%r24, %tid.x;
	bar.sync 	0;
	setp.lt.s32 	%p13, %r24, %r3;
	@%p13 bra 	$L__BB11_12;
	bra.uni 	$L__BB11_22;

$L__BB11_12:
	mov.u32 	%r90, shraw;
	add.s32 	%r25, %r90, %r4;
	and.b32  	%r91, %r61, 3;
	add.s32 	%r92, %r1, 1;
	sub.s32 	%r26, %r92, %r60;
	mov.u32 	%r27, %tid.y;
	or.b32  	%r28, %r91, %r27;
	mov.u32 	%r29, %ntid.x;
	shl.b32 	%r30, %r3, 2;
	add.s32 	%r31, %r2, %r27;
	mov.u32 	%r125, %r24;

$L__BB11_13:
	add.s32 	%r33, %r26, %r125;
	setp.gt.s32 	%p14, %r33, -1;
	setp.lt.s32 	%p15, %r33, %r62;
	and.pred  	%p16, %p14, %p15;
	shl.b32 	%r93, %r125, 2;
	add.s32 	%r94, %r93, %r27;
	shl.b32 	%r95, %r94, 2;
	add.s32 	%r34, %r25, %r95;
	@%p16 bra 	$L__BB11_16;
	bra.uni 	$L__BB11_14;

$L__BB11_16:
	setp.eq.s32 	%p18, %r28, 0;
	@%p18 bra 	$L__BB11_20;

	setp.ge.s32 	%p19, %r31, %r61;
	mov.f32 	%f42, 0f00000000;
	@%p19 bra 	$L__BB11_19;

	mad.lo.s32 	%r99, %r33, %r61, %r31;
	mul.wide.s32 	%rd18, %r99, 4;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.nc.f32 	%f42, [%rd19];

$L__BB11_19:
	st.shared.f32 	[%r34], %f42;
	bra.uni 	$L__BB11_21;

$L__BB11_14:
	setp.ge.s32 	%p17, %r94, %r30;
	@%p17 bra 	$L__BB11_21;

	mov.u32 	%r98, 0;
	st.shared.u32 	[%r34], %r98;
	bra.uni 	$L__BB11_21;

$L__BB11_20:
	mad.lo.s32 	%r100, %r33, %r61, %r2;
	mul.wide.s32 	%rd20, %r100, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.v4.f32 	{%f13, %f14, %f15, %f16}, [%rd21];
	shl.b32 	%r101, %r125, 4;
	add.s32 	%r102, %r25, %r101;
	st.shared.v4.f32 	[%r102], {%f13, %f14, %f15, %f16};

$L__BB11_21:
	add.s32 	%r125, %r125, %r29;
	setp.lt.s32 	%p20, %r125, %r3;
	@%p20 bra 	$L__BB11_13;

$L__BB11_22:
	mov.u32 	%r36, %tid.y;
	add.s32 	%r37, %r2, %r36;
	bar.sync 	0;
	setp.ge.s32 	%p21, %r37, %r61;
	add.s32 	%r38, %r1, %r24;
	setp.ge.s32 	%p22, %r38, %r62;
	or.pred  	%p23, %p21, %p22;
	@%p23 bra 	$L__BB11_33;

	cvta.to.global.u64 	%rd22, %rd8;
	mul.wide.s32 	%rd23, %r37, 4;
	add.s64 	%rd24, %rd22, %rd23;
	add.s32 	%r39, %r60, -1;
	ld.global.nc.u32 	%r103, [%rd24];
	add.s32 	%r104, %r39, %r103;
	mad.lo.s32 	%r105, %r38, %r61, %r37;
	setp.lt.s32 	%p24, %r38, %r104;
	cvta.to.global.u64 	%rd25, %rd9;
	mul.wide.s32 	%rd26, %r105, 4;
	add.s64 	%rd6, %rd25, %rd26;
	@%p24 bra 	$L__BB11_32;
	bra.uni 	$L__BB11_24;

$L__BB11_32:
	mov.u32 	%r120, 2143289344;
	st.global.u32 	[%rd6], %r120;
	bra.uni 	$L__BB11_33;

$L__BB11_24:
	setp.lt.s32 	%p25, %r60, 1;
	mov.f32 	%f47, 0f00000000;
	@%p25 bra 	$L__BB11_31;

	and.b32  	%r133, %r60, 3;
	setp.lt.u32 	%p26, %r39, 3;
	mov.f32 	%f47, 0f00000000;
	mov.u32 	%r130, 0;
	@%p26 bra 	$L__BB11_28;

	sub.s32 	%r129, %r60, %r133;
	shl.b32 	%r109, %r24, 4;
	add.s32 	%r110, %r4, %r109;
	shl.b32 	%r111, %r36, 2;
	add.s32 	%r112, %r110, %r111;
	mov.u32 	%r127, shraw;
	add.s32 	%r126, %r127, %r112;
	mov.f32 	%f47, 0f00000000;
	mov.u32 	%r130, 0;

$L__BB11_27:
	.pragma "nounroll";
	ld.shared.v4.f32 	{%f25, %f26, %f27, %f28}, [%r127];
	ld.shared.f32 	%f33, [%r126];
	fma.rn.ftz.f32 	%f34, %f33, %f25, %f47;
	ld.shared.f32 	%f35, [%r126+16];
	fma.rn.ftz.f32 	%f36, %f35, %f26, %f34;
	ld.shared.f32 	%f37, [%r126+32];
	fma.rn.ftz.f32 	%f38, %f37, %f27, %f36;
	ld.shared.f32 	%f39, [%r126+48];
	fma.rn.ftz.f32 	%f47, %f39, %f28, %f38;
	add.s32 	%r130, %r130, 4;
	add.s32 	%r127, %r127, 16;
	add.s32 	%r126, %r126, 64;
	add.s32 	%r129, %r129, -4;
	setp.ne.s32 	%p27, %r129, 0;
	@%p27 bra 	$L__BB11_27;

$L__BB11_28:
	setp.eq.s32 	%p28, %r133, 0;
	@%p28 bra 	$L__BB11_31;

	shl.b32 	%r113, %r130, 2;
	mov.u32 	%r114, shraw;
	add.s32 	%r132, %r114, %r113;
	add.s32 	%r115, %r24, %r130;
	shl.b32 	%r116, %r115, 4;
	add.s32 	%r117, %r4, %r116;
	shl.b32 	%r118, %r36, 2;
	add.s32 	%r119, %r117, %r118;
	add.s32 	%r131, %r114, %r119;

$L__BB11_30:
	.pragma "nounroll";
	ld.shared.f32 	%f40, [%r132];
	ld.shared.f32 	%f41, [%r131];
	fma.rn.ftz.f32 	%f47, %f41, %f40, %f47;
	add.s32 	%r132, %r132, 4;
	add.s32 	%r131, %r131, 16;
	add.s32 	%r133, %r133, -1;
	setp.ne.s32 	%p29, %r133, 0;
	@%p29 bra 	$L__BB11_30;

$L__BB11_31:
	st.global.f32 	[%rd6], %f47;

$L__BB11_33:
	ret;

}

