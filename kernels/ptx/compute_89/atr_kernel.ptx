//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_89
.address_size 64

	// .globl	tr_from_hlc_f32
.shared .align 4 .b8 _ZZN44_INTERNAL_4d743621_13_atr_kernel_cu_e49a8fd216block_reduce_sumEfE9warp_sums[128];

.visible .entry tr_from_hlc_f32(
	.param .u64 tr_from_hlc_f32_param_0,
	.param .u64 tr_from_hlc_f32_param_1,
	.param .u64 tr_from_hlc_f32_param_2,
	.param .u32 tr_from_hlc_f32_param_3,
	.param .u32 tr_from_hlc_f32_param_4,
	.param .u64 tr_from_hlc_f32_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd6, [tr_from_hlc_f32_param_0];
	ld.param.u64 	%rd7, [tr_from_hlc_f32_param_1];
	ld.param.u64 	%rd8, [tr_from_hlc_f32_param_2];
	ld.param.u32 	%r7, [tr_from_hlc_f32_param_3];
	ld.param.u32 	%r8, [tr_from_hlc_f32_param_4];
	ld.param.u64 	%rd9, [tr_from_hlc_f32_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r15, %r9, %r1, %r10;
	setp.ge.s32 	%p1, %r15, %r7;
	@%p1 bra 	$L__BB0_7;

	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd8;
	cvta.to.global.u64 	%rd4, %rd9;

$L__BB0_2:
	cvt.s64.s32 	%rd5, %r15;
	setp.lt.s32 	%p2, %r15, %r8;
	mov.f32 	%f17, 0f00000000;
	@%p2 bra 	$L__BB0_6;

	cvt.u32.u64 	%r5, %rd5;
	shl.b64 	%rd10, %rd5, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.f32 	%f1, [%rd11];
	add.s64 	%rd12, %rd2, %rd10;
	ld.global.nc.f32 	%f2, [%rd12];
	setp.eq.s32 	%p3, %r5, %r8;
	mov.f32 	%f16, 0f00000000;
	@%p3 bra 	$L__BB0_5;

	add.s32 	%r13, %r5, -1;
	mul.wide.s32 	%rd13, %r13, 4;
	add.s64 	%rd14, %rd3, %rd13;
	ld.global.nc.f32 	%f16, [%rd14];

$L__BB0_5:
	sub.ftz.f32 	%f9, %f1, %f16;
	abs.ftz.f32 	%f10, %f9;
	sub.ftz.f32 	%f11, %f2, %f16;
	abs.ftz.f32 	%f12, %f11;
	max.ftz.f32 	%f13, %f10, %f12;
	sub.ftz.f32 	%f14, %f1, %f2;
	max.ftz.f32 	%f15, %f14, %f13;
	selp.f32 	%f17, %f14, %f15, %p3;

$L__BB0_6:
	shl.b64 	%rd15, %rd5, 2;
	add.s64 	%rd16, %rd4, %rd15;
	st.global.f32 	[%rd16], %f17;
	cvt.u32.u64 	%r14, %rd5;
	add.s32 	%r15, %r14, %r3;
	setp.lt.s32 	%p5, %r15, %r7;
	@%p5 bra 	$L__BB0_2;

$L__BB0_7:
	ret;

}
	// .globl	exclusive_prefix_float2_from_tr
.visible .entry exclusive_prefix_float2_from_tr(
	.param .u64 exclusive_prefix_float2_from_tr_param_0,
	.param .u32 exclusive_prefix_float2_from_tr_param_1,
	.param .u64 exclusive_prefix_float2_from_tr_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<58>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd14, [exclusive_prefix_float2_from_tr_param_0];
	ld.param.u32 	%r10, [exclusive_prefix_float2_from_tr_param_1];
	ld.param.u64 	%rd13, [exclusive_prefix_float2_from_tr_param_2];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd18, %rd13;
	mov.u32 	%r11, %tid.x;
	mov.u32 	%r12, %ctaid.x;
	or.b32  	%r13, %r12, %r11;
	setp.ne.s32 	%p1, %r13, 0;
	@%p1 bra 	$L__BB1_8;
	bra.uni 	$L__BB1_1;

$L__BB1_8:
	ret;

$L__BB1_1:
	mov.f32 	%f54, 0f00000000;
	st.global.v2.f32 	[%rd18], {%f54, %f54};
	setp.lt.s32 	%p2, %r10, 1;
	@%p2 bra 	$L__BB1_8;

	add.s32 	%r15, %r10, -1;
	and.b32  	%r21, %r10, 3;
	setp.lt.u32 	%p3, %r15, 3;
	mov.u32 	%r20, 0;
	mov.f32 	%f55, %f54;
	@%p3 bra 	$L__BB1_5;

	sub.s32 	%r19, %r10, %r21;
	mov.f32 	%f54, 0f00000000;
	mov.u32 	%r20, 0;
	mov.u64 	%rd19, %rd1;
	mov.f32 	%f55, %f54;

$L__BB1_4:
	ld.global.nc.f32 	%f16, [%rd19];
	add.ftz.f32 	%f17, %f55, %f16;
	sub.ftz.f32 	%f18, %f17, %f55;
	sub.ftz.f32 	%f19, %f17, %f18;
	sub.ftz.f32 	%f20, %f55, %f19;
	sub.ftz.f32 	%f21, %f16, %f18;
	add.ftz.f32 	%f22, %f21, %f20;
	add.ftz.f32 	%f23, %f54, %f22;
	st.global.v2.f32 	[%rd18+8], {%f17, %f23};
	ld.global.nc.f32 	%f24, [%rd19+4];
	add.ftz.f32 	%f25, %f17, %f24;
	sub.ftz.f32 	%f26, %f25, %f17;
	sub.ftz.f32 	%f27, %f25, %f26;
	sub.ftz.f32 	%f28, %f17, %f27;
	sub.ftz.f32 	%f29, %f24, %f26;
	add.ftz.f32 	%f30, %f29, %f28;
	add.ftz.f32 	%f31, %f23, %f30;
	st.global.v2.f32 	[%rd18+16], {%f25, %f31};
	ld.global.nc.f32 	%f32, [%rd19+8];
	add.ftz.f32 	%f33, %f25, %f32;
	sub.ftz.f32 	%f34, %f33, %f25;
	sub.ftz.f32 	%f35, %f33, %f34;
	sub.ftz.f32 	%f36, %f25, %f35;
	sub.ftz.f32 	%f37, %f32, %f34;
	add.ftz.f32 	%f38, %f37, %f36;
	add.ftz.f32 	%f39, %f31, %f38;
	st.global.v2.f32 	[%rd18+24], {%f33, %f39};
	ld.global.nc.f32 	%f40, [%rd19+12];
	add.ftz.f32 	%f55, %f33, %f40;
	sub.ftz.f32 	%f41, %f55, %f33;
	sub.ftz.f32 	%f42, %f55, %f41;
	sub.ftz.f32 	%f43, %f33, %f42;
	sub.ftz.f32 	%f44, %f40, %f41;
	add.ftz.f32 	%f45, %f44, %f43;
	add.ftz.f32 	%f54, %f39, %f45;
	add.s32 	%r20, %r20, 4;
	add.s64 	%rd6, %rd18, 32;
	st.global.v2.f32 	[%rd18+32], {%f55, %f54};
	add.s64 	%rd19, %rd19, 16;
	add.s32 	%r19, %r19, -4;
	setp.ne.s32 	%p4, %r19, 0;
	mov.u64 	%rd18, %rd6;
	@%p4 bra 	$L__BB1_4;

$L__BB1_5:
	setp.eq.s32 	%p5, %r21, 0;
	@%p5 bra 	$L__BB1_8;

	add.s32 	%r17, %r20, 1;
	cvta.to.global.u64 	%rd15, %rd13;
	mul.wide.s32 	%rd16, %r17, 8;
	add.s64 	%rd21, %rd15, %rd16;
	mul.wide.u32 	%rd17, %r20, 4;
	add.s64 	%rd20, %rd1, %rd17;

$L__BB1_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f46, [%rd20];
	add.ftz.f32 	%f9, %f55, %f46;
	sub.ftz.f32 	%f47, %f9, %f55;
	sub.ftz.f32 	%f48, %f9, %f47;
	sub.ftz.f32 	%f49, %f55, %f48;
	sub.ftz.f32 	%f50, %f46, %f47;
	add.ftz.f32 	%f51, %f50, %f49;
	add.ftz.f32 	%f54, %f54, %f51;
	st.global.v2.f32 	[%rd21], {%f9, %f54};
	add.s64 	%rd21, %rd21, 8;
	add.s64 	%rd20, %rd20, 4;
	add.s32 	%r21, %r21, -1;
	setp.ne.s32 	%p6, %r21, 0;
	mov.f32 	%f55, %f9;
	@%p6 bra 	$L__BB1_7;
	bra.uni 	$L__BB1_8;

}
	// .globl	atr_batch_unified_f32
.visible .entry atr_batch_unified_f32(
	.param .u64 atr_batch_unified_f32_param_0,
	.param .u64 atr_batch_unified_f32_param_1,
	.param .u64 atr_batch_unified_f32_param_2,
	.param .u64 atr_batch_unified_f32_param_3,
	.param .u64 atr_batch_unified_f32_param_4,
	.param .u64 atr_batch_unified_f32_param_5,
	.param .u64 atr_batch_unified_f32_param_6,
	.param .u64 atr_batch_unified_f32_param_7,
	.param .u32 atr_batch_unified_f32_param_8,
	.param .u32 atr_batch_unified_f32_param_9,
	.param .u32 atr_batch_unified_f32_param_10,
	.param .u64 atr_batch_unified_f32_param_11
)
{
	.reg .pred 	%p<43>;
	.reg .f32 	%f<187>;
	.reg .b32 	%r<107>;
	.reg .b64 	%rd<99>;


	ld.param.u64 	%rd47, [atr_batch_unified_f32_param_0];
	ld.param.u64 	%rd48, [atr_batch_unified_f32_param_1];
	ld.param.u64 	%rd49, [atr_batch_unified_f32_param_2];
	ld.param.u64 	%rd42, [atr_batch_unified_f32_param_3];
	ld.param.u64 	%rd43, [atr_batch_unified_f32_param_4];
	ld.param.u64 	%rd44, [atr_batch_unified_f32_param_5];
	ld.param.u64 	%rd45, [atr_batch_unified_f32_param_6];
	ld.param.u64 	%rd46, [atr_batch_unified_f32_param_7];
	ld.param.u32 	%r56, [atr_batch_unified_f32_param_8];
	ld.param.u32 	%r57, [atr_batch_unified_f32_param_9];
	ld.param.u32 	%r58, [atr_batch_unified_f32_param_10];
	ld.param.u64 	%rd50, [atr_batch_unified_f32_param_11];
	cvta.to.global.u64 	%rd1, %rd49;
	cvta.to.global.u64 	%rd2, %rd48;
	cvta.to.global.u64 	%rd3, %rd47;
	cvta.to.global.u64 	%rd4, %rd42;
	cvta.to.global.u64 	%rd98, %rd50;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r58;
	@%p1 bra 	$L__BB2_46;

	cvta.to.global.u64 	%rd51, %rd44;
	mul.wide.s32 	%rd52, %r1, 4;
	add.s64 	%rd53, %rd51, %rd52;
	cvta.to.global.u64 	%rd54, %rd45;
	add.s64 	%rd55, %rd54, %rd52;
	ld.global.nc.f32 	%f1, [%rd55];
	cvta.to.global.u64 	%rd56, %rd46;
	add.s64 	%rd57, %rd56, %rd52;
	ld.global.nc.u32 	%r102, [%rd57];
	ld.global.nc.u32 	%r3, [%rd53];
	setp.lt.s32 	%p2, %r3, 1;
	setp.ge.s32 	%p3, %r102, %r56;
	or.pred  	%p4, %p2, %p3;
	setp.ge.s32 	%p5, %r57, %r56;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB2_46;

	mul.lo.s32 	%r4, %r1, %r56;
	mov.u32 	%r5, %tid.x;
	setp.ge.s32 	%p7, %r5, %r102;
	@%p7 bra 	$L__BB2_5;

	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r89, %r5;

$L__BB2_4:
	add.s32 	%r59, %r89, %r4;
	mul.wide.s32 	%rd58, %r59, 4;
	add.s64 	%rd59, %rd98, %rd58;
	mov.u32 	%r60, 2143289344;
	st.global.u32 	[%rd59], %r60;
	add.s32 	%r89, %r89, %r6;
	setp.lt.s32 	%p8, %r89, %r102;
	@%p8 bra 	$L__BB2_4;

$L__BB2_5:
	bar.sync 	0;
	setp.eq.s64 	%p9, %rd43, 0;
	@%p9 bra 	$L__BB2_9;

	setp.ne.s32 	%p10, %r5, 0;
	mov.f32 	%f180, 0f00000000;
	@%p10 bra 	$L__BB2_8;

	cvta.to.global.u64 	%rd60, %rd43;
	add.s32 	%r61, %r102, 1;
	mul.wide.s32 	%rd61, %r61, 8;
	add.s64 	%rd62, %rd60, %rd61;
	ld.global.nc.v2.f32 	{%f35, %f36}, [%rd62];
	mul.wide.s32 	%rd63, %r57, 8;
	add.s64 	%rd64, %rd60, %rd63;
	ld.global.nc.v2.f32 	{%f39, %f40}, [%rd64];
	add.ftz.f32 	%f43, %f35, 0f00000000;
	sub.ftz.f32 	%f44, %f43, %f43;
	add.ftz.f32 	%f45, %f44, 0f00000000;
	sub.ftz.f32 	%f46, %f35, %f43;
	add.ftz.f32 	%f47, %f46, %f45;
	add.ftz.f32 	%f48, %f47, 0f00000000;
	add.ftz.f32 	%f49, %f36, %f43;
	sub.ftz.f32 	%f50, %f49, %f43;
	sub.ftz.f32 	%f51, %f50, %f49;
	add.ftz.f32 	%f52, %f43, %f51;
	sub.ftz.f32 	%f53, %f36, %f50;
	add.ftz.f32 	%f54, %f53, %f52;
	add.ftz.f32 	%f55, %f48, %f54;
	neg.ftz.f32 	%f56, %f39;
	sub.ftz.f32 	%f57, %f49, %f39;
	sub.ftz.f32 	%f58, %f57, %f49;
	sub.ftz.f32 	%f59, %f57, %f58;
	sub.ftz.f32 	%f60, %f49, %f59;
	sub.ftz.f32 	%f61, %f56, %f58;
	add.ftz.f32 	%f62, %f61, %f60;
	add.ftz.f32 	%f63, %f55, %f62;
	neg.ftz.f32 	%f64, %f40;
	sub.ftz.f32 	%f65, %f57, %f40;
	sub.ftz.f32 	%f66, %f65, %f57;
	sub.ftz.f32 	%f67, %f65, %f66;
	sub.ftz.f32 	%f68, %f57, %f67;
	sub.ftz.f32 	%f69, %f64, %f66;
	add.ftz.f32 	%f70, %f69, %f68;
	add.ftz.f32 	%f71, %f63, %f70;
	add.ftz.f32 	%f72, %f65, %f71;
	cvt.rn.f32.s32 	%f73, %r3;
	div.approx.ftz.f32 	%f180, %f72, %f73;

$L__BB2_8:
	bar.sync 	0;
	bra.uni 	$L__BB2_30;

$L__BB2_9:
	setp.ge.s32 	%p11, %r5, %r3;
	mov.f32 	%f173, 0f00000000;
	@%p11 bra 	$L__BB2_17;

	setp.eq.s64 	%p12, %rd42, 0;
	mov.u32 	%r9, %ntid.x;
	@%p12 bra 	$L__BB2_13;

	mov.f32 	%f173, 0f00000000;
	mov.u32 	%r90, %r5;

$L__BB2_12:
	add.s32 	%r62, %r90, %r57;
	mul.wide.s32 	%rd65, %r62, 4;
	add.s64 	%rd66, %rd4, %rd65;
	ld.global.nc.f32 	%f76, [%rd66];
	add.ftz.f32 	%f173, %f173, %f76;
	add.s32 	%r90, %r90, %r9;
	setp.lt.s32 	%p13, %r90, %r3;
	@%p13 bra 	$L__BB2_12;
	bra.uni 	$L__BB2_17;

$L__BB2_13:
	mov.f32 	%f77, 0f00000000;
	mov.u32 	%r91, %r5;
	mov.f32 	%f173, %f77;

$L__BB2_14:
	add.s32 	%r13, %r91, %r57;
	mul.wide.s32 	%rd67, %r13, 4;
	add.s64 	%rd68, %rd3, %rd67;
	ld.global.nc.f32 	%f7, [%rd68];
	add.s64 	%rd69, %rd2, %rd67;
	ld.global.nc.f32 	%f8, [%rd69];
	setp.eq.s32 	%p14, %r91, 0;
	mov.f32 	%f172, %f77;
	@%p14 bra 	$L__BB2_16;

	add.s32 	%r63, %r13, -1;
	mul.wide.s32 	%rd70, %r63, 4;
	add.s64 	%rd71, %rd1, %rd70;
	ld.global.nc.f32 	%f172, [%rd71];

$L__BB2_16:
	sub.ftz.f32 	%f79, %f7, %f172;
	abs.ftz.f32 	%f80, %f79;
	sub.ftz.f32 	%f81, %f8, %f172;
	abs.ftz.f32 	%f82, %f81;
	max.ftz.f32 	%f83, %f80, %f82;
	sub.ftz.f32 	%f84, %f7, %f8;
	max.ftz.f32 	%f85, %f84, %f83;
	selp.f32 	%f86, %f84, %f85, %p14;
	add.ftz.f32 	%f173, %f173, %f86;
	add.s32 	%r91, %r91, %r9;
	setp.lt.s32 	%p16, %r91, %r3;
	@%p16 bra 	$L__BB2_14;

$L__BB2_17:
	mov.u32 	%r93, WARP_SZ;
	add.s32 	%r16, %r93, -1;
	and.b32  	%r17, %r16, %r5;
	shr.u32 	%r18, %r5, 5;
	// begin inline asm
	activemask.b32 %r64;
	// end inline asm
	setp.lt.s32 	%p17, %r93, 2;
	@%p17 bra 	$L__BB2_20;

	mov.u32 	%r92, %r93;

$L__BB2_19:
	mov.b32 	%r65, %f173;
	shr.s32 	%r21, %r92, 1;
	mov.u32 	%r66, 31;
	shfl.sync.down.b32 	%r67|%p18, %r65, %r21, %r66, %r64;
	mov.b32 	%f87, %r67;
	add.ftz.f32 	%f173, %f173, %f87;
	setp.gt.s32 	%p19, %r92, 3;
	mov.u32 	%r92, %r21;
	@%p19 bra 	$L__BB2_19;

$L__BB2_20:
	setp.ne.s32 	%p20, %r17, 0;
	@%p20 bra 	$L__BB2_22;

	shl.b32 	%r68, %r18, 2;
	mov.u32 	%r69, _ZZN44_INTERNAL_4d743621_13_atr_kernel_cu_e49a8fd216block_reduce_sumEfE9warp_sums;
	add.s32 	%r70, %r69, %r68;
	st.shared.f32 	[%r70], %f173;

$L__BB2_22:
	bar.sync 	0;
	setp.ne.s32 	%p21, %r18, 0;
	mov.f32 	%f180, 0f00000000;
	mov.f32 	%f178, %f180;
	@%p21 bra 	$L__BB2_27;

	mov.u32 	%r71, %ntid.x;
	add.s32 	%r72, %r16, %r71;
	shr.u32 	%r73, %r72, 5;
	setp.ge.s32 	%p22, %r17, %r73;
	mov.f32 	%f178, 0f00000000;
	@%p22 bra 	$L__BB2_25;

	shl.b32 	%r74, %r17, 2;
	mov.u32 	%r75, _ZZN44_INTERNAL_4d743621_13_atr_kernel_cu_e49a8fd216block_reduce_sumEfE9warp_sums;
	add.s32 	%r76, %r75, %r74;
	ld.shared.f32 	%f178, [%r76];

$L__BB2_25:
	// begin inline asm
	activemask.b32 %r77;
	// end inline asm
	@%p17 bra 	$L__BB2_27;

$L__BB2_26:
	mov.b32 	%r78, %f178;
	shr.s32 	%r24, %r93, 1;
	mov.u32 	%r79, 31;
	shfl.sync.down.b32 	%r80|%p24, %r78, %r24, %r79, %r77;
	mov.b32 	%f90, %r80;
	add.ftz.f32 	%f178, %f178, %f90;
	setp.gt.s32 	%p25, %r93, 3;
	mov.u32 	%r93, %r24;
	@%p25 bra 	$L__BB2_26;

$L__BB2_27:
	setp.ne.s32 	%p26, %r5, 0;
	@%p26 bra 	$L__BB2_29;

	cvt.rn.f32.s32 	%f92, %r3;
	div.approx.ftz.f32 	%f180, %f178, %f92;

$L__BB2_29:
	bar.sync 	0;

$L__BB2_30:
	setp.ne.s32 	%p27, %r5, 0;
	@%p27 bra 	$L__BB2_46;

	add.s32 	%r25, %r102, %r4;
	mul.wide.s32 	%rd72, %r25, 4;
	add.s64 	%rd73, %rd98, %rd72;
	st.global.f32 	[%rd73], %f180;
	add.s32 	%r96, %r102, 1;
	setp.ge.s32 	%p28, %r96, %r56;
	@%p28 bra 	$L__BB2_46;

	setp.eq.s64 	%p29, %rd42, 0;
	not.b32 	%r81, %r102;
	add.s32 	%r82, %r81, %r56;
	add.s32 	%r83, %r56, -2;
	sub.s32 	%r27, %r83, %r102;
	and.b32  	%r95, %r82, 3;
	@%p29 bra 	$L__BB2_39;

	setp.eq.s32 	%p30, %r95, 0;
	@%p30 bra 	$L__BB2_36;

	add.s32 	%r84, %r25, 1;
	mul.wide.s32 	%rd74, %r84, 4;
	add.s64 	%rd88, %rd98, %rd74;
	mul.wide.s32 	%rd75, %r96, 4;
	add.s64 	%rd87, %rd4, %rd75;

$L__BB2_35:
	.pragma "nounroll";
	ld.global.nc.f32 	%f93, [%rd87];
	sub.ftz.f32 	%f94, %f93, %f180;
	fma.rn.ftz.f32 	%f180, %f94, %f1, %f180;
	st.global.f32 	[%rd88], %f180;
	add.s32 	%r96, %r96, 1;
	add.s64 	%rd88, %rd88, 4;
	add.s64 	%rd87, %rd87, 4;
	add.s32 	%r95, %r95, -1;
	setp.ne.s32 	%p31, %r95, 0;
	@%p31 bra 	$L__BB2_35;

$L__BB2_36:
	setp.lt.u32 	%p32, %r27, 3;
	@%p32 bra 	$L__BB2_46;

	add.s32 	%r85, %r96, %r4;
	mul.wide.s32 	%rd76, %r85, 4;
	add.s64 	%rd90, %rd98, %rd76;
	mul.wide.s32 	%rd77, %r96, 4;
	add.s64 	%rd78, %rd4, %rd77;
	add.s64 	%rd89, %rd78, 8;

$L__BB2_38:
	ld.global.nc.f32 	%f95, [%rd89+-8];
	sub.ftz.f32 	%f96, %f95, %f180;
	fma.rn.ftz.f32 	%f97, %f96, %f1, %f180;
	st.global.f32 	[%rd90], %f97;
	ld.global.nc.f32 	%f98, [%rd89+-4];
	sub.ftz.f32 	%f99, %f98, %f97;
	fma.rn.ftz.f32 	%f100, %f99, %f1, %f97;
	st.global.f32 	[%rd90+4], %f100;
	ld.global.nc.f32 	%f101, [%rd89];
	sub.ftz.f32 	%f102, %f101, %f100;
	fma.rn.ftz.f32 	%f103, %f102, %f1, %f100;
	st.global.f32 	[%rd90+8], %f103;
	ld.global.nc.f32 	%f104, [%rd89+4];
	sub.ftz.f32 	%f105, %f104, %f103;
	fma.rn.ftz.f32 	%f180, %f105, %f1, %f103;
	st.global.f32 	[%rd90+12], %f180;
	add.s64 	%rd90, %rd90, 16;
	add.s64 	%rd89, %rd89, 16;
	add.s32 	%r96, %r96, 4;
	setp.lt.s32 	%p33, %r96, %r56;
	@%p33 bra 	$L__BB2_38;
	bra.uni 	$L__BB2_46;

$L__BB2_39:
	setp.eq.s32 	%p34, %r95, 0;
	@%p34 bra 	$L__BB2_43;

	add.s32 	%r86, %r25, 1;
	mul.wide.s32 	%rd79, %r86, 4;
	add.s64 	%rd94, %rd98, %rd79;
	sub.s32 	%r98, %r102, %r57;
	mul.wide.s32 	%rd80, %r96, 4;
	add.s64 	%rd93, %rd2, %rd80;
	add.s64 	%rd92, %rd3, %rd80;
	mul.wide.s32 	%rd81, %r102, 4;
	add.s64 	%rd91, %rd1, %rd81;

$L__BB2_41:
	.pragma "nounroll";
	ld.global.nc.f32 	%f106, [%rd93];
	ld.global.nc.f32 	%f107, [%rd92];
	sub.ftz.f32 	%f108, %f107, %f106;
	ld.global.nc.f32 	%f109, [%rd91];
	sub.ftz.f32 	%f110, %f107, %f109;
	abs.ftz.f32 	%f111, %f110;
	sub.ftz.f32 	%f112, %f106, %f109;
	abs.ftz.f32 	%f113, %f112;
	max.ftz.f32 	%f114, %f111, %f113;
	max.ftz.f32 	%f115, %f108, %f114;
	setp.eq.s32 	%p35, %r98, -1;
	selp.f32 	%f116, %f108, %f115, %p35;
	sub.ftz.f32 	%f117, %f116, %f180;
	fma.rn.ftz.f32 	%f180, %f117, %f1, %f180;
	st.global.f32 	[%rd94], %f180;
	add.s32 	%r102, %r102, 1;
	add.s64 	%rd94, %rd94, 4;
	add.s32 	%r98, %r98, 1;
	add.s64 	%rd93, %rd93, 4;
	add.s64 	%rd92, %rd92, 4;
	add.s64 	%rd91, %rd91, 4;
	add.s32 	%r95, %r95, -1;
	setp.ne.s32 	%p36, %r95, 0;
	@%p36 bra 	$L__BB2_41;

	add.s32 	%r96, %r102, 1;

$L__BB2_43:
	setp.lt.u32 	%p37, %r27, 3;
	@%p37 bra 	$L__BB2_46;

	add.s32 	%r87, %r96, %r4;
	add.s32 	%r88, %r57, -1;
	sub.s32 	%r104, %r88, %r96;
	sub.s32 	%r103, %r96, %r57;
	mul.wide.s32 	%rd30, %r87, 4;
	mul.wide.s32 	%rd82, %r96, 4;
	add.s64 	%rd83, %rd82, 8;
	add.s64 	%rd97, %rd3, %rd83;
	add.s64 	%rd96, %rd2, %rd83;
	add.s64 	%rd95, %rd1, %rd83;

$L__BB2_45:
	mul.wide.s32 	%rd84, %r102, 4;
	add.s64 	%rd85, %rd1, %rd84;
	ld.global.nc.f32 	%f118, [%rd96+-8];
	ld.global.nc.f32 	%f119, [%rd97+-8];
	sub.ftz.f32 	%f120, %f119, %f118;
	ld.global.nc.f32 	%f121, [%rd85];
	sub.ftz.f32 	%f122, %f119, %f121;
	abs.ftz.f32 	%f123, %f122;
	sub.ftz.f32 	%f124, %f118, %f121;
	abs.ftz.f32 	%f125, %f124;
	max.ftz.f32 	%f126, %f123, %f125;
	max.ftz.f32 	%f127, %f120, %f126;
	setp.eq.s32 	%p38, %r104, -1;
	selp.f32 	%f128, %f120, %f127, %p38;
	sub.ftz.f32 	%f129, %f128, %f180;
	fma.rn.ftz.f32 	%f130, %f129, %f1, %f180;
	add.s64 	%rd86, %rd98, %rd30;
	st.global.f32 	[%rd86], %f130;
	ld.global.nc.f32 	%f131, [%rd96+-4];
	ld.global.nc.f32 	%f132, [%rd97+-4];
	sub.ftz.f32 	%f133, %f132, %f131;
	ld.global.nc.f32 	%f134, [%rd95+-8];
	sub.ftz.f32 	%f135, %f132, %f134;
	abs.ftz.f32 	%f136, %f135;
	sub.ftz.f32 	%f137, %f131, %f134;
	abs.ftz.f32 	%f138, %f137;
	max.ftz.f32 	%f139, %f136, %f138;
	max.ftz.f32 	%f140, %f133, %f139;
	setp.eq.s32 	%p39, %r103, -1;
	selp.f32 	%f141, %f133, %f140, %p39;
	sub.ftz.f32 	%f142, %f141, %f130;
	fma.rn.ftz.f32 	%f143, %f142, %f1, %f130;
	st.global.f32 	[%rd86+4], %f143;
	ld.global.nc.f32 	%f144, [%rd96];
	ld.global.nc.f32 	%f145, [%rd97];
	sub.ftz.f32 	%f146, %f145, %f144;
	ld.global.nc.f32 	%f147, [%rd95+-4];
	sub.ftz.f32 	%f148, %f145, %f147;
	abs.ftz.f32 	%f149, %f148;
	sub.ftz.f32 	%f150, %f144, %f147;
	abs.ftz.f32 	%f151, %f150;
	max.ftz.f32 	%f152, %f149, %f151;
	max.ftz.f32 	%f153, %f146, %f152;
	setp.eq.s32 	%p40, %r104, 1;
	selp.f32 	%f154, %f146, %f153, %p40;
	sub.ftz.f32 	%f155, %f154, %f143;
	fma.rn.ftz.f32 	%f156, %f155, %f1, %f143;
	st.global.f32 	[%rd86+8], %f156;
	add.s32 	%r102, %r96, 3;
	ld.global.nc.f32 	%f157, [%rd96+4];
	ld.global.nc.f32 	%f158, [%rd97+4];
	sub.ftz.f32 	%f159, %f158, %f157;
	ld.global.nc.f32 	%f160, [%rd95];
	sub.ftz.f32 	%f161, %f158, %f160;
	abs.ftz.f32 	%f162, %f161;
	sub.ftz.f32 	%f163, %f157, %f160;
	abs.ftz.f32 	%f164, %f163;
	max.ftz.f32 	%f165, %f162, %f164;
	max.ftz.f32 	%f166, %f159, %f165;
	setp.eq.s32 	%p41, %r104, 2;
	selp.f32 	%f167, %f159, %f166, %p41;
	sub.ftz.f32 	%f168, %f167, %f156;
	fma.rn.ftz.f32 	%f180, %f168, %f1, %f156;
	st.global.f32 	[%rd86+12], %f180;
	add.s64 	%rd98, %rd98, 16;
	add.s32 	%r104, %r104, -4;
	add.s32 	%r103, %r103, 4;
	add.s64 	%rd97, %rd97, 16;
	add.s64 	%rd96, %rd96, 16;
	add.s64 	%rd95, %rd95, 16;
	add.s32 	%r96, %r96, 4;
	setp.lt.s32 	%p42, %r96, %r56;
	@%p42 bra 	$L__BB2_45;

$L__BB2_46:
	ret;

}
	// .globl	atr_batch_f32
.visible .entry atr_batch_f32(
	.param .u64 atr_batch_f32_param_0,
	.param .u64 atr_batch_f32_param_1,
	.param .u64 atr_batch_f32_param_2,
	.param .u64 atr_batch_f32_param_3,
	.param .u64 atr_batch_f32_param_4,
	.param .u64 atr_batch_f32_param_5,
	.param .u32 atr_batch_f32_param_6,
	.param .u32 atr_batch_f32_param_7,
	.param .u32 atr_batch_f32_param_8,
	.param .u64 atr_batch_f32_param_9
)
{
	.reg .pred 	%p<33>;
	.reg .f32 	%f<113>;
	.reg .b32 	%r<93>;
	.reg .b64 	%rd<73>;


	ld.param.u64 	%rd34, [atr_batch_f32_param_0];
	ld.param.u64 	%rd29, [atr_batch_f32_param_1];
	ld.param.u64 	%rd35, [atr_batch_f32_param_2];
	ld.param.u64 	%rd30, [atr_batch_f32_param_3];
	ld.param.u64 	%rd31, [atr_batch_f32_param_4];
	ld.param.u64 	%rd32, [atr_batch_f32_param_5];
	ld.param.u32 	%r46, [atr_batch_f32_param_6];
	ld.param.u32 	%r47, [atr_batch_f32_param_7];
	ld.param.u32 	%r48, [atr_batch_f32_param_8];
	ld.param.u64 	%rd33, [atr_batch_f32_param_9];
	cvta.to.global.u64 	%rd1, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r48;
	@%p1 bra 	$L__BB3_29;

	cvta.to.global.u64 	%rd36, %rd30;
	mul.wide.s32 	%rd37, %r1, 4;
	add.s64 	%rd38, %rd36, %rd37;
	cvta.to.global.u64 	%rd39, %rd31;
	add.s64 	%rd40, %rd39, %rd37;
	ld.global.nc.f32 	%f1, [%rd40];
	cvta.to.global.u64 	%rd41, %rd32;
	add.s64 	%rd42, %rd41, %rd37;
	ld.global.nc.u32 	%r2, [%rd38];
	setp.lt.s32 	%p2, %r2, 1;
	ld.global.nc.u32 	%r3, [%rd42];
	setp.ge.s32 	%p3, %r3, %r46;
	or.pred  	%p4, %p2, %p3;
	setp.ge.s32 	%p5, %r47, %r46;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB3_29;

	mov.u32 	%r81, %tid.x;
	setp.ge.s32 	%p7, %r81, %r3;
	@%p7 bra 	$L__BB3_5;

	mov.u32 	%r5, %ntid.x;
	cvta.to.global.u64 	%rd3, %rd33;
	mul.lo.s32 	%r6, %r1, %r46;
	mov.u32 	%r80, %r81;

$L__BB3_4:
	add.s32 	%r49, %r80, %r6;
	mul.wide.s32 	%rd43, %r49, 4;
	add.s64 	%rd44, %rd3, %rd43;
	mov.u32 	%r50, 2143289344;
	st.global.u32 	[%rd44], %r50;
	add.s32 	%r80, %r80, %r5;
	setp.lt.s32 	%p8, %r80, %r3;
	@%p8 bra 	$L__BB3_4;

$L__BB3_5:
	bar.sync 	0;
	setp.ge.s32 	%p9, %r81, %r2;
	mov.f32 	%f106, 0f00000000;
	@%p9 bra 	$L__BB3_10;

	mov.f32 	%f24, 0f00000000;
	mov.u32 	%r9, %ntid.x;
	cvta.to.global.u64 	%rd47, %rd29;
	mov.f32 	%f106, %f24;

$L__BB3_7:
	add.s32 	%r11, %r81, %r47;
	mul.wide.s32 	%rd45, %r11, 4;
	add.s64 	%rd46, %rd2, %rd45;
	ld.global.nc.f32 	%f3, [%rd46];
	add.s64 	%rd48, %rd47, %rd45;
	ld.global.nc.f32 	%f4, [%rd48];
	setp.eq.s32 	%p10, %r81, 0;
	mov.f32 	%f103, %f24;
	@%p10 bra 	$L__BB3_9;

	add.s32 	%r51, %r11, -1;
	mul.wide.s32 	%rd49, %r51, 4;
	add.s64 	%rd50, %rd1, %rd49;
	ld.global.nc.f32 	%f103, [%rd50];

$L__BB3_9:
	sub.ftz.f32 	%f26, %f3, %f103;
	abs.ftz.f32 	%f27, %f26;
	sub.ftz.f32 	%f28, %f4, %f103;
	abs.ftz.f32 	%f29, %f28;
	max.ftz.f32 	%f30, %f27, %f29;
	sub.ftz.f32 	%f31, %f3, %f4;
	max.ftz.f32 	%f32, %f31, %f30;
	selp.f32 	%f33, %f31, %f32, %p10;
	add.ftz.f32 	%f106, %f106, %f33;
	add.s32 	%r81, %r81, %r9;
	setp.lt.s32 	%p12, %r81, %r2;
	@%p12 bra 	$L__BB3_7;

$L__BB3_10:
	mov.u32 	%r83, WARP_SZ;
	add.s32 	%r14, %r83, -1;
	mov.u32 	%r53, %tid.x;
	and.b32  	%r15, %r14, %r53;
	shr.u32 	%r16, %r53, 5;
	// begin inline asm
	activemask.b32 %r52;
	// end inline asm
	setp.lt.s32 	%p13, %r83, 2;
	@%p13 bra 	$L__BB3_13;

	mov.u32 	%r82, %r83;

$L__BB3_12:
	mov.b32 	%r54, %f106;
	shr.s32 	%r19, %r82, 1;
	mov.u32 	%r55, 31;
	shfl.sync.down.b32 	%r56|%p14, %r54, %r19, %r55, %r52;
	mov.b32 	%f34, %r56;
	add.ftz.f32 	%f106, %f106, %f34;
	setp.gt.s32 	%p15, %r82, 3;
	mov.u32 	%r82, %r19;
	@%p15 bra 	$L__BB3_12;

$L__BB3_13:
	setp.ne.s32 	%p16, %r15, 0;
	@%p16 bra 	$L__BB3_15;

	shl.b32 	%r57, %r16, 2;
	mov.u32 	%r58, _ZZN44_INTERNAL_4d743621_13_atr_kernel_cu_e49a8fd216block_reduce_sumEfE9warp_sums;
	add.s32 	%r59, %r58, %r57;
	st.shared.f32 	[%r59], %f106;

$L__BB3_15:
	bar.sync 	0;
	setp.ne.s32 	%p17, %r16, 0;
	mov.f32 	%f109, 0f00000000;
	@%p17 bra 	$L__BB3_20;

	mov.u32 	%r60, %ntid.x;
	add.s32 	%r61, %r14, %r60;
	shr.u32 	%r62, %r61, 5;
	setp.ge.s32 	%p18, %r15, %r62;
	mov.f32 	%f109, 0f00000000;
	@%p18 bra 	$L__BB3_18;

	shl.b32 	%r63, %r15, 2;
	mov.u32 	%r64, _ZZN44_INTERNAL_4d743621_13_atr_kernel_cu_e49a8fd216block_reduce_sumEfE9warp_sums;
	add.s32 	%r65, %r64, %r63;
	ld.shared.f32 	%f109, [%r65];

$L__BB3_18:
	// begin inline asm
	activemask.b32 %r66;
	// end inline asm
	@%p13 bra 	$L__BB3_20;

$L__BB3_19:
	mov.b32 	%r67, %f109;
	shr.s32 	%r22, %r83, 1;
	mov.u32 	%r68, 31;
	shfl.sync.down.b32 	%r69|%p20, %r67, %r22, %r68, %r66;
	mov.b32 	%f37, %r69;
	add.ftz.f32 	%f109, %f109, %f37;
	setp.gt.s32 	%p21, %r83, 3;
	mov.u32 	%r83, %r22;
	@%p21 bra 	$L__BB3_19;

$L__BB3_20:
	setp.ne.s32 	%p22, %r53, 0;
	@%p22 bra 	$L__BB3_29;

	cvt.rn.f32.s32 	%f38, %r2;
	div.approx.ftz.f32 	%f110, %f109, %f38;
	mad.lo.s32 	%r23, %r1, %r46, %r3;
	cvta.to.global.u64 	%rd72, %rd33;
	mul.wide.s32 	%rd52, %r23, 4;
	add.s64 	%rd53, %rd72, %rd52;
	st.global.f32 	[%rd53], %f110;
	add.s32 	%r91, %r3, 1;
	setp.ge.s32 	%p23, %r91, %r46;
	@%p23 bra 	$L__BB3_29;

	not.b32 	%r72, %r3;
	add.s32 	%r73, %r72, %r46;
	and.b32  	%r86, %r73, 3;
	setp.eq.s32 	%p24, %r86, 0;
	mov.u32 	%r88, %r3;
	@%p24 bra 	$L__BB3_26;

	add.s32 	%r74, %r23, 1;
	cvta.to.global.u64 	%rd54, %rd33;
	mul.wide.s32 	%rd55, %r74, 4;
	add.s64 	%rd68, %rd54, %rd55;
	sub.s32 	%r84, %r3, %r47;
	cvta.to.global.u64 	%rd56, %rd29;
	mul.wide.s32 	%rd57, %r91, 4;
	add.s64 	%rd67, %rd56, %rd57;
	add.s64 	%rd66, %rd2, %rd57;
	mul.wide.s32 	%rd58, %r3, 4;
	add.s64 	%rd65, %rd1, %rd58;
	mov.u32 	%r88, %r3;

$L__BB3_24:
	.pragma "nounroll";
	ld.global.nc.f32 	%f39, [%rd67];
	ld.global.nc.f32 	%f40, [%rd66];
	sub.ftz.f32 	%f41, %f40, %f39;
	ld.global.nc.f32 	%f42, [%rd65];
	sub.ftz.f32 	%f43, %f40, %f42;
	abs.ftz.f32 	%f44, %f43;
	sub.ftz.f32 	%f45, %f39, %f42;
	abs.ftz.f32 	%f46, %f45;
	max.ftz.f32 	%f47, %f44, %f46;
	max.ftz.f32 	%f48, %f41, %f47;
	setp.eq.s32 	%p25, %r84, -1;
	selp.f32 	%f49, %f41, %f48, %p25;
	sub.ftz.f32 	%f50, %f49, %f110;
	fma.rn.ftz.f32 	%f110, %f50, %f1, %f110;
	st.global.f32 	[%rd68], %f110;
	add.s32 	%r88, %r88, 1;
	add.s64 	%rd68, %rd68, 4;
	add.s32 	%r84, %r84, 1;
	add.s64 	%rd67, %rd67, 4;
	add.s64 	%rd66, %rd66, 4;
	add.s64 	%rd65, %rd65, 4;
	add.s32 	%r86, %r86, -1;
	setp.ne.s32 	%p26, %r86, 0;
	@%p26 bra 	$L__BB3_24;

	add.s32 	%r91, %r88, 1;

$L__BB3_26:
	add.s32 	%r75, %r46, -2;
	sub.s32 	%r76, %r75, %r3;
	setp.lt.u32 	%p27, %r76, 3;
	@%p27 bra 	$L__BB3_29;

	mad.lo.s32 	%r78, %r1, %r46, %r91;
	add.s32 	%r79, %r47, -1;
	sub.s32 	%r90, %r79, %r91;
	sub.s32 	%r89, %r91, %r47;
	mul.wide.s32 	%rd16, %r78, 4;
	mul.wide.s32 	%rd59, %r91, 4;
	add.s64 	%rd60, %rd59, 8;
	add.s64 	%rd71, %rd1, %rd60;
	add.s64 	%rd70, %rd2, %rd60;
	cvta.to.global.u64 	%rd61, %rd29;
	add.s64 	%rd69, %rd61, %rd60;

$L__BB3_28:
	mul.wide.s32 	%rd62, %r88, 4;
	add.s64 	%rd63, %rd1, %rd62;
	ld.global.nc.f32 	%f51, [%rd69+-8];
	ld.global.nc.f32 	%f52, [%rd70+-8];
	sub.ftz.f32 	%f53, %f52, %f51;
	ld.global.nc.f32 	%f54, [%rd63];
	sub.ftz.f32 	%f55, %f52, %f54;
	abs.ftz.f32 	%f56, %f55;
	sub.ftz.f32 	%f57, %f51, %f54;
	abs.ftz.f32 	%f58, %f57;
	max.ftz.f32 	%f59, %f56, %f58;
	max.ftz.f32 	%f60, %f53, %f59;
	setp.eq.s32 	%p28, %r90, -1;
	selp.f32 	%f61, %f53, %f60, %p28;
	sub.ftz.f32 	%f62, %f61, %f110;
	fma.rn.ftz.f32 	%f63, %f62, %f1, %f110;
	add.s64 	%rd64, %rd72, %rd16;
	st.global.f32 	[%rd64], %f63;
	ld.global.nc.f32 	%f64, [%rd69+-4];
	ld.global.nc.f32 	%f65, [%rd70+-4];
	sub.ftz.f32 	%f66, %f65, %f64;
	ld.global.nc.f32 	%f67, [%rd71+-8];
	sub.ftz.f32 	%f68, %f65, %f67;
	abs.ftz.f32 	%f69, %f68;
	sub.ftz.f32 	%f70, %f64, %f67;
	abs.ftz.f32 	%f71, %f70;
	max.ftz.f32 	%f72, %f69, %f71;
	max.ftz.f32 	%f73, %f66, %f72;
	setp.eq.s32 	%p29, %r89, -1;
	selp.f32 	%f74, %f66, %f73, %p29;
	sub.ftz.f32 	%f75, %f74, %f63;
	fma.rn.ftz.f32 	%f76, %f75, %f1, %f63;
	st.global.f32 	[%rd64+4], %f76;
	ld.global.nc.f32 	%f77, [%rd69];
	ld.global.nc.f32 	%f78, [%rd70];
	sub.ftz.f32 	%f79, %f78, %f77;
	ld.global.nc.f32 	%f80, [%rd71+-4];
	sub.ftz.f32 	%f81, %f78, %f80;
	abs.ftz.f32 	%f82, %f81;
	sub.ftz.f32 	%f83, %f77, %f80;
	abs.ftz.f32 	%f84, %f83;
	max.ftz.f32 	%f85, %f82, %f84;
	max.ftz.f32 	%f86, %f79, %f85;
	setp.eq.s32 	%p30, %r90, 1;
	selp.f32 	%f87, %f79, %f86, %p30;
	sub.ftz.f32 	%f88, %f87, %f76;
	fma.rn.ftz.f32 	%f89, %f88, %f1, %f76;
	st.global.f32 	[%rd64+8], %f89;
	add.s32 	%r88, %r91, 3;
	ld.global.nc.f32 	%f90, [%rd69+4];
	ld.global.nc.f32 	%f91, [%rd70+4];
	sub.ftz.f32 	%f92, %f91, %f90;
	ld.global.nc.f32 	%f93, [%rd71];
	sub.ftz.f32 	%f94, %f91, %f93;
	abs.ftz.f32 	%f95, %f94;
	sub.ftz.f32 	%f96, %f90, %f93;
	abs.ftz.f32 	%f97, %f96;
	max.ftz.f32 	%f98, %f95, %f97;
	max.ftz.f32 	%f99, %f92, %f98;
	setp.eq.s32 	%p31, %r90, 2;
	selp.f32 	%f100, %f92, %f99, %p31;
	sub.ftz.f32 	%f101, %f100, %f89;
	fma.rn.ftz.f32 	%f110, %f101, %f1, %f89;
	st.global.f32 	[%rd64+12], %f110;
	add.s64 	%rd72, %rd72, 16;
	add.s32 	%r90, %r90, -4;
	add.s32 	%r89, %r89, 4;
	add.s64 	%rd71, %rd71, 16;
	add.s64 	%rd70, %rd70, 16;
	add.s64 	%rd69, %rd69, 16;
	add.s32 	%r91, %r91, 4;
	setp.lt.s32 	%p32, %r91, %r46;
	@%p32 bra 	$L__BB3_28;

$L__BB3_29:
	ret;

}
	// .globl	atr_batch_from_tr_prefix_f32
.visible .entry atr_batch_from_tr_prefix_f32(
	.param .u64 atr_batch_from_tr_prefix_f32_param_0,
	.param .u64 atr_batch_from_tr_prefix_f32_param_1,
	.param .u64 atr_batch_from_tr_prefix_f32_param_2,
	.param .u64 atr_batch_from_tr_prefix_f32_param_3,
	.param .u64 atr_batch_from_tr_prefix_f32_param_4,
	.param .u32 atr_batch_from_tr_prefix_f32_param_5,
	.param .u32 atr_batch_from_tr_prefix_f32_param_6,
	.param .u32 atr_batch_from_tr_prefix_f32_param_7,
	.param .u64 atr_batch_from_tr_prefix_f32_param_8
)
{
	.reg .pred 	%p<29>;
	.reg .f32 	%f<63>;
	.reg .b32 	%r<70>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<48>;


	ld.param.u64 	%rd19, [atr_batch_from_tr_prefix_f32_param_0];
	ld.param.u64 	%rd15, [atr_batch_from_tr_prefix_f32_param_1];
	ld.param.u64 	%rd16, [atr_batch_from_tr_prefix_f32_param_2];
	ld.param.u64 	%rd17, [atr_batch_from_tr_prefix_f32_param_3];
	ld.param.u64 	%rd18, [atr_batch_from_tr_prefix_f32_param_4];
	ld.param.u32 	%r32, [atr_batch_from_tr_prefix_f32_param_5];
	ld.param.u32 	%r33, [atr_batch_from_tr_prefix_f32_param_6];
	ld.param.u32 	%r34, [atr_batch_from_tr_prefix_f32_param_7];
	ld.param.u64 	%rd20, [atr_batch_from_tr_prefix_f32_param_8];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd20;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r34;
	@%p1 bra 	$L__BB4_33;

	cvta.to.global.u64 	%rd21, %rd16;
	mul.wide.s32 	%rd22, %r1, 4;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd17;
	add.s64 	%rd25, %rd24, %rd22;
	ld.global.nc.f32 	%f1, [%rd25];
	cvta.to.global.u64 	%rd26, %rd18;
	add.s64 	%rd27, %rd26, %rd22;
	ld.global.nc.u32 	%r2, [%rd23];
	setp.lt.s32 	%p2, %r2, 1;
	ld.global.nc.u32 	%r3, [%rd27];
	setp.ge.s32 	%p3, %r3, %r32;
	or.pred  	%p4, %p2, %p3;
	setp.ge.s32 	%p5, %r33, %r32;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB4_33;

	mul.lo.s32 	%r4, %r1, %r32;
	mov.u32 	%r5, %tid.x;
	setp.ge.s32 	%p7, %r5, %r3;
	@%p7 bra 	$L__BB4_5;

	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r62, %r5;

$L__BB4_4:
	add.s32 	%r35, %r62, %r4;
	mul.wide.s32 	%rd28, %r35, 4;
	add.s64 	%rd29, %rd2, %rd28;
	mov.u32 	%r36, 2143289344;
	st.global.u32 	[%rd29], %r36;
	add.s32 	%r62, %r62, %r6;
	setp.lt.s32 	%p8, %r62, %r3;
	@%p8 bra 	$L__BB4_4;

$L__BB4_5:
	bar.sync 	0;
	setp.eq.s64 	%p9, %rd15, 0;
	@%p9 bra 	$L__BB4_9;

	setp.ne.s32 	%p10, %r5, 0;
	mov.f32 	%f59, 0f00000000;
	@%p10 bra 	$L__BB4_8;

	cvta.to.global.u64 	%rd30, %rd15;
	add.s32 	%r37, %r3, 1;
	mul.wide.s32 	%rd31, %r37, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.f64 	%fd1, [%rd32];
	cvt.rn.ftz.f32.f64 	%f24, %fd1;
	mul.wide.s32 	%rd33, %r33, 8;
	add.s64 	%rd34, %rd30, %rd33;
	ld.global.nc.f64 	%fd2, [%rd34];
	cvt.rn.ftz.f32.f64 	%f25, %fd2;
	sub.ftz.f32 	%f26, %f24, %f25;
	cvt.rn.f32.s32 	%f27, %r2;
	div.approx.ftz.f32 	%f59, %f26, %f27;

$L__BB4_8:
	bar.sync 	0;
	bra.uni 	$L__BB4_25;

$L__BB4_9:
	setp.ge.s32 	%p11, %r5, %r2;
	mov.f32 	%f54, 0f00000000;
	@%p11 bra 	$L__BB4_12;

	mov.f32 	%f54, 0f00000000;
	mov.u32 	%r9, %ntid.x;
	mov.u32 	%r63, %r5;

$L__BB4_11:
	add.s32 	%r38, %r63, %r33;
	mul.wide.s32 	%rd35, %r38, 4;
	add.s64 	%rd36, %rd1, %rd35;
	ld.global.nc.f32 	%f30, [%rd36];
	add.ftz.f32 	%f54, %f54, %f30;
	add.s32 	%r63, %r63, %r9;
	setp.lt.s32 	%p12, %r63, %r2;
	@%p12 bra 	$L__BB4_11;

$L__BB4_12:
	mov.u32 	%r65, WARP_SZ;
	add.s32 	%r13, %r65, -1;
	and.b32  	%r14, %r13, %r5;
	shr.u32 	%r15, %r5, 5;
	// begin inline asm
	activemask.b32 %r39;
	// end inline asm
	setp.lt.s32 	%p13, %r65, 2;
	@%p13 bra 	$L__BB4_15;

	mov.u32 	%r64, %r65;

$L__BB4_14:
	mov.b32 	%r40, %f54;
	shr.s32 	%r18, %r64, 1;
	mov.u32 	%r41, 31;
	shfl.sync.down.b32 	%r42|%p14, %r40, %r18, %r41, %r39;
	mov.b32 	%f31, %r42;
	add.ftz.f32 	%f54, %f54, %f31;
	setp.gt.s32 	%p15, %r64, 3;
	mov.u32 	%r64, %r18;
	@%p15 bra 	$L__BB4_14;

$L__BB4_15:
	setp.ne.s32 	%p16, %r14, 0;
	@%p16 bra 	$L__BB4_17;

	shl.b32 	%r43, %r15, 2;
	mov.u32 	%r44, _ZZN44_INTERNAL_4d743621_13_atr_kernel_cu_e49a8fd216block_reduce_sumEfE9warp_sums;
	add.s32 	%r45, %r44, %r43;
	st.shared.f32 	[%r45], %f54;

$L__BB4_17:
	bar.sync 	0;
	setp.ne.s32 	%p17, %r15, 0;
	mov.f32 	%f59, 0f00000000;
	mov.f32 	%f57, %f59;
	@%p17 bra 	$L__BB4_22;

	mov.u32 	%r46, %ntid.x;
	add.s32 	%r47, %r13, %r46;
	shr.u32 	%r48, %r47, 5;
	setp.ge.s32 	%p18, %r14, %r48;
	mov.f32 	%f57, 0f00000000;
	@%p18 bra 	$L__BB4_20;

	shl.b32 	%r49, %r14, 2;
	mov.u32 	%r50, _ZZN44_INTERNAL_4d743621_13_atr_kernel_cu_e49a8fd216block_reduce_sumEfE9warp_sums;
	add.s32 	%r51, %r50, %r49;
	ld.shared.f32 	%f57, [%r51];

$L__BB4_20:
	// begin inline asm
	activemask.b32 %r52;
	// end inline asm
	@%p13 bra 	$L__BB4_22;

$L__BB4_21:
	mov.b32 	%r53, %f57;
	shr.s32 	%r21, %r65, 1;
	mov.u32 	%r54, 31;
	shfl.sync.down.b32 	%r55|%p20, %r53, %r21, %r54, %r52;
	mov.b32 	%f34, %r55;
	add.ftz.f32 	%f57, %f57, %f34;
	setp.gt.s32 	%p21, %r65, 3;
	mov.u32 	%r65, %r21;
	@%p21 bra 	$L__BB4_21;

$L__BB4_22:
	setp.ne.s32 	%p22, %r5, 0;
	@%p22 bra 	$L__BB4_24;

	cvt.rn.f32.s32 	%f36, %r2;
	div.approx.ftz.f32 	%f59, %f57, %f36;

$L__BB4_24:
	bar.sync 	0;

$L__BB4_25:
	setp.ne.s32 	%p23, %r5, 0;
	@%p23 bra 	$L__BB4_33;

	add.s32 	%r22, %r3, %r4;
	mul.wide.s32 	%rd37, %r22, 4;
	add.s64 	%rd38, %rd2, %rd37;
	st.global.f32 	[%rd38], %f59;
	add.s32 	%r68, %r3, 1;
	setp.ge.s32 	%p24, %r68, %r32;
	@%p24 bra 	$L__BB4_33;

	not.b32 	%r56, %r3;
	add.s32 	%r57, %r56, %r32;
	and.b32  	%r67, %r57, 3;
	setp.eq.s32 	%p25, %r67, 0;
	@%p25 bra 	$L__BB4_30;

	add.s32 	%r58, %r22, 1;
	mul.wide.s32 	%rd39, %r58, 4;
	add.s64 	%rd45, %rd2, %rd39;
	mul.wide.s32 	%rd40, %r68, 4;
	add.s64 	%rd44, %rd1, %rd40;

$L__BB4_29:
	.pragma "nounroll";
	ld.global.nc.f32 	%f37, [%rd44];
	sub.ftz.f32 	%f38, %f37, %f59;
	fma.rn.ftz.f32 	%f59, %f38, %f1, %f59;
	st.global.f32 	[%rd45], %f59;
	add.s32 	%r68, %r68, 1;
	add.s64 	%rd45, %rd45, 4;
	add.s64 	%rd44, %rd44, 4;
	add.s32 	%r67, %r67, -1;
	setp.ne.s32 	%p26, %r67, 0;
	@%p26 bra 	$L__BB4_29;

$L__BB4_30:
	add.s32 	%r59, %r32, -2;
	sub.s32 	%r60, %r59, %r3;
	setp.lt.u32 	%p27, %r60, 3;
	@%p27 bra 	$L__BB4_33;

	add.s32 	%r61, %r68, %r4;
	mul.wide.s32 	%rd41, %r61, 4;
	add.s64 	%rd47, %rd2, %rd41;
	mul.wide.s32 	%rd42, %r68, 4;
	add.s64 	%rd43, %rd1, %rd42;
	add.s64 	%rd46, %rd43, 8;

$L__BB4_32:
	ld.global.nc.f32 	%f39, [%rd46+-8];
	sub.ftz.f32 	%f40, %f39, %f59;
	fma.rn.ftz.f32 	%f41, %f40, %f1, %f59;
	st.global.f32 	[%rd47], %f41;
	ld.global.nc.f32 	%f42, [%rd46+-4];
	sub.ftz.f32 	%f43, %f42, %f41;
	fma.rn.ftz.f32 	%f44, %f43, %f1, %f41;
	st.global.f32 	[%rd47+4], %f44;
	ld.global.nc.f32 	%f45, [%rd46];
	sub.ftz.f32 	%f46, %f45, %f44;
	fma.rn.ftz.f32 	%f47, %f46, %f1, %f44;
	st.global.f32 	[%rd47+8], %f47;
	ld.global.nc.f32 	%f48, [%rd46+4];
	sub.ftz.f32 	%f49, %f48, %f47;
	fma.rn.ftz.f32 	%f59, %f49, %f1, %f47;
	st.global.f32 	[%rd47+12], %f59;
	add.s64 	%rd47, %rd47, 16;
	add.s64 	%rd46, %rd46, 16;
	add.s32 	%r68, %r68, 4;
	setp.lt.s32 	%p28, %r68, %r32;
	@%p28 bra 	$L__BB4_32;

$L__BB4_33:
	ret;

}
	// .globl	atr_many_series_one_param_f32
.visible .entry atr_many_series_one_param_f32(
	.param .u64 atr_many_series_one_param_f32_param_0,
	.param .u64 atr_many_series_one_param_f32_param_1,
	.param .u64 atr_many_series_one_param_f32_param_2,
	.param .u64 atr_many_series_one_param_f32_param_3,
	.param .u32 atr_many_series_one_param_f32_param_4,
	.param .f32 atr_many_series_one_param_f32_param_5,
	.param .u32 atr_many_series_one_param_f32_param_6,
	.param .u32 atr_many_series_one_param_f32_param_7,
	.param .u64 atr_many_series_one_param_f32_param_8
)
{
	.reg .pred 	%p<34>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<83>;
	.reg .b64 	%rd<77>;


	ld.param.u64 	%rd20, [atr_many_series_one_param_f32_param_0];
	ld.param.u64 	%rd21, [atr_many_series_one_param_f32_param_1];
	ld.param.u64 	%rd22, [atr_many_series_one_param_f32_param_2];
	ld.param.u64 	%rd19, [atr_many_series_one_param_f32_param_3];
	ld.param.u32 	%r37, [atr_many_series_one_param_f32_param_4];
	ld.param.f32 	%f15, [atr_many_series_one_param_f32_param_5];
	ld.param.u32 	%r38, [atr_many_series_one_param_f32_param_6];
	ld.param.u32 	%r39, [atr_many_series_one_param_f32_param_7];
	ld.param.u64 	%rd23, [atr_many_series_one_param_f32_param_8];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd21;
	cvta.to.global.u64 	%rd3, %rd20;
	cvta.to.global.u64 	%rd4, %rd23;
	setp.lt.s32 	%p1, %r38, 1;
	setp.lt.s32 	%p2, %r37, 1;
	or.pred  	%p3, %p2, %p1;
	setp.lt.s32 	%p4, %r39, 1;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB5_26;

	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, WARP_SZ;
	shr.u32 	%r40, %r1, 5;
	mov.u32 	%r41, %ntid.x;
	shr.u32 	%r3, %r41, 5;
	mov.u32 	%r42, %ctaid.x;
	mad.lo.s32 	%r43, %r3, %r42, %r40;
	mul.lo.s32 	%r75, %r43, %r2;
	setp.ge.s32 	%p6, %r75, %r38;
	@%p6 bra 	$L__BB5_26;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r45, %r2, %r44;
	mul.lo.s32 	%r5, %r45, %r3;
	cvt.rn.f32.s32 	%f1, %r37;
	sub.s32 	%r6, %r39, %r37;
	not.b32 	%r46, %r37;
	add.s32 	%r7, %r46, %r39;
	mul.wide.s32 	%rd5, %r38, 4;
	cvta.to.global.u64 	%rd6, %rd19;
	add.s32 	%r47, %r2, -1;
	and.b32  	%r8, %r47, %r1;

$L__BB5_3:
	add.s32 	%r10, %r75, %r8;
	setp.ge.s32 	%p7, %r10, %r38;
	@%p7 bra 	$L__BB5_25;

	mul.wide.s32 	%rd24, %r10, 4;
	add.s64 	%rd25, %rd6, %rd24;
	ld.global.nc.u32 	%r11, [%rd25];
	setp.lt.s32 	%p8, %r11, 0;
	setp.ge.s32 	%p9, %r11, %r39;
	or.pred  	%p10, %p8, %p9;
	@%p10 bra 	$L__BB5_25;

	add.s32 	%r12, %r11, %r37;
	setp.gt.s32 	%p11, %r12, %r39;
	@%p11 bra 	$L__BB5_25;

	add.s32 	%r13, %r12, -1;
	setp.lt.s32 	%p12, %r12, 2;
	@%p12 bra 	$L__BB5_14;

	max.s32 	%r14, %r13, 1;
	add.s32 	%r49, %r14, -1;
	and.b32  	%r15, %r14, 3;
	setp.lt.u32 	%p13, %r49, 3;
	mov.u32 	%r78, 0;
	@%p13 bra 	$L__BB5_10;

	sub.s32 	%r77, %r14, %r15;
	mov.u32 	%r78, 0;

$L__BB5_9:
	mad.lo.s32 	%r51, %r78, %r38, %r10;
	mul.wide.s32 	%rd26, %r51, 4;
	add.s64 	%rd27, %rd4, %rd26;
	mov.u32 	%r52, 2143289344;
	st.global.u32 	[%rd27], %r52;
	add.s64 	%rd28, %rd27, %rd5;
	st.global.u32 	[%rd28], %r52;
	add.s64 	%rd29, %rd28, %rd5;
	st.global.u32 	[%rd29], %r52;
	add.s64 	%rd30, %rd29, %rd5;
	st.global.u32 	[%rd30], %r52;
	add.s32 	%r78, %r78, 4;
	add.s32 	%r77, %r77, -4;
	setp.ne.s32 	%p14, %r77, 0;
	@%p14 bra 	$L__BB5_9;

$L__BB5_10:
	setp.eq.s32 	%p15, %r15, 0;
	@%p15 bra 	$L__BB5_14;

	mad.lo.s32 	%r22, %r78, %r38, %r10;
	mul.wide.s32 	%rd31, %r22, 4;
	add.s64 	%rd32, %rd4, %rd31;
	mov.u32 	%r53, 2143289344;
	st.global.u32 	[%rd32], %r53;
	setp.eq.s32 	%p16, %r15, 1;
	@%p16 bra 	$L__BB5_14;

	add.s32 	%r23, %r22, %r38;
	mul.wide.s32 	%rd33, %r23, 4;
	add.s64 	%rd34, %rd4, %rd33;
	st.global.u32 	[%rd34], %r53;
	setp.eq.s32 	%p17, %r15, 2;
	@%p17 bra 	$L__BB5_14;

	add.s32 	%r55, %r23, %r38;
	mul.wide.s32 	%rd35, %r55, 4;
	add.s64 	%rd36, %rd4, %rd35;
	mov.u32 	%r56, 2143289344;
	st.global.u32 	[%rd36], %r56;

$L__BB5_14:
	add.s32 	%r58, %r11, -1;
	mad.lo.s32 	%r79, %r38, %r58, %r10;
	mad.lo.s32 	%r59, %r38, %r11, %r10;
	mul.wide.s32 	%rd37, %r59, 4;
	add.s64 	%rd76, %rd2, %rd37;
	add.s64 	%rd75, %rd3, %rd37;
	mov.f32 	%f16, 0f00000000;
	mov.u32 	%r80, 0;
	mov.f32 	%f112, %f16;

$L__BB5_15:
	.pragma "nounroll";
	ld.global.nc.f32 	%f3, [%rd75];
	ld.global.nc.f32 	%f4, [%rd76];
	setp.eq.s32 	%p18, %r80, 0;
	mov.f32 	%f113, %f16;
	@%p18 bra 	$L__BB5_17;

	mul.wide.s32 	%rd38, %r79, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.global.nc.f32 	%f113, [%rd39];

$L__BB5_17:
	sub.ftz.f32 	%f18, %f3, %f113;
	abs.ftz.f32 	%f19, %f18;
	sub.ftz.f32 	%f20, %f4, %f113;
	abs.ftz.f32 	%f21, %f20;
	max.ftz.f32 	%f22, %f19, %f21;
	sub.ftz.f32 	%f23, %f3, %f4;
	max.ftz.f32 	%f24, %f23, %f22;
	selp.f32 	%f25, %f23, %f24, %p18;
	add.ftz.f32 	%f112, %f112, %f25;
	add.s32 	%r79, %r79, %r38;
	add.s64 	%rd76, %rd76, %rd5;
	add.s64 	%rd75, %rd75, %rd5;
	add.s32 	%r80, %r80, 1;
	setp.lt.s32 	%p20, %r80, %r37;
	@%p20 bra 	$L__BB5_15;

	div.approx.ftz.f32 	%f114, %f112, %f1;
	mad.lo.s32 	%r60, %r13, %r38, %r10;
	cvt.s64.s32 	%rd13, %r60;
	mul.wide.s32 	%rd40, %r60, 4;
	add.s64 	%rd14, %rd4, %rd40;
	st.global.f32 	[%rd14], %f114;
	setp.ge.s32 	%p21, %r12, %r39;
	@%p21 bra 	$L__BB5_25;

	sub.s32 	%r61, %r6, %r11;
	and.b32  	%r29, %r61, 3;
	setp.eq.s32 	%p22, %r29, 0;
	mov.u32 	%r81, %r12;
	@%p22 bra 	$L__BB5_23;

	mad.lo.s32 	%r62, %r12, %r38, %r10;
	cvt.s64.s32 	%rd15, %r62;
	mul.wide.s32 	%rd41, %r62, 4;
	add.s64 	%rd42, %rd3, %rd41;
	add.s64 	%rd43, %rd2, %rd41;
	ld.global.nc.f32 	%f26, [%rd43];
	ld.global.nc.f32 	%f27, [%rd42];
	sub.ftz.f32 	%f28, %f27, %f26;
	shl.b64 	%rd44, %rd13, 2;
	add.s64 	%rd45, %rd1, %rd44;
	ld.global.nc.f32 	%f29, [%rd45];
	sub.ftz.f32 	%f30, %f27, %f29;
	abs.ftz.f32 	%f31, %f30;
	sub.ftz.f32 	%f32, %f26, %f29;
	abs.ftz.f32 	%f33, %f32;
	max.ftz.f32 	%f34, %f31, %f33;
	max.ftz.f32 	%f35, %f28, %f34;
	sub.ftz.f32 	%f36, %f35, %f114;
	fma.rn.ftz.f32 	%f114, %f36, %f15, %f114;
	add.s64 	%rd16, %rd14, %rd5;
	st.global.f32 	[%rd16], %f114;
	add.s32 	%r81, %r12, 1;
	setp.eq.s32 	%p23, %r29, 1;
	@%p23 bra 	$L__BB5_23;

	cvt.u32.u64 	%r63, %rd15;
	add.s32 	%r64, %r63, %r38;
	cvt.s64.s32 	%rd17, %r64;
	mul.wide.s32 	%rd46, %r64, 4;
	add.s64 	%rd47, %rd3, %rd46;
	add.s64 	%rd48, %rd2, %rd46;
	ld.global.nc.f32 	%f37, [%rd48];
	ld.global.nc.f32 	%f38, [%rd47];
	sub.ftz.f32 	%f39, %f38, %f37;
	shl.b64 	%rd49, %rd15, 2;
	add.s64 	%rd50, %rd1, %rd49;
	ld.global.nc.f32 	%f40, [%rd50];
	sub.ftz.f32 	%f41, %f38, %f40;
	abs.ftz.f32 	%f42, %f41;
	sub.ftz.f32 	%f43, %f37, %f40;
	abs.ftz.f32 	%f44, %f43;
	max.ftz.f32 	%f45, %f42, %f44;
	max.ftz.f32 	%f46, %f39, %f45;
	setp.eq.s32 	%p24, %r81, %r11;
	selp.f32 	%f47, %f39, %f46, %p24;
	sub.ftz.f32 	%f48, %f47, %f114;
	fma.rn.ftz.f32 	%f114, %f48, %f15, %f114;
	add.s64 	%rd18, %rd16, %rd5;
	st.global.f32 	[%rd18], %f114;
	add.s32 	%r81, %r12, 2;
	setp.eq.s32 	%p25, %r29, 2;
	@%p25 bra 	$L__BB5_23;

	cvt.u32.u64 	%r65, %rd17;
	add.s32 	%r66, %r65, %r38;
	mul.wide.s32 	%rd51, %r66, 4;
	add.s64 	%rd52, %rd3, %rd51;
	add.s64 	%rd53, %rd2, %rd51;
	ld.global.nc.f32 	%f49, [%rd53];
	ld.global.nc.f32 	%f50, [%rd52];
	sub.ftz.f32 	%f51, %f50, %f49;
	shl.b64 	%rd54, %rd17, 2;
	add.s64 	%rd55, %rd1, %rd54;
	ld.global.nc.f32 	%f52, [%rd55];
	sub.ftz.f32 	%f53, %f50, %f52;
	abs.ftz.f32 	%f54, %f53;
	sub.ftz.f32 	%f55, %f49, %f52;
	abs.ftz.f32 	%f56, %f55;
	max.ftz.f32 	%f57, %f54, %f56;
	max.ftz.f32 	%f58, %f51, %f57;
	setp.eq.s32 	%p26, %r81, %r11;
	selp.f32 	%f59, %f51, %f58, %p26;
	sub.ftz.f32 	%f60, %f59, %f114;
	fma.rn.ftz.f32 	%f114, %f60, %f15, %f114;
	add.s64 	%rd56, %rd18, %rd5;
	st.global.f32 	[%rd56], %f114;
	add.s32 	%r81, %r12, 3;

$L__BB5_23:
	sub.s32 	%r67, %r7, %r11;
	setp.lt.u32 	%p27, %r67, 3;
	@%p27 bra 	$L__BB5_25;

$L__BB5_24:
	mul.lo.s32 	%r68, %r81, %r38;
	add.s32 	%r69, %r68, %r10;
	mul.wide.s32 	%rd57, %r69, 4;
	add.s64 	%rd58, %rd3, %rd57;
	add.s64 	%rd59, %rd2, %rd57;
	sub.s32 	%r70, %r68, %r38;
	add.s32 	%r71, %r70, %r10;
	mul.wide.s32 	%rd60, %r71, 4;
	add.s64 	%rd61, %rd1, %rd60;
	ld.global.nc.f32 	%f61, [%rd59];
	ld.global.nc.f32 	%f62, [%rd58];
	sub.ftz.f32 	%f63, %f62, %f61;
	ld.global.nc.f32 	%f64, [%rd61];
	sub.ftz.f32 	%f65, %f62, %f64;
	abs.ftz.f32 	%f66, %f65;
	sub.ftz.f32 	%f67, %f61, %f64;
	abs.ftz.f32 	%f68, %f67;
	max.ftz.f32 	%f69, %f66, %f68;
	max.ftz.f32 	%f70, %f63, %f69;
	setp.eq.s32 	%p28, %r81, %r11;
	selp.f32 	%f71, %f63, %f70, %p28;
	sub.ftz.f32 	%f72, %f71, %f114;
	fma.rn.ftz.f32 	%f73, %f72, %f15, %f114;
	add.s64 	%rd62, %rd4, %rd57;
	st.global.f32 	[%rd62], %f73;
	add.s64 	%rd63, %rd58, %rd5;
	add.s64 	%rd64, %rd59, %rd5;
	add.s64 	%rd65, %rd61, %rd5;
	ld.global.nc.f32 	%f74, [%rd64];
	ld.global.nc.f32 	%f75, [%rd63];
	sub.ftz.f32 	%f76, %f75, %f74;
	ld.global.nc.f32 	%f77, [%rd65];
	sub.ftz.f32 	%f78, %f75, %f77;
	abs.ftz.f32 	%f79, %f78;
	sub.ftz.f32 	%f80, %f74, %f77;
	abs.ftz.f32 	%f81, %f80;
	max.ftz.f32 	%f82, %f79, %f81;
	max.ftz.f32 	%f83, %f76, %f82;
	add.s32 	%r72, %r81, 1;
	setp.eq.s32 	%p29, %r72, %r11;
	selp.f32 	%f84, %f76, %f83, %p29;
	sub.ftz.f32 	%f85, %f84, %f73;
	fma.rn.ftz.f32 	%f86, %f85, %f15, %f73;
	add.s64 	%rd66, %rd62, %rd5;
	st.global.f32 	[%rd66], %f86;
	add.s64 	%rd67, %rd63, %rd5;
	add.s64 	%rd68, %rd64, %rd5;
	add.s64 	%rd69, %rd65, %rd5;
	ld.global.nc.f32 	%f87, [%rd68];
	ld.global.nc.f32 	%f88, [%rd67];
	sub.ftz.f32 	%f89, %f88, %f87;
	ld.global.nc.f32 	%f90, [%rd69];
	sub.ftz.f32 	%f91, %f88, %f90;
	abs.ftz.f32 	%f92, %f91;
	sub.ftz.f32 	%f93, %f87, %f90;
	abs.ftz.f32 	%f94, %f93;
	max.ftz.f32 	%f95, %f92, %f94;
	max.ftz.f32 	%f96, %f89, %f95;
	add.s32 	%r73, %r81, 2;
	setp.eq.s32 	%p30, %r73, %r11;
	selp.f32 	%f97, %f89, %f96, %p30;
	sub.ftz.f32 	%f98, %f97, %f86;
	fma.rn.ftz.f32 	%f99, %f98, %f15, %f86;
	add.s64 	%rd70, %rd66, %rd5;
	st.global.f32 	[%rd70], %f99;
	add.s64 	%rd71, %rd67, %rd5;
	add.s64 	%rd72, %rd68, %rd5;
	add.s64 	%rd73, %rd69, %rd5;
	ld.global.nc.f32 	%f100, [%rd72];
	ld.global.nc.f32 	%f101, [%rd71];
	sub.ftz.f32 	%f102, %f101, %f100;
	ld.global.nc.f32 	%f103, [%rd73];
	sub.ftz.f32 	%f104, %f101, %f103;
	abs.ftz.f32 	%f105, %f104;
	sub.ftz.f32 	%f106, %f100, %f103;
	abs.ftz.f32 	%f107, %f106;
	max.ftz.f32 	%f108, %f105, %f107;
	max.ftz.f32 	%f109, %f102, %f108;
	add.s32 	%r74, %r81, 3;
	setp.eq.s32 	%p31, %r74, %r11;
	selp.f32 	%f110, %f102, %f109, %p31;
	sub.ftz.f32 	%f111, %f110, %f99;
	fma.rn.ftz.f32 	%f114, %f111, %f15, %f99;
	add.s64 	%rd74, %rd70, %rd5;
	st.global.f32 	[%rd74], %f114;
	add.s32 	%r81, %r81, 4;
	setp.lt.s32 	%p32, %r81, %r39;
	@%p32 bra 	$L__BB5_24;

$L__BB5_25:
	add.s32 	%r75, %r75, %r5;
	setp.lt.s32 	%p33, %r75, %r38;
	@%p33 bra 	$L__BB5_3;

$L__BB5_26:
	ret;

}

