







.version 9.0
.target sm_89
.address_size 64

	
.func  (.param .align 8 .b8 func_retval0[16]) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0
)
;
.global .align 16 .b8 __cudart_sin_cos_coeffs[128] = {70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};
.global .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry supersmoother_3_pole_batch_f32(
	.param .u64 supersmoother_3_pole_batch_f32_param_0,
	.param .u64 supersmoother_3_pole_batch_f32_param_1,
	.param .u32 supersmoother_3_pole_batch_f32_param_2,
	.param .u32 supersmoother_3_pole_batch_f32_param_3,
	.param .u32 supersmoother_3_pole_batch_f32_param_4,
	.param .u64 supersmoother_3_pole_batch_f32_param_5
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<26>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<104>;
	.reg .f64 	%fd<147>;
	.reg .b64 	%rd<69>;


	ld.param.u64 	%rd23, [supersmoother_3_pole_batch_f32_param_0];
	ld.param.u64 	%rd24, [supersmoother_3_pole_batch_f32_param_1];
	ld.param.u32 	%r31, [supersmoother_3_pole_batch_f32_param_2];
	ld.param.u32 	%r33, [supersmoother_3_pole_batch_f32_param_3];
	ld.param.u32 	%r32, [supersmoother_3_pole_batch_f32_param_4];
	ld.param.u64 	%rd25, [supersmoother_3_pole_batch_f32_param_5];
	mov.u32 	%r34, %ctaid.x;
	mov.u32 	%r35, %ntid.x;
	mov.u32 	%r36, %tid.x;
	mad.lo.s32 	%r1, %r34, %r35, %r36;
	setp.ge.s32 	%p1, %r1, %r33;
	@%p1 bra 	$L__BB0_30;

	cvta.to.global.u64 	%rd26, %rd24;
	mul.wide.s32 	%rd27, %r1, 4;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.u32 	%r2, [%rd28];
	setp.lt.s32 	%p2, %r2, 1;
	setp.lt.s32 	%p3, %r31, 1;
	or.pred  	%p4, %p3, %p2;
	@%p4 bra 	$L__BB0_30;

	cvt.rn.f64.s32 	%fd40, %r2;
	rcp.rn.f64 	%fd1, %fd40;
	mov.f64 	%fd41, 0d3FF0000000000000;
	mul.f64 	%fd2, %fd1, 0dC00921FB54442D18;
	mov.f64 	%fd42, 0d4338000000000000;
	mov.f64 	%fd43, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd44, %fd2, %fd43, %fd42;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd44;
	}
	mov.f64 	%fd45, 0dC338000000000000;
	add.rn.f64 	%fd46, %fd44, %fd45;
	mov.f64 	%fd47, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd48, %fd46, %fd47, %fd2;
	mov.f64 	%fd49, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd50, %fd46, %fd49, %fd48;
	mov.f64 	%fd51, 0d3E928AF3FCA213EA;
	mov.f64 	%fd52, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd53, %fd52, %fd50, %fd51;
	mov.f64 	%fd54, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd55, %fd53, %fd50, %fd54;
	mov.f64 	%fd56, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd57, %fd55, %fd50, %fd56;
	mov.f64 	%fd58, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd59, %fd57, %fd50, %fd58;
	mov.f64 	%fd60, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd61, %fd59, %fd50, %fd60;
	mov.f64 	%fd62, 0d3F81111111122322;
	fma.rn.f64 	%fd63, %fd61, %fd50, %fd62;
	mov.f64 	%fd64, 0d3FA55555555502A1;
	fma.rn.f64 	%fd65, %fd63, %fd50, %fd64;
	mov.f64 	%fd66, 0d3FC5555555555511;
	fma.rn.f64 	%fd67, %fd65, %fd50, %fd66;
	mov.f64 	%fd68, 0d3FE000000000000B;
	fma.rn.f64 	%fd69, %fd67, %fd50, %fd68;
	fma.rn.f64 	%fd70, %fd69, %fd50, %fd41;
	fma.rn.f64 	%fd71, %fd70, %fd50, %fd41;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd71;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd71;
	}
	shl.b32 	%r37, %r3, 20;
	add.s32 	%r38, %r5, %r37;
	mov.b64 	%fd134, {%r4, %r38};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd2;
	}
	mov.b32 	%f4, %r39;
	abs.ftz.f32 	%f1, %f4;
	setp.lt.ftz.f32 	%p5, %f1, 0f4086232B;
	@%p5 bra 	$L__BB0_5;

	setp.lt.f64 	%p6, %fd2, 0d0000000000000000;
	add.f64 	%fd72, %fd2, 0d7FF0000000000000;
	selp.f64 	%fd134, 0d0000000000000000, %fd72, %p6;
	setp.geu.ftz.f32 	%p7, %f1, 0f40874800;
	@%p7 bra 	$L__BB0_5;

	shr.u32 	%r40, %r3, 31;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 1;
	shl.b32 	%r43, %r42, 20;
	add.s32 	%r44, %r5, %r43;
	mov.b64 	%fd73, {%r4, %r44};
	sub.s32 	%r45, %r3, %r42;
	shl.b32 	%r46, %r45, 20;
	add.s32 	%r47, %r46, 1072693248;
	mov.u32 	%r48, 0;
	mov.b64 	%fd74, {%r48, %r47};
	mul.f64 	%fd134, %fd73, %fd74;

$L__BB0_5:
	mul.f64 	%fd7, %fd1, 0d4015D7215129D64A;
	abs.f64 	%fd8, %fd7;
	setp.eq.f64 	%p8, %fd8, 0d7FF0000000000000;
	@%p8 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_6;

$L__BB0_8:
	mov.f64 	%fd83, 0d0000000000000000;
	mul.rn.f64 	%fd135, %fd7, %fd83;
	mov.u32 	%r95, 0;
	bra.uni 	$L__BB0_9;

$L__BB0_6:
	mul.f64 	%fd75, %fd7, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r95, %fd75;
	cvt.rn.f64.s32 	%fd76, %r95;
	neg.f64 	%fd77, %fd76;
	mov.f64 	%fd78, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd79, %fd77, %fd78, %fd7;
	mov.f64 	%fd80, 0d3C91A62633145C00;
	fma.rn.f64 	%fd81, %fd77, %fd80, %fd79;
	mov.f64 	%fd82, 0d397B839A252049C0;
	fma.rn.f64 	%fd135, %fd77, %fd82, %fd81;
	setp.ltu.f64 	%p9, %fd8, 0d41E0000000000000;
	@%p9 bra 	$L__BB0_9;

	{ 
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd7;
	.param .align 8 .b8 retval0[16];
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0
	);
	ld.param.f64 	%fd135, [retval0+0];
	ld.param.b32 	%r95, [retval0+8];
	} 

$L__BB0_9:
	add.s32 	%r9, %r95, 1;
	and.b32  	%r50, %r9, 1;
	shl.b32 	%r51, %r9, 3;
	and.b32  	%r52, %r51, 8;
	mul.wide.u32 	%rd29, %r52, 8;
	mov.u64 	%rd30, __cudart_sin_cos_coeffs;
	add.s64 	%rd31, %rd30, %rd29;
	setp.eq.s32 	%p10, %r50, 0;
	selp.f64 	%fd84, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p10;
	ld.global.nc.v2.f64 	{%fd85, %fd86}, [%rd31];
	mul.rn.f64 	%fd13, %fd135, %fd135;
	fma.rn.f64 	%fd89, %fd84, %fd13, %fd85;
	fma.rn.f64 	%fd90, %fd89, %fd13, %fd86;
	ld.global.nc.v2.f64 	{%fd91, %fd92}, [%rd31+16];
	fma.rn.f64 	%fd95, %fd90, %fd13, %fd91;
	fma.rn.f64 	%fd96, %fd95, %fd13, %fd92;
	ld.global.nc.v2.f64 	{%fd97, %fd98}, [%rd31+32];
	fma.rn.f64 	%fd101, %fd96, %fd13, %fd97;
	fma.rn.f64 	%fd14, %fd101, %fd13, %fd98;
	fma.rn.f64 	%fd137, %fd14, %fd135, %fd135;
	@%p10 bra 	$L__BB0_11;

	mov.f64 	%fd102, 0d3FF0000000000000;
	fma.rn.f64 	%fd137, %fd14, %fd13, %fd102;

$L__BB0_11:
	and.b32  	%r53, %r9, 2;
	setp.eq.s32 	%p11, %r53, 0;
	@%p11 bra 	$L__BB0_13;

	mov.f64 	%fd103, 0d0000000000000000;
	mov.f64 	%fd104, 0dBFF0000000000000;
	fma.rn.f64 	%fd137, %fd137, %fd104, %fd103;

$L__BB0_13:
	mul.f64 	%fd105, %fd134, %fd134;
	mul.f64 	%fd20, %fd105, %fd105;
	mov.f64 	%fd106, 0d3FF0000000000000;
	sub.f64 	%fd107, %fd106, %fd20;
	add.f64 	%fd108, %fd134, %fd134;
	mul.f64 	%fd109, %fd108, %fd137;
	sub.f64 	%fd110, %fd107, %fd109;
	mul.f64 	%fd111, %fd105, %fd109;
	add.f64 	%fd21, %fd110, %fd111;
	add.f64 	%fd22, %fd105, %fd109;
	neg.f64 	%fd112, %fd105;
	sub.f64 	%fd23, %fd112, %fd111;
	setp.lt.s32 	%p12, %r32, 1;
	setp.gt.s32 	%p13, %r32, 0;
	selp.b32 	%r10, %r32, 0, %p13;
	@%p12 bra 	$L__BB0_20;

	neg.s32 	%r55, %r10;
	mov.u32 	%r98, 0;
	neg.s32 	%r56, %r31;
	max.u32 	%r11, %r55, %r56;
	neg.s32 	%r57, %r11;
	and.b32  	%r99, %r57, 3;
	setp.gt.u32 	%p14, %r11, -4;
	@%p14 bra 	$L__BB0_17;

	mul.wide.s32 	%rd32, %r31, %r1;
	cvta.to.global.u64 	%rd33, %rd25;
	shl.b64 	%rd34, %rd32, 2;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd63, %rd35, 8;
	add.s32 	%r63, %r11, %r99;
	neg.s32 	%r96, %r63;
	mov.u32 	%r98, 0;

$L__BB0_16:
	mov.u32 	%r64, 2147483647;
	st.global.u32 	[%rd63+-8], %r64;
	st.global.u32 	[%rd63+-4], %r64;
	st.global.u32 	[%rd63], %r64;
	st.global.u32 	[%rd63+4], %r64;
	add.s32 	%r98, %r98, 4;
	add.s64 	%rd63, %rd63, 16;
	add.s32 	%r96, %r96, -4;
	setp.ne.s32 	%p15, %r96, 0;
	@%p15 bra 	$L__BB0_16;

$L__BB0_17:
	setp.eq.s32 	%p16, %r99, 0;
	@%p16 bra 	$L__BB0_20;

	cvt.s64.s32 	%rd36, %r98;
	mul.wide.s32 	%rd37, %r31, %r1;
	add.s64 	%rd38, %rd37, %rd36;
	cvta.to.global.u64 	%rd39, %rd25;
	shl.b64 	%rd40, %rd38, 2;
	add.s64 	%rd64, %rd39, %rd40;

$L__BB0_19:
	.pragma "nounroll";
	mov.u32 	%r69, 2147483647;
	st.global.u32 	[%rd64], %r69;
	add.s64 	%rd64, %rd64, 4;
	add.s32 	%r99, %r99, -1;
	setp.ne.s32 	%p17, %r99, 0;
	@%p17 bra 	$L__BB0_19;

$L__BB0_20:
	setp.ge.s32 	%p18, %r10, %r31;
	@%p18 bra 	$L__BB0_30;

	cvt.s64.s32 	%rd41, %r10;
	cvta.to.global.u64 	%rd42, %rd23;
	mul.wide.s32 	%rd43, %r10, 4;
	add.s64 	%rd7, %rd42, %rd43;
	ld.global.nc.f32 	%f5, [%rd7];
	cvt.ftz.f64.f32 	%fd146, %f5;
	mul.wide.s32 	%rd44, %r31, %r1;
	add.s64 	%rd45, %rd44, %rd41;
	cvta.to.global.u64 	%rd46, %rd25;
	shl.b64 	%rd47, %rd45, 2;
	add.s64 	%rd8, %rd46, %rd47;
	st.global.f32 	[%rd8], %f5;
	add.s32 	%r74, %r10, 1;
	setp.ge.s32 	%p19, %r74, %r31;
	@%p19 bra 	$L__BB0_30;

	ld.global.nc.f32 	%f2, [%rd7+4];
	st.global.f32 	[%rd8+4], %f2;
	add.s32 	%r75, %r10, 2;
	setp.ge.s32 	%p20, %r75, %r31;
	@%p20 bra 	$L__BB0_30;

	ld.global.nc.f32 	%f3, [%rd7+8];
	st.global.f32 	[%rd8+8], %f3;
	max.s32 	%r76, %r32, 0;
	add.s32 	%r102, %r76, 3;
	setp.ge.s32 	%p21, %r102, %r31;
	@%p21 bra 	$L__BB0_30;

	cvt.ftz.f64.f32 	%fd141, %f3;
	cvt.ftz.f64.f32 	%fd142, %f2;
	add.s32 	%r78, %r31, 1;
	sub.s32 	%r79, %r78, %r76;
	and.b32  	%r101, %r79, 3;
	setp.eq.s32 	%p22, %r101, 0;
	@%p22 bra 	$L__BB0_27;

	add.s32 	%r102, %r76, 3;
	cvt.s64.s32 	%rd48, %r102;
	add.s64 	%rd50, %rd44, %rd48;
	shl.b64 	%rd52, %rd50, 2;
	add.s64 	%rd66, %rd46, %rd52;
	mul.wide.s32 	%rd54, %r102, 4;
	add.s64 	%rd65, %rd42, %rd54;
	mov.f64 	%fd140, %fd146;

$L__BB0_26:
	.pragma "nounroll";
	mov.f64 	%fd146, %fd142;
	mov.f64 	%fd142, %fd141;
	ld.global.nc.f32 	%f6, [%rd65];
	cvt.ftz.f64.f32 	%fd113, %f6;
	mul.f64 	%fd114, %fd21, %fd113;
	fma.rn.f64 	%fd115, %fd22, %fd142, %fd114;
	fma.rn.f64 	%fd116, %fd23, %fd146, %fd115;
	fma.rn.f64 	%fd141, %fd20, %fd140, %fd116;
	cvt.rn.ftz.f32.f64 	%f7, %fd141;
	st.global.f32 	[%rd66], %f7;
	add.s32 	%r102, %r102, 1;
	add.s64 	%rd66, %rd66, 4;
	add.s64 	%rd65, %rd65, 4;
	add.s32 	%r101, %r101, -1;
	setp.ne.s32 	%p23, %r101, 0;
	mov.f64 	%fd140, %fd146;
	@%p23 bra 	$L__BB0_26;

$L__BB0_27:
	add.s32 	%r89, %r31, -4;
	sub.s32 	%r90, %r89, %r76;
	setp.lt.u32 	%p24, %r90, 3;
	@%p24 bra 	$L__BB0_30;

	cvt.s64.s32 	%rd55, %r102;
	add.s64 	%rd57, %rd44, %rd55;
	shl.b64 	%rd59, %rd57, 2;
	add.s64 	%rd68, %rd46, %rd59;
	mul.wide.s32 	%rd61, %r102, 4;
	add.s64 	%rd62, %rd42, %rd61;
	add.s64 	%rd67, %rd62, 8;

$L__BB0_29:
	.pragma "nounroll";
	ld.global.nc.f32 	%f8, [%rd67+-8];
	cvt.ftz.f64.f32 	%fd117, %f8;
	mul.f64 	%fd118, %fd21, %fd117;
	fma.rn.f64 	%fd119, %fd22, %fd141, %fd118;
	fma.rn.f64 	%fd120, %fd23, %fd142, %fd119;
	fma.rn.f64 	%fd121, %fd20, %fd146, %fd120;
	cvt.rn.ftz.f32.f64 	%f9, %fd121;
	st.global.f32 	[%rd68], %f9;
	ld.global.nc.f32 	%f10, [%rd67+-4];
	cvt.ftz.f64.f32 	%fd122, %f10;
	mul.f64 	%fd123, %fd21, %fd122;
	fma.rn.f64 	%fd124, %fd22, %fd121, %fd123;
	fma.rn.f64 	%fd125, %fd23, %fd141, %fd124;
	fma.rn.f64 	%fd146, %fd20, %fd142, %fd125;
	cvt.rn.ftz.f32.f64 	%f11, %fd146;
	st.global.f32 	[%rd68+4], %f11;
	ld.global.nc.f32 	%f12, [%rd67];
	cvt.ftz.f64.f32 	%fd126, %f12;
	mul.f64 	%fd127, %fd21, %fd126;
	fma.rn.f64 	%fd128, %fd22, %fd146, %fd127;
	fma.rn.f64 	%fd129, %fd23, %fd121, %fd128;
	fma.rn.f64 	%fd142, %fd20, %fd141, %fd129;
	cvt.rn.ftz.f32.f64 	%f13, %fd142;
	st.global.f32 	[%rd68+8], %f13;
	ld.global.nc.f32 	%f14, [%rd67+4];
	cvt.ftz.f64.f32 	%fd130, %f14;
	mul.f64 	%fd131, %fd21, %fd130;
	fma.rn.f64 	%fd132, %fd22, %fd142, %fd131;
	fma.rn.f64 	%fd133, %fd23, %fd146, %fd132;
	fma.rn.f64 	%fd141, %fd20, %fd121, %fd133;
	cvt.rn.ftz.f32.f64 	%f15, %fd141;
	st.global.f32 	[%rd68+12], %f15;
	add.s64 	%rd68, %rd68, 16;
	add.s64 	%rd67, %rd67, 16;
	add.s32 	%r102, %r102, 4;
	setp.lt.s32 	%p25, %r102, %r31;
	@%p25 bra 	$L__BB0_29;

$L__BB0_30:
	ret;

}
	
.visible .entry supersmoother_3_pole_batch_f32_precomp(
	.param .u64 supersmoother_3_pole_batch_f32_precomp_param_0,
	.param .u64 supersmoother_3_pole_batch_f32_precomp_param_1,
	.param .u32 supersmoother_3_pole_batch_f32_precomp_param_2,
	.param .u32 supersmoother_3_pole_batch_f32_precomp_param_3,
	.param .u32 supersmoother_3_pole_batch_f32_precomp_param_4,
	.param .u64 supersmoother_3_pole_batch_f32_precomp_param_5
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<17>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<67>;
	.reg .f64 	%fd<51>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd22, [supersmoother_3_pole_batch_f32_precomp_param_0];
	ld.param.u64 	%rd23, [supersmoother_3_pole_batch_f32_precomp_param_1];
	ld.param.u32 	%r23, [supersmoother_3_pole_batch_f32_precomp_param_2];
	ld.param.u32 	%r25, [supersmoother_3_pole_batch_f32_precomp_param_3];
	ld.param.u32 	%r24, [supersmoother_3_pole_batch_f32_precomp_param_4];
	ld.param.u64 	%rd24, [supersmoother_3_pole_batch_f32_precomp_param_5];
	mov.u32 	%r26, %ntid.x;
	mov.u32 	%r27, %ctaid.x;
	mov.u32 	%r28, %tid.x;
	mad.lo.s32 	%r1, %r27, %r26, %r28;
	setp.ge.s32 	%p1, %r1, %r25;
	@%p1 bra 	$L__BB1_19;

	cvta.to.global.u64 	%rd25, %rd23;
	mul.wide.s32 	%rd26, %r1, 32;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.nc.f64 	%fd1, [%rd27];
	ld.global.nc.f64 	%fd2, [%rd27+8];
	ld.global.nc.f64 	%fd3, [%rd27+16];
	ld.global.nc.f64 	%fd4, [%rd27+24];
	mul.wide.s32 	%rd1, %r23, %r1;
	setp.lt.s32 	%p2, %r23, 1;
	@%p2 bra 	$L__BB1_19;

	setp.lt.s32 	%p3, %r24, 1;
	setp.gt.s32 	%p4, %r24, 0;
	selp.b32 	%r2, %r24, 0, %p4;
	@%p3 bra 	$L__BB1_9;

	neg.s32 	%r30, %r2;
	mov.u32 	%r61, 0;
	neg.s32 	%r31, %r23;
	max.u32 	%r3, %r30, %r31;
	neg.s32 	%r32, %r3;
	and.b32  	%r62, %r32, 3;
	setp.gt.u32 	%p5, %r3, -4;
	@%p5 bra 	$L__BB1_6;

	cvta.to.global.u64 	%rd28, %rd24;
	shl.b64 	%rd29, %rd1, 2;
	add.s64 	%rd30, %rd28, %rd29;
	add.s64 	%rd64, %rd30, 8;
	add.s32 	%r34, %r3, %r62;
	neg.s32 	%r59, %r34;
	mov.u32 	%r61, 0;

$L__BB1_5:
	mov.u32 	%r35, 2147483647;
	st.global.u32 	[%rd64+-8], %r35;
	st.global.u32 	[%rd64+-4], %r35;
	st.global.u32 	[%rd64], %r35;
	st.global.u32 	[%rd64+4], %r35;
	add.s32 	%r61, %r61, 4;
	add.s64 	%rd64, %rd64, 16;
	add.s32 	%r59, %r59, -4;
	setp.ne.s32 	%p6, %r59, 0;
	@%p6 bra 	$L__BB1_5;

$L__BB1_6:
	setp.eq.s32 	%p7, %r62, 0;
	@%p7 bra 	$L__BB1_9;

	cvt.s64.s32 	%rd31, %r61;
	add.s64 	%rd32, %rd1, %rd31;
	cvta.to.global.u64 	%rd33, %rd24;
	shl.b64 	%rd34, %rd32, 2;
	add.s64 	%rd65, %rd33, %rd34;

$L__BB1_8:
	.pragma "nounroll";
	mov.u32 	%r36, 2147483647;
	st.global.u32 	[%rd65], %r36;
	add.s64 	%rd65, %rd65, 4;
	add.s32 	%r62, %r62, -1;
	setp.ne.s32 	%p8, %r62, 0;
	@%p8 bra 	$L__BB1_8;

$L__BB1_9:
	setp.ge.s32 	%p9, %r2, %r23;
	@%p9 bra 	$L__BB1_19;

	cvt.s64.s32 	%rd35, %r2;
	cvta.to.global.u64 	%rd36, %rd22;
	mul.wide.s32 	%rd37, %r2, 4;
	add.s64 	%rd38, %rd36, %rd37;
	ld.global.nc.f32 	%f3, [%rd38];
	cvt.ftz.f64.f32 	%fd50, %f3;
	add.s64 	%rd39, %rd1, %rd35;
	cvta.to.global.u64 	%rd40, %rd24;
	shl.b64 	%rd41, %rd39, 2;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.f32 	[%rd42], %f3;
	add.s32 	%r37, %r2, 1;
	setp.ge.s32 	%p10, %r37, %r23;
	@%p10 bra 	$L__BB1_19;

	max.s32 	%r38, %r24, 0;
	cvt.s64.s32 	%rd43, %r38;
	mul.wide.s32 	%rd45, %r38, 4;
	add.s64 	%rd8, %rd36, %rd45;
	ld.global.nc.f32 	%f1, [%rd8+4];
	add.s64 	%rd46, %rd1, %rd43;
	shl.b64 	%rd48, %rd46, 2;
	add.s64 	%rd9, %rd40, %rd48;
	st.global.f32 	[%rd9+4], %f1;
	add.s32 	%r39, %r38, 2;
	setp.ge.s32 	%p11, %r39, %r23;
	@%p11 bra 	$L__BB1_19;

	ld.global.nc.f32 	%f2, [%rd8+8];
	st.global.f32 	[%rd9+8], %f2;
	add.s32 	%r65, %r38, 3;
	setp.ge.s32 	%p12, %r65, %r23;
	@%p12 bra 	$L__BB1_19;

	cvt.ftz.f64.f32 	%fd45, %f2;
	cvt.ftz.f64.f32 	%fd46, %f1;
	add.s32 	%r42, %r23, 1;
	sub.s32 	%r43, %r42, %r38;
	and.b32  	%r64, %r43, 3;
	setp.eq.s32 	%p13, %r64, 0;
	@%p13 bra 	$L__BB1_16;

	add.s32 	%r65, %r38, 3;
	cvt.s64.s32 	%rd49, %r65;
	add.s64 	%rd51, %rd1, %rd49;
	shl.b64 	%rd53, %rd51, 2;
	add.s64 	%rd67, %rd40, %rd53;
	mul.wide.s32 	%rd55, %r65, 4;
	add.s64 	%rd66, %rd36, %rd55;
	mov.f64 	%fd44, %fd50;

$L__BB1_15:
	.pragma "nounroll";
	mov.f64 	%fd50, %fd46;
	mov.f64 	%fd46, %fd45;
	ld.global.nc.f32 	%f4, [%rd66];
	cvt.ftz.f64.f32 	%fd21, %f4;
	mul.f64 	%fd22, %fd1, %fd21;
	fma.rn.f64 	%fd23, %fd2, %fd46, %fd22;
	fma.rn.f64 	%fd24, %fd3, %fd50, %fd23;
	fma.rn.f64 	%fd45, %fd4, %fd44, %fd24;
	cvt.rn.ftz.f32.f64 	%f5, %fd45;
	st.global.f32 	[%rd67], %f5;
	add.s32 	%r65, %r65, 1;
	add.s64 	%rd67, %rd67, 4;
	add.s64 	%rd66, %rd66, 4;
	add.s32 	%r64, %r64, -1;
	setp.ne.s32 	%p14, %r64, 0;
	mov.f64 	%fd44, %fd50;
	@%p14 bra 	$L__BB1_15;

$L__BB1_16:
	add.s32 	%r53, %r23, -4;
	sub.s32 	%r54, %r53, %r38;
	setp.lt.u32 	%p15, %r54, 3;
	@%p15 bra 	$L__BB1_19;

	cvt.s64.s32 	%rd56, %r65;
	add.s64 	%rd58, %rd1, %rd56;
	shl.b64 	%rd60, %rd58, 2;
	add.s64 	%rd69, %rd40, %rd60;
	mul.wide.s32 	%rd62, %r65, 4;
	add.s64 	%rd63, %rd36, %rd62;
	add.s64 	%rd68, %rd63, 8;

$L__BB1_18:
	.pragma "nounroll";
	ld.global.nc.f32 	%f6, [%rd68+-8];
	cvt.ftz.f64.f32 	%fd25, %f6;
	mul.f64 	%fd26, %fd1, %fd25;
	fma.rn.f64 	%fd27, %fd2, %fd45, %fd26;
	fma.rn.f64 	%fd28, %fd3, %fd46, %fd27;
	fma.rn.f64 	%fd29, %fd4, %fd50, %fd28;
	cvt.rn.ftz.f32.f64 	%f7, %fd29;
	st.global.f32 	[%rd69], %f7;
	ld.global.nc.f32 	%f8, [%rd68+-4];
	cvt.ftz.f64.f32 	%fd30, %f8;
	mul.f64 	%fd31, %fd1, %fd30;
	fma.rn.f64 	%fd32, %fd2, %fd29, %fd31;
	fma.rn.f64 	%fd33, %fd3, %fd45, %fd32;
	fma.rn.f64 	%fd50, %fd4, %fd46, %fd33;
	cvt.rn.ftz.f32.f64 	%f9, %fd50;
	st.global.f32 	[%rd69+4], %f9;
	ld.global.nc.f32 	%f10, [%rd68];
	cvt.ftz.f64.f32 	%fd34, %f10;
	mul.f64 	%fd35, %fd1, %fd34;
	fma.rn.f64 	%fd36, %fd2, %fd50, %fd35;
	fma.rn.f64 	%fd37, %fd3, %fd29, %fd36;
	fma.rn.f64 	%fd46, %fd4, %fd45, %fd37;
	cvt.rn.ftz.f32.f64 	%f11, %fd46;
	st.global.f32 	[%rd69+8], %f11;
	ld.global.nc.f32 	%f12, [%rd68+4];
	cvt.ftz.f64.f32 	%fd38, %f12;
	mul.f64 	%fd39, %fd1, %fd38;
	fma.rn.f64 	%fd40, %fd2, %fd46, %fd39;
	fma.rn.f64 	%fd41, %fd3, %fd50, %fd40;
	fma.rn.f64 	%fd45, %fd4, %fd29, %fd41;
	cvt.rn.ftz.f32.f64 	%f13, %fd45;
	st.global.f32 	[%rd69+12], %f13;
	add.s64 	%rd69, %rd69, 16;
	add.s64 	%rd68, %rd68, 16;
	add.s32 	%r65, %r65, 4;
	setp.lt.s32 	%p16, %r65, %r23;
	@%p16 bra 	$L__BB1_18;

$L__BB1_19:
	ret;

}
	
.visible .entry supersmoother_3_pole_batch_warp_scan_f32(
	.param .u64 supersmoother_3_pole_batch_warp_scan_f32_param_0,
	.param .u64 supersmoother_3_pole_batch_warp_scan_f32_param_1,
	.param .u32 supersmoother_3_pole_batch_warp_scan_f32_param_2,
	.param .u32 supersmoother_3_pole_batch_warp_scan_f32_param_3,
	.param .u32 supersmoother_3_pole_batch_warp_scan_f32_param_4,
	.param .u64 supersmoother_3_pole_batch_warp_scan_f32_param_5
)
{
	.reg .pred 	%p<187>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<395>;
	.reg .f64 	%fd<672>;
	.reg .b64 	%rd<89>;


	ld.param.u64 	%rd32, [supersmoother_3_pole_batch_warp_scan_f32_param_0];
	ld.param.u64 	%rd31, [supersmoother_3_pole_batch_warp_scan_f32_param_1];
	ld.param.u32 	%r48, [supersmoother_3_pole_batch_warp_scan_f32_param_2];
	ld.param.u32 	%r50, [supersmoother_3_pole_batch_warp_scan_f32_param_3];
	ld.param.u32 	%r49, [supersmoother_3_pole_batch_warp_scan_f32_param_4];
	ld.param.u64 	%rd33, [supersmoother_3_pole_batch_warp_scan_f32_param_5];
	cvta.to.global.u64 	%rd1, %rd33;
	cvta.to.global.u64 	%rd2, %rd32;
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r50;
	setp.lt.s32 	%p2, %r48, 1;
	or.pred  	%p3, %p2, %p1;
	mov.u32 	%r2, %tid.x;
	setp.gt.u32 	%p4, %r2, 31;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB2_60;
	bra.uni 	$L__BB2_1;

$L__BB2_60:
	ret;

$L__BB2_1:
	and.b32  	%r393, %r2, 31;
	cvt.s64.s32 	%rd3, %r1;
	mul.wide.s32 	%rd4, %r48, %r1;
	setp.ge.s32 	%p6, %r49, %r48;
	setp.lt.s32 	%p7, %r49, 0;
	or.pred  	%p8, %p7, %p6;
	@%p8 bra 	$L__BB2_53;
	bra.uni 	$L__BB2_2;

$L__BB2_53:
	setp.ge.s32 	%p177, %r393, %r48;
	@%p177 bra 	$L__BB2_60;

	not.b32 	%r369, %r393;
	add.s32 	%r39, %r369, %r48;
	shr.u32 	%r370, %r39, 5;
	add.s32 	%r371, %r370, 1;
	and.b32  	%r392, %r371, 3;
	setp.eq.s32 	%p178, %r392, 0;
	@%p178 bra 	$L__BB2_57;

	cvt.u64.u32 	%rd64, %r2;
	and.b64  	%rd65, %rd64, 31;
	add.s64 	%rd66, %rd4, %rd65;
	shl.b64 	%rd67, %rd66, 2;
	add.s64 	%rd87, %rd1, %rd67;

$L__BB2_56:
	.pragma "nounroll";
	mov.u32 	%r372, 2147483647;
	st.global.u32 	[%rd87], %r372;
	add.s32 	%r393, %r393, 32;
	add.s64 	%rd87, %rd87, 128;
	add.s32 	%r392, %r392, -1;
	setp.ne.s32 	%p179, %r392, 0;
	@%p179 bra 	$L__BB2_56;

$L__BB2_57:
	setp.lt.u32 	%p180, %r39, 96;
	@%p180 bra 	$L__BB2_60;

	cvt.u64.u32 	%rd68, %r393;
	add.s64 	%rd69, %rd4, %rd68;
	shl.b64 	%rd70, %rd69, 2;
	add.s64 	%rd71, %rd1, %rd70;
	add.s64 	%rd88, %rd71, 256;

$L__BB2_59:
	mov.u32 	%r373, 2147483647;
	st.global.u32 	[%rd88+-256], %r373;
	st.global.u32 	[%rd88+-128], %r373;
	st.global.u32 	[%rd88], %r373;
	st.global.u32 	[%rd88+128], %r373;
	add.s64 	%rd88, %rd88, 512;
	add.s32 	%r393, %r393, 128;
	setp.lt.s32 	%p181, %r393, %r48;
	@%p181 bra 	$L__BB2_59;
	bra.uni 	$L__BB2_60;

$L__BB2_2:
	cvta.to.global.u64 	%rd34, %rd31;
	shl.b64 	%rd35, %rd3, 2;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.nc.u32 	%r4, [%rd36];
	setp.lt.s32 	%p9, %r4, 1;
	setp.gt.s32 	%p10, %r4, %r48;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB2_46;
	bra.uni 	$L__BB2_3;

$L__BB2_46:
	setp.ge.s32 	%p172, %r393, %r48;
	@%p172 bra 	$L__BB2_60;

	not.b32 	%r364, %r393;
	add.s32 	%r30, %r364, %r48;
	shr.u32 	%r365, %r30, 5;
	add.s32 	%r366, %r365, 1;
	and.b32  	%r388, %r366, 3;
	setp.eq.s32 	%p173, %r388, 0;
	@%p173 bra 	$L__BB2_50;

	cvt.u64.u32 	%rd56, %r2;
	and.b64  	%rd57, %rd56, 31;
	add.s64 	%rd58, %rd4, %rd57;
	shl.b64 	%rd59, %rd58, 2;
	add.s64 	%rd85, %rd1, %rd59;

$L__BB2_49:
	.pragma "nounroll";
	mov.u32 	%r367, 2147483647;
	st.global.u32 	[%rd85], %r367;
	add.s32 	%r393, %r393, 32;
	add.s64 	%rd85, %rd85, 128;
	add.s32 	%r388, %r388, -1;
	setp.ne.s32 	%p174, %r388, 0;
	@%p174 bra 	$L__BB2_49;

$L__BB2_50:
	setp.lt.u32 	%p175, %r30, 96;
	@%p175 bra 	$L__BB2_60;

	cvt.u64.u32 	%rd60, %r393;
	add.s64 	%rd61, %rd4, %rd60;
	shl.b64 	%rd62, %rd61, 2;
	add.s64 	%rd63, %rd1, %rd62;
	add.s64 	%rd86, %rd63, 256;

$L__BB2_52:
	mov.u32 	%r368, 2147483647;
	st.global.u32 	[%rd86+-256], %r368;
	st.global.u32 	[%rd86+-128], %r368;
	st.global.u32 	[%rd86], %r368;
	st.global.u32 	[%rd86+128], %r368;
	add.s64 	%rd86, %rd86, 512;
	add.s32 	%r393, %r393, 128;
	setp.lt.s32 	%p176, %r393, %r48;
	@%p176 bra 	$L__BB2_52;
	bra.uni 	$L__BB2_60;

$L__BB2_3:
	setp.ge.s32 	%p12, %r393, %r49;
	@%p12 bra 	$L__BB2_10;

	not.b32 	%r51, %r393;
	add.s32 	%r5, %r51, %r49;
	shr.u32 	%r52, %r5, 5;
	add.s32 	%r53, %r52, 1;
	and.b32  	%r380, %r53, 3;
	setp.eq.s32 	%p13, %r380, 0;
	mov.u32 	%r381, %r393;
	@%p13 bra 	$L__BB2_7;

	cvt.u64.u32 	%rd37, %r2;
	and.b64  	%rd38, %rd37, 31;
	add.s64 	%rd39, %rd4, %rd38;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd81, %rd1, %rd40;
	mov.u32 	%r381, %r393;

$L__BB2_6:
	.pragma "nounroll";
	mov.u32 	%r54, 2147483647;
	st.global.u32 	[%rd81], %r54;
	add.s32 	%r381, %r381, 32;
	add.s64 	%rd81, %rd81, 128;
	add.s32 	%r380, %r380, -1;
	setp.ne.s32 	%p14, %r380, 0;
	@%p14 bra 	$L__BB2_6;

$L__BB2_7:
	setp.lt.u32 	%p15, %r5, 96;
	@%p15 bra 	$L__BB2_10;

	cvt.u64.u32 	%rd41, %r381;
	add.s64 	%rd42, %rd4, %rd41;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd1, %rd43;
	add.s64 	%rd82, %rd44, 256;

$L__BB2_9:
	mov.u32 	%r55, 2147483647;
	st.global.u32 	[%rd82+-256], %r55;
	st.global.u32 	[%rd82+-128], %r55;
	st.global.u32 	[%rd82], %r55;
	st.global.u32 	[%rd82+128], %r55;
	add.s64 	%rd82, %rd82, 512;
	add.s32 	%r381, %r381, 128;
	setp.lt.s32 	%p16, %r381, %r49;
	@%p16 bra 	$L__BB2_9;

$L__BB2_10:
	setp.eq.s32 	%p17, %r393, 0;
	mul.wide.s32 	%rd45, %r49, 4;
	add.s64 	%rd11, %rd2, %rd45;
	@%p17 bra 	$L__BB2_11;
	bra.uni 	$L__BB2_15;

$L__BB2_11:
	cvt.s64.s32 	%rd46, %r49;
	ld.global.nc.f32 	%f2, [%rd11];
	add.s64 	%rd47, %rd4, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd12, %rd1, %rd48;
	st.global.f32 	[%rd12], %f2;
	add.s32 	%r56, %r49, 1;
	setp.ge.s32 	%p18, %r56, %r48;
	@%p18 bra 	$L__BB2_13;

	ld.global.nc.f32 	%f3, [%rd11+4];
	st.global.f32 	[%rd12+4], %f3;

$L__BB2_13:
	add.s32 	%r57, %r49, 2;
	setp.ge.s32 	%p19, %r57, %r48;
	@%p19 bra 	$L__BB2_15;

	ld.global.nc.f32 	%f4, [%rd11+8];
	st.global.f32 	[%rd12+8], %f4;

$L__BB2_15:
	add.s32 	%r58, %r49, 2;
	setp.ge.s32 	%p20, %r58, %r48;
	@%p20 bra 	$L__BB2_60;

	cvt.rn.f64.s32 	%fd229, %r4;
	rcp.rn.f64 	%fd1, %fd229;
	mov.f64 	%fd230, 0d3FF0000000000000;
	mul.f64 	%fd2, %fd1, 0dC00921FB54442D18;
	mov.f64 	%fd231, 0d4338000000000000;
	mov.f64 	%fd232, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd233, %fd2, %fd232, %fd231;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd233;
	}
	mov.f64 	%fd234, 0dC338000000000000;
	add.rn.f64 	%fd235, %fd233, %fd234;
	mov.f64 	%fd236, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd237, %fd235, %fd236, %fd2;
	mov.f64 	%fd238, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd239, %fd235, %fd238, %fd237;
	mov.f64 	%fd240, 0d3E928AF3FCA213EA;
	mov.f64 	%fd241, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd242, %fd241, %fd239, %fd240;
	mov.f64 	%fd243, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd244, %fd242, %fd239, %fd243;
	mov.f64 	%fd245, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd246, %fd244, %fd239, %fd245;
	mov.f64 	%fd247, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd248, %fd246, %fd239, %fd247;
	mov.f64 	%fd249, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd250, %fd248, %fd239, %fd249;
	mov.f64 	%fd251, 0d3F81111111122322;
	fma.rn.f64 	%fd252, %fd250, %fd239, %fd251;
	mov.f64 	%fd253, 0d3FA55555555502A1;
	fma.rn.f64 	%fd254, %fd252, %fd239, %fd253;
	mov.f64 	%fd255, 0d3FC5555555555511;
	fma.rn.f64 	%fd256, %fd254, %fd239, %fd255;
	mov.f64 	%fd257, 0d3FE000000000000B;
	fma.rn.f64 	%fd258, %fd256, %fd239, %fd257;
	fma.rn.f64 	%fd259, %fd258, %fd239, %fd230;
	fma.rn.f64 	%fd260, %fd259, %fd239, %fd230;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r15, %temp}, %fd260;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r16}, %fd260;
	}
	shl.b32 	%r59, %r14, 20;
	add.s32 	%r60, %r16, %r59;
	mov.b64 	%fd601, {%r15, %r60};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd2;
	}
	mov.b32 	%f5, %r61;
	abs.ftz.f32 	%f1, %f5;
	setp.lt.ftz.f32 	%p21, %f1, 0f4086232B;
	@%p21 bra 	$L__BB2_19;

	mul.f64 	%fd595, %fd1, 0dC00921FB54442D18;
	setp.lt.f64 	%p22, %fd595, 0d0000000000000000;
	add.f64 	%fd261, %fd595, 0d7FF0000000000000;
	selp.f64 	%fd601, 0d0000000000000000, %fd261, %p22;
	setp.geu.ftz.f32 	%p23, %f1, 0f40874800;
	@%p23 bra 	$L__BB2_19;

	mul.f64 	%fd596, %fd1, 0dC00921FB54442D18;
	mov.f64 	%fd593, 0d4338000000000000;
	mov.f64 	%fd592, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd591, %fd596, %fd592, %fd593;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r374, %temp}, %fd591;
	}
	shr.u32 	%r62, %r374, 31;
	add.s32 	%r63, %r374, %r62;
	shr.s32 	%r64, %r63, 1;
	shl.b32 	%r65, %r64, 20;
	add.s32 	%r66, %r16, %r65;
	mov.b64 	%fd262, {%r15, %r66};
	sub.s32 	%r67, %r374, %r64;
	shl.b32 	%r68, %r67, 20;
	add.s32 	%r69, %r68, 1072693248;
	mov.u32 	%r70, 0;
	mov.b64 	%fd263, {%r70, %r69};
	mul.f64 	%fd601, %fd262, %fd263;

$L__BB2_19:
	mul.f64 	%fd7, %fd1, 0d4015D7215129D64A;
	abs.f64 	%fd8, %fd7;
	setp.eq.f64 	%p24, %fd8, 0d7FF0000000000000;
	@%p24 bra 	$L__BB2_22;
	bra.uni 	$L__BB2_20;

$L__BB2_22:
	mov.f64 	%fd272, 0d0000000000000000;
	mul.rn.f64 	%fd602, %fd7, %fd272;
	mov.u32 	%r383, 0;
	bra.uni 	$L__BB2_23;

$L__BB2_20:
	mul.f64 	%fd264, %fd7, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r383, %fd264;
	cvt.rn.f64.s32 	%fd265, %r383;
	neg.f64 	%fd266, %fd265;
	mov.f64 	%fd267, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd268, %fd266, %fd267, %fd7;
	mov.f64 	%fd269, 0d3C91A62633145C00;
	fma.rn.f64 	%fd270, %fd266, %fd269, %fd268;
	mov.f64 	%fd271, 0d397B839A252049C0;
	fma.rn.f64 	%fd602, %fd266, %fd271, %fd270;
	setp.ltu.f64 	%p25, %fd8, 0d41E0000000000000;
	@%p25 bra 	$L__BB2_23;

	{ 
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd7;
	.param .align 8 .b8 retval0[16];
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0
	);
	ld.param.f64 	%fd602, [retval0+0];
	ld.param.b32 	%r383, [retval0+8];
	} 

$L__BB2_23:
	add.s32 	%r20, %r383, 1;
	and.b32  	%r72, %r20, 1;
	shl.b32 	%r73, %r20, 3;
	and.b32  	%r74, %r73, 8;
	mul.wide.u32 	%rd49, %r74, 8;
	mov.u64 	%rd50, __cudart_sin_cos_coeffs;
	add.s64 	%rd51, %rd50, %rd49;
	setp.eq.s32 	%p26, %r72, 0;
	selp.f64 	%fd273, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p26;
	ld.global.nc.v2.f64 	{%fd274, %fd275}, [%rd51];
	mul.rn.f64 	%fd13, %fd602, %fd602;
	fma.rn.f64 	%fd278, %fd273, %fd13, %fd274;
	fma.rn.f64 	%fd279, %fd278, %fd13, %fd275;
	ld.global.nc.v2.f64 	{%fd280, %fd281}, [%rd51+16];
	fma.rn.f64 	%fd284, %fd279, %fd13, %fd280;
	fma.rn.f64 	%fd285, %fd284, %fd13, %fd281;
	ld.global.nc.v2.f64 	{%fd286, %fd287}, [%rd51+32];
	fma.rn.f64 	%fd290, %fd285, %fd13, %fd286;
	fma.rn.f64 	%fd14, %fd290, %fd13, %fd287;
	fma.rn.f64 	%fd604, %fd14, %fd602, %fd602;
	@%p26 bra 	$L__BB2_25;

	mov.f64 	%fd291, 0d3FF0000000000000;
	fma.rn.f64 	%fd604, %fd14, %fd13, %fd291;

$L__BB2_25:
	and.b32  	%r75, %r20, 2;
	setp.eq.s32 	%p27, %r75, 0;
	@%p27 bra 	$L__BB2_27;

	mov.f64 	%fd292, 0d0000000000000000;
	mov.f64 	%fd293, 0dBFF0000000000000;
	fma.rn.f64 	%fd604, %fd604, %fd293, %fd292;

$L__BB2_27:
	mul.f64 	%fd297, %fd601, %fd601;
	mul.f64 	%fd20, %fd297, %fd297;
	mov.f64 	%fd298, 0d3FF0000000000000;
	sub.f64 	%fd299, %fd298, %fd20;
	add.f64 	%fd300, %fd601, %fd601;
	mul.f64 	%fd301, %fd300, %fd604;
	sub.f64 	%fd302, %fd299, %fd301;
	mul.f64 	%fd303, %fd297, %fd301;
	add.f64 	%fd21, %fd302, %fd303;
	add.f64 	%fd22, %fd297, %fd301;
	neg.f64 	%fd304, %fd297;
	sub.f64 	%fd23, %fd304, %fd303;
	setp.ne.s32 	%p28, %r393, 0;
	mov.f64 	%fd605, 0d0000000000000000;
	mov.f64 	%fd606, %fd605;
	mov.f64 	%fd607, %fd605;
	@%p28 bra 	$L__BB2_29;

	ld.param.u64 	%rd80, [supersmoother_3_pole_batch_warp_scan_f32_param_0];
	ld.param.u32 	%r378, [supersmoother_3_pole_batch_warp_scan_f32_param_4];
	mul.wide.s32 	%rd79, %r378, 4;
	cvta.to.global.u64 	%rd78, %rd80;
	add.s64 	%rd77, %rd78, %rd79;
	ld.global.nc.f32 	%f6, [%rd77];
	cvt.ftz.f64.f32 	%fd607, %f6;
	ld.global.nc.f32 	%f7, [%rd77+4];
	cvt.ftz.f64.f32 	%fd606, %f7;
	ld.global.nc.f32 	%f8, [%rd77+8];
	cvt.ftz.f64.f32 	%fd605, %f8;

$L__BB2_29:
	ld.param.u32 	%r375, [supersmoother_3_pole_batch_warp_scan_f32_param_4];
	
	mov.b64 {%r76,%r77}, %fd605;
	
	mov.u32 	%r88, 31;
	mov.u32 	%r89, 0;
	mov.u32 	%r90, -1;
	shfl.sync.idx.b32 	%r79|%p29, %r77, %r89, %r88, %r90;
	shfl.sync.idx.b32 	%r78|%p30, %r76, %r89, %r88, %r90;
	
	mov.b64 %fd610, {%r78,%r79};
	
	
	mov.b64 {%r80,%r81}, %fd606;
	
	shfl.sync.idx.b32 	%r83|%p31, %r81, %r89, %r88, %r90;
	shfl.sync.idx.b32 	%r82|%p32, %r80, %r89, %r88, %r90;
	
	mov.b64 %fd609, {%r82,%r83};
	
	
	mov.b64 {%r84,%r85}, %fd607;
	
	shfl.sync.idx.b32 	%r87|%p33, %r85, %r89, %r88, %r90;
	shfl.sync.idx.b32 	%r86|%p34, %r84, %r89, %r88, %r90;
	
	mov.b64 %fd608, {%r86,%r87};
	
	add.s32 	%r386, %r375, 3;
	setp.ge.s32 	%p35, %r386, %r48;
	@%p35 bra 	$L__BB2_60;

	mov.u32 	%r377, %ctaid.x;
	mul.wide.s32 	%rd76, %r48, %r377;
	ld.param.u64 	%rd75, [supersmoother_3_pole_batch_warp_scan_f32_param_0];
	cvta.to.global.u64 	%rd74, %rd75;
	ld.param.u64 	%rd73, [supersmoother_3_pole_batch_warp_scan_f32_param_5];
	cvta.to.global.u64 	%rd72, %rd73;
	ld.param.u32 	%r376, [supersmoother_3_pole_batch_warp_scan_f32_param_4];
	add.s32 	%r91, %r48, -4;
	sub.s32 	%r385, %r91, %r376;
	add.s32 	%r92, %r376, %r393;
	add.s32 	%r384, %r92, 3;
	cvt.s64.s32 	%rd52, %r384;
	mul.wide.s32 	%rd53, %r384, 4;
	add.s64 	%rd84, %rd74, %rd53;
	add.s64 	%rd54, %rd76, %rd52;
	shl.b64 	%rd55, %rd54, 2;
	add.s64 	%rd83, %rd72, %rd55;

$L__BB2_31:
	setp.ge.s32 	%p36, %r384, %r48;
	setp.lt.s32 	%p37, %r384, %r48;
	selp.f64 	%fd624, %fd22, 0d3FF0000000000000, %p37;
	mov.f64 	%fd629, 0d0000000000000000;
	selp.f64 	%fd626, %fd20, 0d0000000000000000, %p37;
	mov.f64 	%fd621, %fd629;
	@%p36 bra 	$L__BB2_33;

	ld.global.nc.f32 	%f9, [%rd84];
	cvt.ftz.f64.f32 	%fd312, %f9;
	mul.f64 	%fd621, %fd21, %fd312;

$L__BB2_33:
	setp.lt.s32 	%p186, %r384, %r48;
	selp.f64 	%fd628, 0d0000000000000000, 0d3FF0000000000000, %p186;
	selp.f64 	%fd627, 0d3FF0000000000000, 0d0000000000000000, %p186;
	selp.f64 	%fd625, %fd23, 0d0000000000000000, %p186;
	
	mov.b64 {%r93,%r94}, %fd624;
	
	mov.u32 	%r141, 0;
	mov.u32 	%r142, 1;
	mov.u32 	%r143, -1;
	shfl.sync.up.b32 	%r96|%p38, %r94, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r95|%p39, %r93, %r142, %r141, %r143;
	
	mov.b64 %fd314, {%r95,%r96};
	
	
	mov.b64 {%r97,%r98}, %fd625;
	
	shfl.sync.up.b32 	%r100|%p40, %r98, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r99|%p41, %r97, %r142, %r141, %r143;
	
	mov.b64 %fd316, {%r99,%r100};
	
	
	mov.b64 {%r101,%r102}, %fd626;
	
	shfl.sync.up.b32 	%r104|%p42, %r102, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r103|%p43, %r101, %r142, %r141, %r143;
	
	mov.b64 %fd318, {%r103,%r104};
	
	
	mov.b64 {%r105,%r106}, %fd627;
	
	shfl.sync.up.b32 	%r108|%p44, %r106, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r107|%p45, %r105, %r142, %r141, %r143;
	
	mov.b64 %fd320, {%r107,%r108};
	
	
	mov.b64 {%r109,%r110}, %fd628;
	
	shfl.sync.up.b32 	%r112|%p46, %r110, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r111|%p47, %r109, %r142, %r141, %r143;
	
	mov.b64 %fd322, {%r111,%r112};
	
	
	mov.b64 {%r113,%r114}, %fd629;
	
	shfl.sync.up.b32 	%r116|%p48, %r114, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r115|%p49, %r113, %r142, %r141, %r143;
	
	mov.b64 %fd324, {%r115,%r116};
	
	
	mov.b64 {%r117,%r118}, %fd629;
	
	shfl.sync.up.b32 	%r120|%p50, %r118, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r119|%p51, %r117, %r142, %r141, %r143;
	
	mov.b64 %fd326, {%r119,%r120};
	
	
	mov.b64 {%r121,%r122}, %fd627;
	
	shfl.sync.up.b32 	%r124|%p52, %r122, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r123|%p53, %r121, %r142, %r141, %r143;
	
	mov.b64 %fd328, {%r123,%r124};
	
	
	mov.b64 {%r125,%r126}, %fd628;
	
	shfl.sync.up.b32 	%r128|%p54, %r126, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r127|%p55, %r125, %r142, %r141, %r143;
	
	mov.b64 %fd330, {%r127,%r128};
	
	
	mov.b64 {%r129,%r130}, %fd621;
	
	shfl.sync.up.b32 	%r132|%p56, %r130, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r131|%p57, %r129, %r142, %r141, %r143;
	
	mov.b64 %fd332, {%r131,%r132};
	
	
	mov.b64 {%r133,%r134}, %fd629;
	
	shfl.sync.up.b32 	%r136|%p58, %r134, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r135|%p59, %r133, %r142, %r141, %r143;
	
	mov.b64 %fd334, {%r135,%r136};
	
	
	mov.b64 {%r137,%r138}, %fd629;
	
	shfl.sync.up.b32 	%r140|%p60, %r138, %r142, %r141, %r143;
	shfl.sync.up.b32 	%r139|%p61, %r137, %r142, %r141, %r143;
	
	mov.b64 %fd336, {%r139,%r140};
	
	mov.f64 	%fd630, %fd629;
	mov.f64 	%fd631, %fd627;
	mov.f64 	%fd632, %fd628;
	mov.f64 	%fd634, %fd629;
	mov.f64 	%fd635, %fd629;
	@%p17 bra 	$L__BB2_35;

	setp.lt.s32 	%p185, %r384, %r48;
	selp.f64 	%fd597, %fd20, 0d0000000000000000, %p185;
	setp.lt.s32 	%p184, %r384, %r48;
	selp.f64 	%fd594, %fd22, 0d3FF0000000000000, %p184;
	setp.lt.s32 	%p182, %r384, %r48;
	selp.f64 	%fd590, 0d0000000000000000, 0d3FF0000000000000, %p182;
	selp.f64 	%fd589, 0d3FF0000000000000, 0d0000000000000000, %p182;
	selp.f64 	%fd588, %fd23, 0d0000000000000000, %p182;
	mul.f64 	%fd341, %fd597, %fd326;
	fma.rn.f64 	%fd342, %fd588, %fd320, %fd341;
	fma.rn.f64 	%fd624, %fd594, %fd314, %fd342;
	mul.f64 	%fd343, %fd597, %fd328;
	fma.rn.f64 	%fd344, %fd588, %fd322, %fd343;
	fma.rn.f64 	%fd625, %fd594, %fd316, %fd344;
	mul.f64 	%fd345, %fd597, %fd330;
	fma.rn.f64 	%fd346, %fd588, %fd324, %fd345;
	fma.rn.f64 	%fd626, %fd594, %fd318, %fd346;
	mul.f64 	%fd347, %fd326, 0d0000000000000000;
	mov.f64 	%fd348, 0d0000000000000000;
	fma.rn.f64 	%fd349, %fd590, %fd320, %fd347;
	fma.rn.f64 	%fd627, %fd589, %fd314, %fd349;
	mul.f64 	%fd350, %fd328, 0d0000000000000000;
	fma.rn.f64 	%fd351, %fd590, %fd322, %fd350;
	fma.rn.f64 	%fd628, %fd589, %fd316, %fd351;
	mul.f64 	%fd352, %fd330, 0d0000000000000000;
	fma.rn.f64 	%fd353, %fd590, %fd324, %fd352;
	fma.rn.f64 	%fd629, %fd589, %fd318, %fd353;
	mul.f64 	%fd354, %fd590, %fd326;
	fma.rn.f64 	%fd355, %fd589, %fd320, %fd354;
	fma.rn.f64 	%fd630, %fd348, %fd314, %fd355;
	mul.f64 	%fd356, %fd590, %fd328;
	fma.rn.f64 	%fd357, %fd589, %fd322, %fd356;
	fma.rn.f64 	%fd631, %fd348, %fd316, %fd357;
	mul.f64 	%fd358, %fd590, %fd330;
	fma.rn.f64 	%fd359, %fd589, %fd324, %fd358;
	fma.rn.f64 	%fd632, %fd348, %fd318, %fd359;
	mul.f64 	%fd360, %fd597, %fd336;
	fma.rn.f64 	%fd361, %fd588, %fd334, %fd360;
	fma.rn.f64 	%fd362, %fd594, %fd332, %fd361;
	mul.f64 	%fd363, %fd336, 0d0000000000000000;
	fma.rn.f64 	%fd364, %fd590, %fd334, %fd363;
	fma.rn.f64 	%fd365, %fd589, %fd332, %fd364;
	mul.f64 	%fd366, %fd590, %fd336;
	fma.rn.f64 	%fd367, %fd589, %fd334, %fd366;
	fma.rn.f64 	%fd368, %fd348, %fd332, %fd367;
	add.f64 	%fd621, %fd621, %fd362;
	add.f64 	%fd634, %fd365, 0d0000000000000000;
	add.f64 	%fd635, %fd368, 0d0000000000000000;

$L__BB2_35:
	
	mov.b64 {%r144,%r145}, %fd624;
	
	mov.u32 	%r193, 2;
	shfl.sync.up.b32 	%r147|%p63, %r145, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r146|%p64, %r144, %r193, %r141, %r143;
	
	mov.b64 %fd370, {%r146,%r147};
	
	
	mov.b64 {%r148,%r149}, %fd625;
	
	shfl.sync.up.b32 	%r151|%p65, %r149, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r150|%p66, %r148, %r193, %r141, %r143;
	
	mov.b64 %fd372, {%r150,%r151};
	
	
	mov.b64 {%r152,%r153}, %fd626;
	
	shfl.sync.up.b32 	%r155|%p67, %r153, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r154|%p68, %r152, %r193, %r141, %r143;
	
	mov.b64 %fd374, {%r154,%r155};
	
	
	mov.b64 {%r156,%r157}, %fd627;
	
	shfl.sync.up.b32 	%r159|%p69, %r157, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r158|%p70, %r156, %r193, %r141, %r143;
	
	mov.b64 %fd376, {%r158,%r159};
	
	
	mov.b64 {%r160,%r161}, %fd628;
	
	shfl.sync.up.b32 	%r163|%p71, %r161, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r162|%p72, %r160, %r193, %r141, %r143;
	
	mov.b64 %fd378, {%r162,%r163};
	
	
	mov.b64 {%r164,%r165}, %fd629;
	
	shfl.sync.up.b32 	%r167|%p73, %r165, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r166|%p74, %r164, %r193, %r141, %r143;
	
	mov.b64 %fd380, {%r166,%r167};
	
	
	mov.b64 {%r168,%r169}, %fd630;
	
	shfl.sync.up.b32 	%r171|%p75, %r169, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r170|%p76, %r168, %r193, %r141, %r143;
	
	mov.b64 %fd382, {%r170,%r171};
	
	
	mov.b64 {%r172,%r173}, %fd631;
	
	shfl.sync.up.b32 	%r175|%p77, %r173, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r174|%p78, %r172, %r193, %r141, %r143;
	
	mov.b64 %fd384, {%r174,%r175};
	
	
	mov.b64 {%r176,%r177}, %fd632;
	
	shfl.sync.up.b32 	%r179|%p79, %r177, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r178|%p80, %r176, %r193, %r141, %r143;
	
	mov.b64 %fd386, {%r178,%r179};
	
	
	mov.b64 {%r180,%r181}, %fd621;
	
	shfl.sync.up.b32 	%r183|%p81, %r181, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r182|%p82, %r180, %r193, %r141, %r143;
	
	mov.b64 %fd388, {%r182,%r183};
	
	
	mov.b64 {%r184,%r185}, %fd634;
	
	shfl.sync.up.b32 	%r187|%p83, %r185, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r186|%p84, %r184, %r193, %r141, %r143;
	
	mov.b64 %fd390, {%r186,%r187};
	
	
	mov.b64 {%r188,%r189}, %fd635;
	
	shfl.sync.up.b32 	%r191|%p85, %r189, %r193, %r141, %r143;
	shfl.sync.up.b32 	%r190|%p86, %r188, %r193, %r141, %r143;
	
	mov.b64 %fd392, {%r190,%r191};
	
	setp.lt.u32 	%p87, %r393, 2;
	@%p87 bra 	$L__BB2_37;

	mul.f64 	%fd393, %fd626, %fd382;
	fma.rn.f64 	%fd394, %fd625, %fd376, %fd393;
	fma.rn.f64 	%fd91, %fd624, %fd370, %fd394;
	mul.f64 	%fd395, %fd626, %fd384;
	fma.rn.f64 	%fd396, %fd625, %fd378, %fd395;
	fma.rn.f64 	%fd92, %fd624, %fd372, %fd396;
	mul.f64 	%fd397, %fd626, %fd386;
	fma.rn.f64 	%fd398, %fd625, %fd380, %fd397;
	fma.rn.f64 	%fd93, %fd624, %fd374, %fd398;
	mul.f64 	%fd399, %fd629, %fd382;
	fma.rn.f64 	%fd400, %fd628, %fd376, %fd399;
	fma.rn.f64 	%fd94, %fd627, %fd370, %fd400;
	mul.f64 	%fd401, %fd629, %fd384;
	fma.rn.f64 	%fd402, %fd628, %fd378, %fd401;
	fma.rn.f64 	%fd95, %fd627, %fd372, %fd402;
	mul.f64 	%fd403, %fd629, %fd386;
	fma.rn.f64 	%fd404, %fd628, %fd380, %fd403;
	fma.rn.f64 	%fd96, %fd627, %fd374, %fd404;
	mul.f64 	%fd405, %fd632, %fd382;
	fma.rn.f64 	%fd406, %fd631, %fd376, %fd405;
	fma.rn.f64 	%fd97, %fd630, %fd370, %fd406;
	mul.f64 	%fd407, %fd632, %fd384;
	fma.rn.f64 	%fd408, %fd631, %fd378, %fd407;
	fma.rn.f64 	%fd98, %fd630, %fd372, %fd408;
	mul.f64 	%fd409, %fd632, %fd386;
	fma.rn.f64 	%fd410, %fd631, %fd380, %fd409;
	fma.rn.f64 	%fd99, %fd630, %fd374, %fd410;
	mul.f64 	%fd411, %fd626, %fd392;
	fma.rn.f64 	%fd412, %fd625, %fd390, %fd411;
	fma.rn.f64 	%fd413, %fd624, %fd388, %fd412;
	mul.f64 	%fd414, %fd629, %fd392;
	fma.rn.f64 	%fd415, %fd628, %fd390, %fd414;
	fma.rn.f64 	%fd416, %fd627, %fd388, %fd415;
	mul.f64 	%fd417, %fd632, %fd392;
	fma.rn.f64 	%fd418, %fd631, %fd390, %fd417;
	fma.rn.f64 	%fd419, %fd630, %fd388, %fd418;
	add.f64 	%fd621, %fd621, %fd413;
	add.f64 	%fd634, %fd634, %fd416;
	add.f64 	%fd635, %fd635, %fd419;
	mov.f64 	%fd624, %fd91;
	mov.f64 	%fd625, %fd92;
	mov.f64 	%fd626, %fd93;
	mov.f64 	%fd627, %fd94;
	mov.f64 	%fd628, %fd95;
	mov.f64 	%fd629, %fd96;
	mov.f64 	%fd630, %fd97;
	mov.f64 	%fd631, %fd98;
	mov.f64 	%fd632, %fd99;

$L__BB2_37:
	
	mov.b64 {%r195,%r196}, %fd624;
	
	mov.u32 	%r243, 0;
	mov.u32 	%r244, 4;
	mov.u32 	%r245, -1;
	shfl.sync.up.b32 	%r198|%p88, %r196, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r197|%p89, %r195, %r244, %r243, %r245;
	
	mov.b64 %fd421, {%r197,%r198};
	
	
	mov.b64 {%r199,%r200}, %fd625;
	
	shfl.sync.up.b32 	%r202|%p90, %r200, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r201|%p91, %r199, %r244, %r243, %r245;
	
	mov.b64 %fd423, {%r201,%r202};
	
	
	mov.b64 {%r203,%r204}, %fd626;
	
	shfl.sync.up.b32 	%r206|%p92, %r204, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r205|%p93, %r203, %r244, %r243, %r245;
	
	mov.b64 %fd425, {%r205,%r206};
	
	
	mov.b64 {%r207,%r208}, %fd627;
	
	shfl.sync.up.b32 	%r210|%p94, %r208, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r209|%p95, %r207, %r244, %r243, %r245;
	
	mov.b64 %fd427, {%r209,%r210};
	
	
	mov.b64 {%r211,%r212}, %fd628;
	
	shfl.sync.up.b32 	%r214|%p96, %r212, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r213|%p97, %r211, %r244, %r243, %r245;
	
	mov.b64 %fd429, {%r213,%r214};
	
	
	mov.b64 {%r215,%r216}, %fd629;
	
	shfl.sync.up.b32 	%r218|%p98, %r216, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r217|%p99, %r215, %r244, %r243, %r245;
	
	mov.b64 %fd431, {%r217,%r218};
	
	
	mov.b64 {%r219,%r220}, %fd630;
	
	shfl.sync.up.b32 	%r222|%p100, %r220, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r221|%p101, %r219, %r244, %r243, %r245;
	
	mov.b64 %fd433, {%r221,%r222};
	
	
	mov.b64 {%r223,%r224}, %fd631;
	
	shfl.sync.up.b32 	%r226|%p102, %r224, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r225|%p103, %r223, %r244, %r243, %r245;
	
	mov.b64 %fd435, {%r225,%r226};
	
	
	mov.b64 {%r227,%r228}, %fd632;
	
	shfl.sync.up.b32 	%r230|%p104, %r228, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r229|%p105, %r227, %r244, %r243, %r245;
	
	mov.b64 %fd437, {%r229,%r230};
	
	
	mov.b64 {%r231,%r232}, %fd621;
	
	shfl.sync.up.b32 	%r234|%p106, %r232, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r233|%p107, %r231, %r244, %r243, %r245;
	
	mov.b64 %fd439, {%r233,%r234};
	
	
	mov.b64 {%r235,%r236}, %fd634;
	
	shfl.sync.up.b32 	%r238|%p108, %r236, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r237|%p109, %r235, %r244, %r243, %r245;
	
	mov.b64 %fd441, {%r237,%r238};
	
	
	mov.b64 {%r239,%r240}, %fd635;
	
	shfl.sync.up.b32 	%r242|%p110, %r240, %r244, %r243, %r245;
	shfl.sync.up.b32 	%r241|%p111, %r239, %r244, %r243, %r245;
	
	mov.b64 %fd443, {%r241,%r242};
	
	setp.lt.u32 	%p112, %r393, 4;
	@%p112 bra 	$L__BB2_39;

	mul.f64 	%fd444, %fd626, %fd433;
	fma.rn.f64 	%fd445, %fd625, %fd427, %fd444;
	fma.rn.f64 	%fd127, %fd624, %fd421, %fd445;
	mul.f64 	%fd446, %fd626, %fd435;
	fma.rn.f64 	%fd447, %fd625, %fd429, %fd446;
	fma.rn.f64 	%fd128, %fd624, %fd423, %fd447;
	mul.f64 	%fd448, %fd626, %fd437;
	fma.rn.f64 	%fd449, %fd625, %fd431, %fd448;
	fma.rn.f64 	%fd129, %fd624, %fd425, %fd449;
	mul.f64 	%fd450, %fd629, %fd433;
	fma.rn.f64 	%fd451, %fd628, %fd427, %fd450;
	fma.rn.f64 	%fd130, %fd627, %fd421, %fd451;
	mul.f64 	%fd452, %fd629, %fd435;
	fma.rn.f64 	%fd453, %fd628, %fd429, %fd452;
	fma.rn.f64 	%fd131, %fd627, %fd423, %fd453;
	mul.f64 	%fd454, %fd629, %fd437;
	fma.rn.f64 	%fd455, %fd628, %fd431, %fd454;
	fma.rn.f64 	%fd132, %fd627, %fd425, %fd455;
	mul.f64 	%fd456, %fd632, %fd433;
	fma.rn.f64 	%fd457, %fd631, %fd427, %fd456;
	fma.rn.f64 	%fd133, %fd630, %fd421, %fd457;
	mul.f64 	%fd458, %fd632, %fd435;
	fma.rn.f64 	%fd459, %fd631, %fd429, %fd458;
	fma.rn.f64 	%fd134, %fd630, %fd423, %fd459;
	mul.f64 	%fd460, %fd632, %fd437;
	fma.rn.f64 	%fd461, %fd631, %fd431, %fd460;
	fma.rn.f64 	%fd135, %fd630, %fd425, %fd461;
	mul.f64 	%fd462, %fd626, %fd443;
	fma.rn.f64 	%fd463, %fd625, %fd441, %fd462;
	fma.rn.f64 	%fd464, %fd624, %fd439, %fd463;
	mul.f64 	%fd465, %fd629, %fd443;
	fma.rn.f64 	%fd466, %fd628, %fd441, %fd465;
	fma.rn.f64 	%fd467, %fd627, %fd439, %fd466;
	mul.f64 	%fd468, %fd632, %fd443;
	fma.rn.f64 	%fd469, %fd631, %fd441, %fd468;
	fma.rn.f64 	%fd470, %fd630, %fd439, %fd469;
	add.f64 	%fd621, %fd621, %fd464;
	add.f64 	%fd634, %fd634, %fd467;
	add.f64 	%fd635, %fd635, %fd470;
	mov.f64 	%fd624, %fd127;
	mov.f64 	%fd625, %fd128;
	mov.f64 	%fd626, %fd129;
	mov.f64 	%fd627, %fd130;
	mov.f64 	%fd628, %fd131;
	mov.f64 	%fd629, %fd132;
	mov.f64 	%fd630, %fd133;
	mov.f64 	%fd631, %fd134;
	mov.f64 	%fd632, %fd135;

$L__BB2_39:
	
	mov.b64 {%r246,%r247}, %fd624;
	
	mov.u32 	%r295, 8;
	shfl.sync.up.b32 	%r249|%p113, %r247, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r248|%p114, %r246, %r295, %r243, %r245;
	
	mov.b64 %fd472, {%r248,%r249};
	
	
	mov.b64 {%r250,%r251}, %fd625;
	
	shfl.sync.up.b32 	%r253|%p115, %r251, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r252|%p116, %r250, %r295, %r243, %r245;
	
	mov.b64 %fd474, {%r252,%r253};
	
	
	mov.b64 {%r254,%r255}, %fd626;
	
	shfl.sync.up.b32 	%r257|%p117, %r255, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r256|%p118, %r254, %r295, %r243, %r245;
	
	mov.b64 %fd476, {%r256,%r257};
	
	
	mov.b64 {%r258,%r259}, %fd627;
	
	shfl.sync.up.b32 	%r261|%p119, %r259, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r260|%p120, %r258, %r295, %r243, %r245;
	
	mov.b64 %fd478, {%r260,%r261};
	
	
	mov.b64 {%r262,%r263}, %fd628;
	
	shfl.sync.up.b32 	%r265|%p121, %r263, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r264|%p122, %r262, %r295, %r243, %r245;
	
	mov.b64 %fd480, {%r264,%r265};
	
	
	mov.b64 {%r266,%r267}, %fd629;
	
	shfl.sync.up.b32 	%r269|%p123, %r267, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r268|%p124, %r266, %r295, %r243, %r245;
	
	mov.b64 %fd482, {%r268,%r269};
	
	
	mov.b64 {%r270,%r271}, %fd630;
	
	shfl.sync.up.b32 	%r273|%p125, %r271, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r272|%p126, %r270, %r295, %r243, %r245;
	
	mov.b64 %fd484, {%r272,%r273};
	
	
	mov.b64 {%r274,%r275}, %fd631;
	
	shfl.sync.up.b32 	%r277|%p127, %r275, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r276|%p128, %r274, %r295, %r243, %r245;
	
	mov.b64 %fd486, {%r276,%r277};
	
	
	mov.b64 {%r278,%r279}, %fd632;
	
	shfl.sync.up.b32 	%r281|%p129, %r279, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r280|%p130, %r278, %r295, %r243, %r245;
	
	mov.b64 %fd488, {%r280,%r281};
	
	
	mov.b64 {%r282,%r283}, %fd621;
	
	shfl.sync.up.b32 	%r285|%p131, %r283, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r284|%p132, %r282, %r295, %r243, %r245;
	
	mov.b64 %fd490, {%r284,%r285};
	
	
	mov.b64 {%r286,%r287}, %fd634;
	
	shfl.sync.up.b32 	%r289|%p133, %r287, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r288|%p134, %r286, %r295, %r243, %r245;
	
	mov.b64 %fd492, {%r288,%r289};
	
	
	mov.b64 {%r290,%r291}, %fd635;
	
	shfl.sync.up.b32 	%r293|%p135, %r291, %r295, %r243, %r245;
	shfl.sync.up.b32 	%r292|%p136, %r290, %r295, %r243, %r245;
	
	mov.b64 %fd494, {%r292,%r293};
	
	setp.lt.u32 	%p137, %r393, 8;
	@%p137 bra 	$L__BB2_41;

	mul.f64 	%fd495, %fd626, %fd484;
	fma.rn.f64 	%fd496, %fd625, %fd478, %fd495;
	fma.rn.f64 	%fd163, %fd624, %fd472, %fd496;
	mul.f64 	%fd497, %fd626, %fd486;
	fma.rn.f64 	%fd498, %fd625, %fd480, %fd497;
	fma.rn.f64 	%fd164, %fd624, %fd474, %fd498;
	mul.f64 	%fd499, %fd626, %fd488;
	fma.rn.f64 	%fd500, %fd625, %fd482, %fd499;
	fma.rn.f64 	%fd165, %fd624, %fd476, %fd500;
	mul.f64 	%fd501, %fd629, %fd484;
	fma.rn.f64 	%fd502, %fd628, %fd478, %fd501;
	fma.rn.f64 	%fd166, %fd627, %fd472, %fd502;
	mul.f64 	%fd503, %fd629, %fd486;
	fma.rn.f64 	%fd504, %fd628, %fd480, %fd503;
	fma.rn.f64 	%fd167, %fd627, %fd474, %fd504;
	mul.f64 	%fd505, %fd629, %fd488;
	fma.rn.f64 	%fd506, %fd628, %fd482, %fd505;
	fma.rn.f64 	%fd168, %fd627, %fd476, %fd506;
	mul.f64 	%fd507, %fd632, %fd484;
	fma.rn.f64 	%fd508, %fd631, %fd478, %fd507;
	fma.rn.f64 	%fd169, %fd630, %fd472, %fd508;
	mul.f64 	%fd509, %fd632, %fd486;
	fma.rn.f64 	%fd510, %fd631, %fd480, %fd509;
	fma.rn.f64 	%fd170, %fd630, %fd474, %fd510;
	mul.f64 	%fd511, %fd632, %fd488;
	fma.rn.f64 	%fd512, %fd631, %fd482, %fd511;
	fma.rn.f64 	%fd171, %fd630, %fd476, %fd512;
	mul.f64 	%fd513, %fd626, %fd494;
	fma.rn.f64 	%fd514, %fd625, %fd492, %fd513;
	fma.rn.f64 	%fd515, %fd624, %fd490, %fd514;
	mul.f64 	%fd516, %fd629, %fd494;
	fma.rn.f64 	%fd517, %fd628, %fd492, %fd516;
	fma.rn.f64 	%fd518, %fd627, %fd490, %fd517;
	mul.f64 	%fd519, %fd632, %fd494;
	fma.rn.f64 	%fd520, %fd631, %fd492, %fd519;
	fma.rn.f64 	%fd521, %fd630, %fd490, %fd520;
	add.f64 	%fd621, %fd621, %fd515;
	add.f64 	%fd634, %fd634, %fd518;
	add.f64 	%fd635, %fd635, %fd521;
	mov.f64 	%fd624, %fd163;
	mov.f64 	%fd625, %fd164;
	mov.f64 	%fd626, %fd165;
	mov.f64 	%fd627, %fd166;
	mov.f64 	%fd628, %fd167;
	mov.f64 	%fd629, %fd168;
	mov.f64 	%fd630, %fd169;
	mov.f64 	%fd631, %fd170;
	mov.f64 	%fd632, %fd171;

$L__BB2_41:
	
	mov.b64 {%r297,%r298}, %fd624;
	
	mov.u32 	%r345, 0;
	mov.u32 	%r346, 16;
	mov.u32 	%r347, -1;
	shfl.sync.up.b32 	%r300|%p138, %r298, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r299|%p139, %r297, %r346, %r345, %r347;
	
	mov.b64 %fd523, {%r299,%r300};
	
	
	mov.b64 {%r301,%r302}, %fd625;
	
	shfl.sync.up.b32 	%r304|%p140, %r302, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r303|%p141, %r301, %r346, %r345, %r347;
	
	mov.b64 %fd525, {%r303,%r304};
	
	
	mov.b64 {%r305,%r306}, %fd626;
	
	shfl.sync.up.b32 	%r308|%p142, %r306, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r307|%p143, %r305, %r346, %r345, %r347;
	
	mov.b64 %fd527, {%r307,%r308};
	
	
	mov.b64 {%r309,%r310}, %fd627;
	
	shfl.sync.up.b32 	%r312|%p144, %r310, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r311|%p145, %r309, %r346, %r345, %r347;
	
	mov.b64 %fd529, {%r311,%r312};
	
	
	mov.b64 {%r313,%r314}, %fd628;
	
	shfl.sync.up.b32 	%r316|%p146, %r314, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r315|%p147, %r313, %r346, %r345, %r347;
	
	mov.b64 %fd531, {%r315,%r316};
	
	
	mov.b64 {%r317,%r318}, %fd629;
	
	shfl.sync.up.b32 	%r320|%p148, %r318, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r319|%p149, %r317, %r346, %r345, %r347;
	
	mov.b64 %fd533, {%r319,%r320};
	
	
	mov.b64 {%r321,%r322}, %fd630;
	
	shfl.sync.up.b32 	%r324|%p150, %r322, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r323|%p151, %r321, %r346, %r345, %r347;
	
	mov.b64 %fd535, {%r323,%r324};
	
	
	mov.b64 {%r325,%r326}, %fd631;
	
	shfl.sync.up.b32 	%r328|%p152, %r326, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r327|%p153, %r325, %r346, %r345, %r347;
	
	mov.b64 %fd537, {%r327,%r328};
	
	
	mov.b64 {%r329,%r330}, %fd632;
	
	shfl.sync.up.b32 	%r332|%p154, %r330, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r331|%p155, %r329, %r346, %r345, %r347;
	
	mov.b64 %fd539, {%r331,%r332};
	
	
	mov.b64 {%r333,%r334}, %fd621;
	
	shfl.sync.up.b32 	%r336|%p156, %r334, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r335|%p157, %r333, %r346, %r345, %r347;
	
	mov.b64 %fd541, {%r335,%r336};
	
	
	mov.b64 {%r337,%r338}, %fd634;
	
	shfl.sync.up.b32 	%r340|%p158, %r338, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r339|%p159, %r337, %r346, %r345, %r347;
	
	mov.b64 %fd543, {%r339,%r340};
	
	
	mov.b64 {%r341,%r342}, %fd635;
	
	shfl.sync.up.b32 	%r344|%p160, %r342, %r346, %r345, %r347;
	shfl.sync.up.b32 	%r343|%p161, %r341, %r346, %r345, %r347;
	
	mov.b64 %fd545, {%r343,%r344};
	
	setp.lt.u32 	%p162, %r393, 16;
	@%p162 bra 	$L__BB2_43;

	mul.f64 	%fd546, %fd626, %fd535;
	fma.rn.f64 	%fd547, %fd625, %fd529, %fd546;
	fma.rn.f64 	%fd199, %fd624, %fd523, %fd547;
	mul.f64 	%fd548, %fd626, %fd537;
	fma.rn.f64 	%fd549, %fd625, %fd531, %fd548;
	fma.rn.f64 	%fd200, %fd624, %fd525, %fd549;
	mul.f64 	%fd550, %fd626, %fd539;
	fma.rn.f64 	%fd551, %fd625, %fd533, %fd550;
	fma.rn.f64 	%fd201, %fd624, %fd527, %fd551;
	mul.f64 	%fd552, %fd629, %fd535;
	fma.rn.f64 	%fd553, %fd628, %fd529, %fd552;
	fma.rn.f64 	%fd202, %fd627, %fd523, %fd553;
	mul.f64 	%fd554, %fd629, %fd537;
	fma.rn.f64 	%fd555, %fd628, %fd531, %fd554;
	fma.rn.f64 	%fd203, %fd627, %fd525, %fd555;
	mul.f64 	%fd556, %fd629, %fd539;
	fma.rn.f64 	%fd557, %fd628, %fd533, %fd556;
	fma.rn.f64 	%fd204, %fd627, %fd527, %fd557;
	mul.f64 	%fd558, %fd632, %fd535;
	fma.rn.f64 	%fd559, %fd631, %fd529, %fd558;
	fma.rn.f64 	%fd205, %fd630, %fd523, %fd559;
	mul.f64 	%fd560, %fd632, %fd537;
	fma.rn.f64 	%fd561, %fd631, %fd531, %fd560;
	fma.rn.f64 	%fd206, %fd630, %fd525, %fd561;
	mul.f64 	%fd562, %fd632, %fd539;
	fma.rn.f64 	%fd563, %fd631, %fd533, %fd562;
	fma.rn.f64 	%fd207, %fd630, %fd527, %fd563;
	mul.f64 	%fd564, %fd626, %fd545;
	fma.rn.f64 	%fd565, %fd625, %fd543, %fd564;
	fma.rn.f64 	%fd566, %fd624, %fd541, %fd565;
	mul.f64 	%fd567, %fd629, %fd545;
	fma.rn.f64 	%fd568, %fd628, %fd543, %fd567;
	fma.rn.f64 	%fd569, %fd627, %fd541, %fd568;
	mul.f64 	%fd570, %fd632, %fd545;
	fma.rn.f64 	%fd571, %fd631, %fd543, %fd570;
	fma.rn.f64 	%fd572, %fd630, %fd541, %fd571;
	add.f64 	%fd621, %fd621, %fd566;
	add.f64 	%fd634, %fd634, %fd569;
	add.f64 	%fd635, %fd635, %fd572;
	mov.f64 	%fd624, %fd199;
	mov.f64 	%fd625, %fd200;
	mov.f64 	%fd626, %fd201;
	mov.f64 	%fd627, %fd202;
	mov.f64 	%fd628, %fd203;
	mov.f64 	%fd629, %fd204;
	mov.f64 	%fd630, %fd205;
	mov.f64 	%fd631, %fd206;
	mov.f64 	%fd632, %fd207;

$L__BB2_43:
	setp.ge.s32 	%p183, %r384, %r48;
	mul.f64 	%fd573, %fd608, %fd626;
	fma.rn.f64 	%fd574, %fd625, %fd609, %fd573;
	fma.rn.f64 	%fd575, %fd624, %fd610, %fd574;
	mul.f64 	%fd576, %fd608, %fd629;
	fma.rn.f64 	%fd577, %fd628, %fd609, %fd576;
	fma.rn.f64 	%fd578, %fd627, %fd610, %fd577;
	mul.f64 	%fd579, %fd608, %fd632;
	fma.rn.f64 	%fd580, %fd631, %fd609, %fd579;
	fma.rn.f64 	%fd581, %fd630, %fd610, %fd580;
	add.f64 	%fd223, %fd621, %fd575;
	add.f64 	%fd224, %fd634, %fd578;
	add.f64 	%fd225, %fd635, %fd581;
	@%p183 bra 	$L__BB2_45;

	cvt.rn.ftz.f32.f64 	%f10, %fd223;
	st.global.f32 	[%rd83], %f10;

$L__BB2_45:
	add.s32 	%r360, %r385, 1;
	setp.gt.s32 	%p164, %r360, 31;
	mov.u32 	%r361, 31;
	selp.b32 	%r362, 31, %r385, %p164;
	
	mov.b64 {%r348,%r349}, %fd223;
	
	mov.u32 	%r363, -1;
	shfl.sync.idx.b32 	%r351|%p165, %r349, %r362, %r361, %r363;
	shfl.sync.idx.b32 	%r350|%p166, %r348, %r362, %r361, %r363;
	
	mov.b64 %fd610, {%r350,%r351};
	
	
	mov.b64 {%r352,%r353}, %fd224;
	
	shfl.sync.idx.b32 	%r355|%p167, %r353, %r362, %r361, %r363;
	shfl.sync.idx.b32 	%r354|%p168, %r352, %r362, %r361, %r363;
	
	mov.b64 %fd609, {%r354,%r355};
	
	
	mov.b64 {%r356,%r357}, %fd225;
	
	shfl.sync.idx.b32 	%r359|%p169, %r357, %r362, %r361, %r363;
	shfl.sync.idx.b32 	%r358|%p170, %r356, %r362, %r361, %r363;
	
	mov.b64 %fd608, {%r358,%r359};
	
	add.s32 	%r385, %r385, -32;
	add.s64 	%rd84, %rd84, 128;
	add.s64 	%rd83, %rd83, 128;
	add.s32 	%r384, %r384, 32;
	add.s32 	%r386, %r386, 32;
	setp.lt.s32 	%p171, %r386, %r48;
	@%p171 bra 	$L__BB2_31;
	bra.uni 	$L__BB2_60;

}
	
.visible .entry supersmoother_3_pole_many_series_one_param_time_major_f32(
	.param .u64 supersmoother_3_pole_many_series_one_param_time_major_f32_param_0,
	.param .u32 supersmoother_3_pole_many_series_one_param_time_major_f32_param_1,
	.param .u32 supersmoother_3_pole_many_series_one_param_time_major_f32_param_2,
	.param .u32 supersmoother_3_pole_many_series_one_param_time_major_f32_param_3,
	.param .u64 supersmoother_3_pole_many_series_one_param_time_major_f32_param_4,
	.param .u64 supersmoother_3_pole_many_series_one_param_time_major_f32_param_5
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<24>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<90>;
	.reg .f64 	%fd<147>;
	.reg .b64 	%rd<119>;


	ld.param.u64 	%rd35, [supersmoother_3_pole_many_series_one_param_time_major_f32_param_0];
	ld.param.u32 	%r27, [supersmoother_3_pole_many_series_one_param_time_major_f32_param_1];
	ld.param.u32 	%r28, [supersmoother_3_pole_many_series_one_param_time_major_f32_param_2];
	ld.param.u32 	%r29, [supersmoother_3_pole_many_series_one_param_time_major_f32_param_3];
	ld.param.u64 	%rd36, [supersmoother_3_pole_many_series_one_param_time_major_f32_param_4];
	ld.param.u64 	%rd37, [supersmoother_3_pole_many_series_one_param_time_major_f32_param_5];
	mov.u32 	%r30, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %tid.x;
	mad.lo.s32 	%r1, %r30, %r31, %r32;
	setp.ge.s32 	%p1, %r1, %r28;
	@%p1 bra 	$L__BB3_30;

	cvta.to.global.u64 	%rd38, %rd36;
	mul.wide.s32 	%rd39, %r1, 4;
	add.s64 	%rd40, %rd38, %rd39;
	ld.global.nc.u32 	%r2, [%rd40];
	cvt.rn.f64.s32 	%fd40, %r27;
	rcp.rn.f64 	%fd1, %fd40;
	mov.f64 	%fd41, 0d3FF0000000000000;
	mul.f64 	%fd2, %fd1, 0dC00921FB54442D18;
	mov.f64 	%fd42, 0d4338000000000000;
	mov.f64 	%fd43, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd44, %fd2, %fd43, %fd42;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r3, %temp}, %fd44;
	}
	mov.f64 	%fd45, 0dC338000000000000;
	add.rn.f64 	%fd46, %fd44, %fd45;
	mov.f64 	%fd47, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd48, %fd46, %fd47, %fd2;
	mov.f64 	%fd49, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd50, %fd46, %fd49, %fd48;
	mov.f64 	%fd51, 0d3E928AF3FCA213EA;
	mov.f64 	%fd52, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd53, %fd52, %fd50, %fd51;
	mov.f64 	%fd54, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd55, %fd53, %fd50, %fd54;
	mov.f64 	%fd56, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd57, %fd55, %fd50, %fd56;
	mov.f64 	%fd58, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd59, %fd57, %fd50, %fd58;
	mov.f64 	%fd60, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd61, %fd59, %fd50, %fd60;
	mov.f64 	%fd62, 0d3F81111111122322;
	fma.rn.f64 	%fd63, %fd61, %fd50, %fd62;
	mov.f64 	%fd64, 0d3FA55555555502A1;
	fma.rn.f64 	%fd65, %fd63, %fd50, %fd64;
	mov.f64 	%fd66, 0d3FC5555555555511;
	fma.rn.f64 	%fd67, %fd65, %fd50, %fd66;
	mov.f64 	%fd68, 0d3FE000000000000B;
	fma.rn.f64 	%fd69, %fd67, %fd50, %fd68;
	fma.rn.f64 	%fd70, %fd69, %fd50, %fd41;
	fma.rn.f64 	%fd71, %fd70, %fd50, %fd41;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd71;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd71;
	}
	shl.b32 	%r33, %r3, 20;
	add.s32 	%r34, %r5, %r33;
	mov.b64 	%fd134, {%r4, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd2;
	}
	mov.b32 	%f4, %r35;
	abs.ftz.f32 	%f1, %f4;
	setp.lt.ftz.f32 	%p2, %f1, 0f4086232B;
	@%p2 bra 	$L__BB3_4;

	setp.lt.f64 	%p3, %fd2, 0d0000000000000000;
	add.f64 	%fd72, %fd2, 0d7FF0000000000000;
	selp.f64 	%fd134, 0d0000000000000000, %fd72, %p3;
	setp.geu.ftz.f32 	%p4, %f1, 0f40874800;
	@%p4 bra 	$L__BB3_4;

	shr.u32 	%r36, %r3, 31;
	add.s32 	%r37, %r3, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r5, %r39;
	mov.b64 	%fd73, {%r4, %r40};
	sub.s32 	%r41, %r3, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd74, {%r44, %r43};
	mul.f64 	%fd134, %fd73, %fd74;

$L__BB3_4:
	mul.f64 	%fd7, %fd1, 0d4015D7215129D64A;
	abs.f64 	%fd8, %fd7;
	setp.eq.f64 	%p5, %fd8, 0d7FF0000000000000;
	@%p5 bra 	$L__BB3_7;
	bra.uni 	$L__BB3_5;

$L__BB3_7:
	mov.f64 	%fd83, 0d0000000000000000;
	mul.rn.f64 	%fd135, %fd7, %fd83;
	mov.u32 	%r83, 0;
	bra.uni 	$L__BB3_8;

$L__BB3_5:
	mul.f64 	%fd75, %fd7, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r83, %fd75;
	cvt.rn.f64.s32 	%fd76, %r83;
	neg.f64 	%fd77, %fd76;
	mov.f64 	%fd78, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd79, %fd77, %fd78, %fd7;
	mov.f64 	%fd80, 0d3C91A62633145C00;
	fma.rn.f64 	%fd81, %fd77, %fd80, %fd79;
	mov.f64 	%fd82, 0d397B839A252049C0;
	fma.rn.f64 	%fd135, %fd77, %fd82, %fd81;
	setp.ltu.f64 	%p6, %fd8, 0d41E0000000000000;
	@%p6 bra 	$L__BB3_8;

	{ 
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd7;
	.param .align 8 .b8 retval0[16];
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0
	);
	ld.param.f64 	%fd135, [retval0+0];
	ld.param.b32 	%r83, [retval0+8];
	} 

$L__BB3_8:
	add.s32 	%r9, %r83, 1;
	and.b32  	%r46, %r9, 1;
	shl.b32 	%r47, %r9, 3;
	and.b32  	%r48, %r47, 8;
	mul.wide.u32 	%rd41, %r48, 8;
	mov.u64 	%rd42, __cudart_sin_cos_coeffs;
	add.s64 	%rd43, %rd42, %rd41;
	setp.eq.s32 	%p7, %r46, 0;
	selp.f64 	%fd84, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p7;
	ld.global.nc.v2.f64 	{%fd85, %fd86}, [%rd43];
	mul.rn.f64 	%fd13, %fd135, %fd135;
	fma.rn.f64 	%fd89, %fd84, %fd13, %fd85;
	fma.rn.f64 	%fd90, %fd89, %fd13, %fd86;
	ld.global.nc.v2.f64 	{%fd91, %fd92}, [%rd43+16];
	fma.rn.f64 	%fd95, %fd90, %fd13, %fd91;
	fma.rn.f64 	%fd96, %fd95, %fd13, %fd92;
	ld.global.nc.v2.f64 	{%fd97, %fd98}, [%rd43+32];
	fma.rn.f64 	%fd101, %fd96, %fd13, %fd97;
	fma.rn.f64 	%fd14, %fd101, %fd13, %fd98;
	fma.rn.f64 	%fd137, %fd14, %fd135, %fd135;
	@%p7 bra 	$L__BB3_10;

	mov.f64 	%fd102, 0d3FF0000000000000;
	fma.rn.f64 	%fd137, %fd14, %fd13, %fd102;

$L__BB3_10:
	and.b32  	%r49, %r9, 2;
	setp.eq.s32 	%p8, %r49, 0;
	@%p8 bra 	$L__BB3_12;

	mov.f64 	%fd103, 0d0000000000000000;
	mov.f64 	%fd104, 0dBFF0000000000000;
	fma.rn.f64 	%fd137, %fd137, %fd104, %fd103;

$L__BB3_12:
	mul.f64 	%fd105, %fd134, %fd134;
	mul.f64 	%fd20, %fd105, %fd105;
	mov.f64 	%fd106, 0d3FF0000000000000;
	sub.f64 	%fd107, %fd106, %fd20;
	add.f64 	%fd108, %fd134, %fd134;
	mul.f64 	%fd109, %fd108, %fd137;
	sub.f64 	%fd110, %fd107, %fd109;
	mul.f64 	%fd111, %fd105, %fd109;
	add.f64 	%fd21, %fd110, %fd111;
	add.f64 	%fd22, %fd105, %fd109;
	neg.f64 	%fd112, %fd105;
	sub.f64 	%fd23, %fd112, %fd111;
	setp.lt.s32 	%p9, %r29, 1;
	@%p9 bra 	$L__BB3_30;

	setp.lt.s32 	%p10, %r2, 1;
	setp.gt.s32 	%p11, %r2, 0;
	selp.b32 	%r10, %r2, 0, %p11;
	@%p10 bra 	$L__BB3_20;

	neg.s32 	%r50, %r29;
	neg.s32 	%r51, %r10;
	max.u32 	%r11, %r51, %r50;
	neg.s32 	%r52, %r11;
	and.b32  	%r85, %r52, 3;
	setp.gt.u32 	%p12, %r11, -4;
	cvta.to.global.u64 	%rd44, %rd37;
	add.s64 	%rd111, %rd44, %rd39;
	@%p12 bra 	$L__BB3_17;

	add.s32 	%r57, %r11, %r85;
	neg.s32 	%r84, %r57;
	add.s64 	%rd109, %rd44, %rd39;
	mul.wide.s32 	%rd48, %r28, 4;

$L__BB3_16:
	mov.u32 	%r62, 2147483647;
	st.global.u32 	[%rd109], %r62;
	add.s64 	%rd49, %rd109, %rd48;
	st.global.u32 	[%rd49], %r62;
	add.s64 	%rd50, %rd49, %rd48;
	st.global.u32 	[%rd50], %r62;
	add.s64 	%rd51, %rd50, %rd48;
	add.s64 	%rd4, %rd51, %rd48;
	st.global.u32 	[%rd51], %r62;
	mul.wide.s32 	%rd52, %r28, 16;
	add.s64 	%rd111, %rd109, %rd52;
	add.s32 	%r84, %r84, -4;
	setp.ne.s32 	%p13, %r84, 0;
	mov.u64 	%rd109, %rd4;
	@%p13 bra 	$L__BB3_16;

$L__BB3_17:
	setp.eq.s32 	%p14, %r85, 0;
	@%p14 bra 	$L__BB3_20;

	mul.wide.s32 	%rd7, %r28, 4;

$L__BB3_19:
	.pragma "nounroll";
	mov.u32 	%r63, 2147483647;
	st.global.u32 	[%rd111], %r63;
	add.s64 	%rd111, %rd111, %rd7;
	add.s32 	%r85, %r85, -1;
	setp.ne.s32 	%p15, %r85, 0;
	@%p15 bra 	$L__BB3_19;

$L__BB3_20:
	setp.ge.s32 	%p16, %r10, %r29;
	@%p16 bra 	$L__BB3_30;

	cvt.s64.s32 	%rd10, %r10;
	cvt.s64.s32 	%rd11, %r28;
	mul.wide.s32 	%rd53, %r10, %r28;
	cvt.s64.s32 	%rd54, %r1;
	add.s64 	%rd55, %rd53, %rd54;
	cvta.to.global.u64 	%rd56, %rd35;
	shl.b64 	%rd57, %rd55, 2;
	add.s64 	%rd12, %rd56, %rd57;
	cvta.to.global.u64 	%rd58, %rd37;
	add.s64 	%rd60, %rd58, %rd39;
	shl.b64 	%rd61, %rd53, 2;
	add.s64 	%rd13, %rd60, %rd61;
	ld.global.nc.f32 	%f5, [%rd12];
	cvt.ftz.f64.f32 	%fd146, %f5;
	st.global.f32 	[%rd13], %f5;
	add.s32 	%r68, %r10, 1;
	setp.ge.s32 	%p17, %r68, %r29;
	@%p17 bra 	$L__BB3_30;

	shl.b64 	%rd62, %rd11, 2;
	add.s64 	%rd14, %rd12, %rd62;
	add.s64 	%rd15, %rd13, %rd62;
	ld.global.nc.f32 	%f2, [%rd14];
	st.global.f32 	[%rd15], %f2;
	add.s32 	%r69, %r10, 2;
	setp.ge.s32 	%p18, %r69, %r29;
	@%p18 bra 	$L__BB3_30;

	add.s64 	%rd64, %rd14, %rd62;
	ld.global.nc.f32 	%f3, [%rd64];
	add.s64 	%rd65, %rd15, %rd62;
	st.global.f32 	[%rd65], %f3;
	add.s32 	%r88, %r10, 3;
	setp.ge.s32 	%p19, %r88, %r29;
	@%p19 bra 	$L__BB3_30;

	cvt.ftz.f64.f32 	%fd141, %f3;
	cvt.ftz.f64.f32 	%fd142, %f2;
	add.s32 	%r70, %r29, 1;
	sub.s32 	%r71, %r70, %r10;
	and.b32  	%r87, %r71, 3;
	setp.eq.s32 	%p20, %r87, 0;
	mul.lo.s64 	%rd67, %rd10, %rd11;
	add.s64 	%rd68, %rd67, %rd54;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd56, %rd70;
	add.s64 	%rd73, %rd71, %rd62;
	add.s64 	%rd74, %rd73, %rd62;
	add.s64 	%rd115, %rd74, %rd62;
	shl.b64 	%rd78, %rd67, 2;
	add.s64 	%rd79, %rd60, %rd78;
	add.s64 	%rd80, %rd79, %rd62;
	add.s64 	%rd81, %rd80, %rd62;
	add.s64 	%rd116, %rd81, %rd62;
	@%p20 bra 	$L__BB3_27;

	mul.wide.s32 	%rd18, %r28, 4;
	sub.s32 	%r80, %r28, %r1;
	mul.wide.s32 	%rd84, %r80, 4;
	sub.s64 	%rd85, %rd58, %rd84;
	shl.b64 	%rd86, %rd10, 2;
	add.s64 	%rd87, %rd86, 16;
	mul.lo.s64 	%rd88, %rd87, %rd11;
	add.s64 	%rd116, %rd85, %rd88;
	add.s64 	%rd95, %rd71, %rd18;
	add.s64 	%rd96, %rd95, %rd18;
	add.s64 	%rd115, %rd96, %rd18;
	add.s64 	%rd101, %rd79, %rd18;
	add.s64 	%rd102, %rd101, %rd18;
	add.s64 	%rd112, %rd102, %rd18;
	mov.f64 	%fd140, %fd146;

$L__BB3_26:
	.pragma "nounroll";
	mov.f64 	%fd146, %fd142;
	mov.f64 	%fd142, %fd141;
	ld.global.nc.f32 	%f6, [%rd115];
	cvt.ftz.f64.f32 	%fd113, %f6;
	mul.f64 	%fd114, %fd21, %fd113;
	fma.rn.f64 	%fd115, %fd22, %fd142, %fd114;
	fma.rn.f64 	%fd116, %fd23, %fd146, %fd115;
	fma.rn.f64 	%fd141, %fd20, %fd140, %fd116;
	cvt.rn.ftz.f32.f64 	%f7, %fd141;
	st.global.f32 	[%rd112], %f7;
	add.s32 	%r88, %r88, 1;
	add.s64 	%rd115, %rd115, %rd18;
	add.s64 	%rd116, %rd116, %rd18;
	add.s64 	%rd112, %rd112, %rd18;
	add.s32 	%r87, %r87, -1;
	setp.ne.s32 	%p21, %r87, 0;
	mov.f64 	%fd140, %fd146;
	@%p21 bra 	$L__BB3_26;

$L__BB3_27:
	add.s32 	%r81, %r29, -4;
	sub.s32 	%r82, %r81, %r10;
	setp.lt.u32 	%p22, %r82, 3;
	@%p22 bra 	$L__BB3_30;

	mul.wide.s32 	%rd30, %r28, 4;

$L__BB3_29:
	.pragma "nounroll";
	ld.global.nc.f32 	%f8, [%rd115];
	cvt.ftz.f64.f32 	%fd117, %f8;
	mul.f64 	%fd118, %fd21, %fd117;
	fma.rn.f64 	%fd119, %fd22, %fd141, %fd118;
	fma.rn.f64 	%fd120, %fd23, %fd142, %fd119;
	fma.rn.f64 	%fd121, %fd20, %fd146, %fd120;
	cvt.rn.ftz.f32.f64 	%f9, %fd121;
	st.global.f32 	[%rd116], %f9;
	add.s64 	%rd103, %rd115, %rd30;
	ld.global.nc.f32 	%f10, [%rd103];
	cvt.ftz.f64.f32 	%fd122, %f10;
	mul.f64 	%fd123, %fd21, %fd122;
	fma.rn.f64 	%fd124, %fd22, %fd121, %fd123;
	fma.rn.f64 	%fd125, %fd23, %fd141, %fd124;
	fma.rn.f64 	%fd146, %fd20, %fd142, %fd125;
	cvt.rn.ftz.f32.f64 	%f11, %fd146;
	add.s64 	%rd104, %rd116, %rd30;
	st.global.f32 	[%rd104], %f11;
	add.s64 	%rd105, %rd103, %rd30;
	ld.global.nc.f32 	%f12, [%rd105];
	cvt.ftz.f64.f32 	%fd126, %f12;
	mul.f64 	%fd127, %fd21, %fd126;
	fma.rn.f64 	%fd128, %fd22, %fd146, %fd127;
	fma.rn.f64 	%fd129, %fd23, %fd121, %fd128;
	fma.rn.f64 	%fd142, %fd20, %fd141, %fd129;
	cvt.rn.ftz.f32.f64 	%f13, %fd142;
	add.s64 	%rd106, %rd104, %rd30;
	st.global.f32 	[%rd106], %f13;
	add.s64 	%rd107, %rd105, %rd30;
	add.s64 	%rd115, %rd107, %rd30;
	ld.global.nc.f32 	%f14, [%rd107];
	cvt.ftz.f64.f32 	%fd130, %f14;
	mul.f64 	%fd131, %fd21, %fd130;
	fma.rn.f64 	%fd132, %fd22, %fd142, %fd131;
	fma.rn.f64 	%fd133, %fd23, %fd146, %fd132;
	fma.rn.f64 	%fd141, %fd20, %fd121, %fd133;
	cvt.rn.ftz.f32.f64 	%f15, %fd141;
	add.s64 	%rd108, %rd106, %rd30;
	add.s64 	%rd116, %rd108, %rd30;
	st.global.f32 	[%rd108], %f15;
	add.s32 	%r88, %r88, 4;
	setp.lt.s32 	%p23, %r88, %r29;
	@%p23 bra 	$L__BB3_29;

$L__BB3_30:
	ret;

}
.func  (.param .align 8 .b8 func_retval0[16]) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0
)
{
	.local .align 8 .b8 	__local_depot4[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<77>;


	mov.u64 	%SPL, __local_depot4;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	add.u64 	%rd1, %SPL, 0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	shr.u32 	%r12, %r1, 20;
	and.b32  	%r2, %r12, 2047;
	setp.eq.s32 	%p1, %r2, 2047;
	mov.u32 	%r37, 0;
	@%p1 bra 	$L__BB4_7;

	add.s32 	%r13, %r2, -1024;
	shr.u32 	%r14, %r13, 6;
	mov.u32 	%r15, 16;
	sub.s32 	%r16, %r15, %r14;
	mov.u32 	%r17, 15;
	sub.s32 	%r3, %r17, %r14;
	mov.u32 	%r18, 19;
	sub.s32 	%r19, %r18, %r14;
	setp.gt.s32 	%p2, %r16, 14;
	selp.b32 	%r4, 18, %r19, %p2;
	setp.gt.s32 	%p3, %r16, %r4;
	mov.u64 	%rd74, 0;
	mov.u32 	%r36, %r3;
	@%p3 bra 	$L__BB4_4;

	mul.wide.s32 	%rd21, %r3, 8;
	mov.u64 	%rd22, __cudart_i2opi_d;
	add.s64 	%rd72, %rd22, %rd21;
	mov.b64 	%rd23, %fd4;
	shl.b64 	%rd24, %rd23, 11;
	or.b64  	%rd3, %rd24, -9223372036854775808;
	mov.u64 	%rd74, 0;
	mov.u64 	%rd71, %rd1;
	mov.u32 	%r36, %r3;

$L__BB4_3:
	.pragma "nounroll";
	ld.global.nc.u64 	%rd25, [%rd72];
	{
	.reg .u32 %r0, %r1, %r2, %r3, %alo, %ahi, %blo, %bhi, %clo, %chi;
	mov.b64 	{%alo,%ahi}, %rd25;
	mov.b64 	{%blo,%bhi}, %rd3;
	mov.b64 	{%clo,%chi}, %rd74;
	mad.lo.cc.u32 	%r0, %alo, %blo, %clo;
	madc.hi.cc.u32 	%r1, %alo, %blo, %chi;
	madc.hi.u32 	%r2, %alo, %bhi, 0;
	mad.lo.cc.u32 	%r1, %alo, %bhi, %r1;
	madc.hi.cc.u32 	%r2, %ahi, %blo, %r2;
	madc.hi.u32 	%r3, %ahi, %bhi, 0;
	mad.lo.cc.u32 	%r1, %ahi, %blo, %r1;
	madc.lo.cc.u32 	%r2, %ahi, %bhi, %r2;
	addc.u32 	%r3, %r3, 0;
	mov.b64 	%rd26, {%r0,%r1};
	mov.b64 	%rd74, {%r2,%r3};
	}
	st.local.u64 	[%rd71], %rd26;
	add.s64 	%rd72, %rd72, 8;
	add.s64 	%rd71, %rd71, 8;
	add.s32 	%r36, %r36, 1;
	setp.lt.s32 	%p4, %r36, %r4;
	@%p4 bra 	$L__BB4_3;

$L__BB4_4:
	sub.s32 	%r20, %r36, %r3;
	mul.wide.s32 	%rd27, %r20, 8;
	add.s64 	%rd28, %rd1, %rd27;
	st.local.u64 	[%rd28], %rd74;
	add.s32 	%r22, %r12, -1024;
	and.b32  	%r8, %r22, 63;
	ld.local.u64 	%rd76, [%rd1+16];
	ld.local.u64 	%rd75, [%rd1+24];
	setp.eq.s32 	%p5, %r8, 0;
	@%p5 bra 	$L__BB4_6;

	mov.u32 	%r23, 64;
	sub.s32 	%r24, %r23, %r8;
	shl.b64 	%rd29, %rd75, %r8;
	shr.u64 	%rd30, %rd76, %r24;
	or.b64  	%rd75, %rd29, %rd30;
	shl.b64 	%rd31, %rd76, %r8;
	ld.local.u64 	%rd32, [%rd1+8];
	shr.u64 	%rd33, %rd32, %r24;
	or.b64  	%rd76, %rd33, %rd31;

$L__BB4_6:
	shr.u64 	%rd34, %rd75, 62;
	cvt.u32.u64 	%r25, %rd34;
	shr.u64 	%rd35, %rd76, 62;
	shl.b64 	%rd36, %rd75, 2;
	or.b64  	%rd37, %rd35, %rd36;
	shr.u64 	%rd38, %rd75, 61;
	cvt.u32.u64 	%r26, %rd38;
	and.b32  	%r27, %r26, 1;
	add.s32 	%r28, %r27, %r25;
	and.b32  	%r29, %r1, -2147483648;
	setp.eq.s32 	%p6, %r29, 0;
	neg.s32 	%r30, %r28;
	selp.b32 	%r37, %r28, %r30, %p6;
	setp.eq.s32 	%p7, %r27, 0;
	shl.b64 	%rd39, %rd76, 2;
	mov.u64 	%rd40, 0;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %a0, %a1, %a2, %a3, %b0, %b1, %b2, %b3;
	mov.b64 	{%a0,%a1}, %rd40;
	mov.b64 	{%a2,%a3}, %rd40;
	mov.b64 	{%b0,%b1}, %rd39;
	mov.b64 	{%b2,%b3}, %rd37;
	sub.cc.u32 	%r0, %a0, %b0;
	subc.cc.u32 	%r1, %a1, %b1;
	subc.cc.u32 	%r2, %a2, %b2;
	subc.u32 	%r3, %a3, %b3;
	mov.b64 	%rd41, {%r0,%r1};
	mov.b64 	%rd42, {%r2,%r3};
	}
	xor.b32  	%r31, %r29, -2147483648;
	selp.b64 	%rd43, %rd37, %rd42, %p7;
	selp.b64 	%rd44, %rd39, %rd41, %p7;
	selp.b32 	%r32, %r29, %r31, %p7;
	clz.b64 	%r33, %rd43;
	cvt.u64.u32 	%rd45, %r33;
	setp.eq.s64 	%p8, %rd45, 0;
	shl.b64 	%rd46, %rd43, %r33;
	mov.u64 	%rd47, 64;
	sub.s64 	%rd48, %rd47, %rd45;
	cvt.u32.u64 	%r34, %rd48;
	shr.u64 	%rd49, %rd44, %r34;
	or.b64  	%rd50, %rd49, %rd46;
	selp.b64 	%rd51, %rd43, %rd50, %p8;
	mov.u64 	%rd52, -3958705157555305931;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %alo, %ahi, %blo, %bhi;
	mov.b64 	{%alo,%ahi}, %rd51;
	mov.b64 	{%blo,%bhi}, %rd52;
	mul.lo.u32 	%r0, %alo, %blo;
	mul.hi.u32 	%r1, %alo, %blo;
	mad.lo.cc.u32 	%r1, %alo, %bhi, %r1;
	madc.hi.u32 	%r2, %alo, %bhi, 0;
	mad.lo.cc.u32 	%r1, %ahi, %blo, %r1;
	madc.hi.cc.u32 	%r2, %ahi, %blo, %r2;
	madc.hi.u32 	%r3, %ahi, %bhi, 0;
	mad.lo.cc.u32 	%r2, %ahi, %bhi, %r2;
	addc.u32 	%r3, %r3, 0;
	mov.b64 	%rd53, {%r0,%r1};
	mov.b64 	%rd54, {%r2,%r3};
	}
	setp.gt.s64 	%p9, %rd54, 0;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %a0, %a1, %a2, %a3, %b0, %b1, %b2, %b3;
	mov.b64 	{%a0,%a1}, %rd53;
	mov.b64 	{%a2,%a3}, %rd54;
	mov.b64 	{%b0,%b1}, %rd53;
	mov.b64 	{%b2,%b3}, %rd54;
	add.cc.u32 	%r0, %a0, %b0;
	addc.cc.u32 	%r1, %a1, %b1;
	addc.cc.u32 	%r2, %a2, %b2;
	addc.u32 	%r3, %a3, %b3;
	mov.b64 	%rd55, {%r0,%r1};
	mov.b64 	%rd56, {%r2,%r3};
	}
	selp.b64 	%rd57, %rd56, %rd54, %p9;
	selp.u64 	%rd58, 1, 0, %p9;
	add.s64 	%rd59, %rd45, %rd58;
	cvt.u64.u32 	%rd60, %r32;
	shl.b64 	%rd61, %rd60, 32;
	shl.b64 	%rd62, %rd59, 52;
	mov.u64 	%rd63, 4602678819172646912;
	sub.s64 	%rd64, %rd63, %rd62;
	add.s64 	%rd65, %rd57, 1;
	shr.u64 	%rd66, %rd65, 10;
	add.s64 	%rd67, %rd66, 1;
	shr.u64 	%rd68, %rd67, 1;
	add.s64 	%rd69, %rd64, %rd68;
	or.b64  	%rd70, %rd69, %rd61;
	mov.b64 	%fd4, %rd70;

$L__BB4_7:
	st.param.f64 	[func_retval0+0], %fd4;
	st.param.b32 	[func_retval0+8], %r37;
	ret;

}

