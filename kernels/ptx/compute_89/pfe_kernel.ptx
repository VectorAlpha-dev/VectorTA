//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_89
.address_size 64

	// .globl	pfe_batch_f32

.visible .entry pfe_batch_f32(
	.param .u64 pfe_batch_f32_param_0,
	.param .u32 pfe_batch_f32_param_1,
	.param .u32 pfe_batch_f32_param_2,
	.param .u64 pfe_batch_f32_param_3,
	.param .u64 pfe_batch_f32_param_4,
	.param .u32 pfe_batch_f32_param_5,
	.param .u64 pfe_batch_f32_param_6
)
{
	.local .align 16 .b8 	__local_depot0[1024];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<28>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<97>;
	.reg .b32 	%r<96>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd30, [pfe_batch_f32_param_0];
	ld.param.u32 	%r47, [pfe_batch_f32_param_1];
	ld.param.u32 	%r48, [pfe_batch_f32_param_2];
	ld.param.u64 	%rd28, [pfe_batch_f32_param_3];
	ld.param.u64 	%rd29, [pfe_batch_f32_param_4];
	ld.param.u32 	%r49, [pfe_batch_f32_param_5];
	ld.param.u64 	%rd31, [pfe_batch_f32_param_6];
	cvta.to.global.u64 	%rd1, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r50, %ntid.x;
	mov.u32 	%r51, %ctaid.x;
	mov.u32 	%r52, %tid.x;
	mad.lo.s32 	%r1, %r51, %r50, %r52;
	setp.ge.s32 	%p1, %r1, %r49;
	@%p1 bra 	$L__BB0_38;

	cvta.to.global.u64 	%rd33, %rd28;
	mul.wide.s32 	%rd34, %r1, 4;
	add.s64 	%rd35, %rd33, %rd34;
	cvta.to.global.u64 	%rd36, %rd29;
	add.s64 	%rd37, %rd36, %rd34;
	ld.global.nc.u32 	%r2, [%rd35];
	setp.lt.s32 	%p2, %r2, 1;
	ld.global.nc.u32 	%r3, [%rd37];
	setp.lt.s32 	%p3, %r3, 1;
	or.pred  	%p4, %p2, %p3;
	setp.gt.s32 	%p5, %r2, %r47;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB0_38;

	add.s32 	%r93, %r2, %r48;
	min.s32 	%r5, %r93, %r47;
	setp.lt.s32 	%p7, %r5, 1;
	@%p7 bra 	$L__BB0_9;

	add.s32 	%r54, %r5, -1;
	and.b32  	%r79, %r5, 3;
	setp.lt.u32 	%p8, %r54, 3;
	mov.u32 	%r78, 0;
	@%p8 bra 	$L__BB0_6;

	mul.lo.s32 	%r56, %r47, %r1;
	mul.wide.s32 	%rd38, %r56, 4;
	add.s64 	%rd64, %rd1, %rd38;
	not.b32 	%r57, %r93;
	not.b32 	%r58, %r47;
	max.s32 	%r59, %r58, %r57;
	add.s32 	%r60, %r59, %r79;
	neg.s32 	%r76, %r60;
	mov.u32 	%r78, 0;

$L__BB0_5:
	mov.u32 	%r61, 2147483647;
	st.global.u32 	[%rd64], %r61;
	st.global.u32 	[%rd64+4], %r61;
	st.global.u32 	[%rd64+8], %r61;
	st.global.u32 	[%rd64+12], %r61;
	add.s32 	%r78, %r78, 4;
	add.s64 	%rd64, %rd64, 16;
	add.s32 	%r76, %r76, -4;
	setp.ne.s32 	%p9, %r76, 1;
	@%p9 bra 	$L__BB0_5;

$L__BB0_6:
	setp.eq.s32 	%p10, %r79, 0;
	@%p10 bra 	$L__BB0_9;

	mad.lo.s32 	%r62, %r47, %r1, %r78;
	mul.wide.s32 	%rd39, %r62, 4;
	add.s64 	%rd65, %rd1, %rd39;

$L__BB0_8:
	.pragma "nounroll";
	mov.u32 	%r63, 2147483647;
	st.global.u32 	[%rd65], %r63;
	add.s64 	%rd65, %rd65, 4;
	add.s32 	%r79, %r79, -1;
	setp.ne.s32 	%p11, %r79, 0;
	@%p11 bra 	$L__BB0_8;

$L__BB0_9:
	setp.ge.s32 	%p12, %r93, %r47;
	@%p12 bra 	$L__BB0_38;

	cvt.rn.f32.s32 	%f43, %r3;
	add.ftz.f32 	%f44, %f43, 0f3F800000;
	mov.f32 	%f45, 0f40000000;
	div.approx.ftz.f32 	%f1, %f45, %f44;
	setp.gt.s32 	%p13, %r2, 0;
	@%p13 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_11;

$L__BB0_12:
	add.s32 	%r15, %r48, 1;
	max.s32 	%r65, %r15, %r93;
	sub.s32 	%r16, %r65, %r48;
	not.b32 	%r66, %r48;
	add.s32 	%r67, %r65, %r66;
	and.b32  	%r90, %r16, 3;
	setp.lt.u32 	%p14, %r67, 3;
	mov.f32 	%f90, 0f00000000;
	mov.u32 	%r83, 0;
	mov.u32 	%r87, %r48;
	@%p14 bra 	$L__BB0_23;

	sub.s32 	%r82, %r16, %r90;
	mul.wide.s32 	%rd40, %r48, 4;
	add.s64 	%rd41, %rd2, %rd40;
	ld.global.nc.f32 	%f84, [%rd41];
	mul.wide.s32 	%rd42, %r15, 4;
	add.s64 	%rd66, %rd2, %rd42;
	mov.f32 	%f90, 0f00000000;
	mov.u32 	%r83, 0;
	mov.u32 	%r87, %r48;

$L__BB0_14:
	ld.global.nc.f32 	%f5, [%rd66];
	sub.ftz.f32 	%f50, %f5, %f84;
	mov.f32 	%f51, 0f3F800000;
	fma.rn.ftz.f32 	%f52, %f50, %f50, %f51;
	sqrt.approx.ftz.f32 	%f6, %f52;
	setp.gt.s32 	%p15, %r2, 256;
	@%p15 bra 	$L__BB0_16;

	add.s32 	%r22, %r83, 1;
	mul.wide.s32 	%rd43, %r83, 4;
	add.s64 	%rd44, %rd3, %rd43;
	st.local.f32 	[%rd44], %f6;
	mov.u32 	%r83, %r22;

$L__BB0_16:
	ld.global.nc.f32 	%f7, [%rd66+4];
	sub.ftz.f32 	%f53, %f7, %f5;
	fma.rn.ftz.f32 	%f55, %f53, %f53, %f51;
	sqrt.approx.ftz.f32 	%f8, %f55;
	add.ftz.f32 	%f9, %f90, %f6;
	@%p15 bra 	$L__BB0_18;

	add.s32 	%r24, %r83, 1;
	mul.wide.s32 	%rd45, %r83, 4;
	add.s64 	%rd46, %rd3, %rd45;
	st.local.f32 	[%rd46], %f8;
	mov.u32 	%r83, %r24;

$L__BB0_18:
	ld.global.nc.f32 	%f10, [%rd66+8];
	sub.ftz.f32 	%f56, %f10, %f7;
	mov.f32 	%f57, 0f3F800000;
	fma.rn.ftz.f32 	%f58, %f56, %f56, %f57;
	sqrt.approx.ftz.f32 	%f11, %f58;
	add.ftz.f32 	%f12, %f9, %f8;
	@%p15 bra 	$L__BB0_20;

	add.s32 	%r26, %r83, 1;
	mul.wide.s32 	%rd47, %r83, 4;
	add.s64 	%rd48, %rd3, %rd47;
	st.local.f32 	[%rd48], %f11;
	mov.u32 	%r83, %r26;

$L__BB0_20:
	add.s32 	%r87, %r87, 4;
	ld.global.nc.f32 	%f84, [%rd66+12];
	sub.ftz.f32 	%f59, %f84, %f10;
	fma.rn.ftz.f32 	%f61, %f59, %f59, %f57;
	sqrt.approx.ftz.f32 	%f14, %f61;
	add.ftz.f32 	%f62, %f12, %f11;
	add.ftz.f32 	%f90, %f62, %f14;
	@%p15 bra 	$L__BB0_22;

	add.s32 	%r29, %r83, 1;
	mul.wide.s32 	%rd49, %r83, 4;
	add.s64 	%rd50, %rd3, %rd49;
	st.local.f32 	[%rd50], %f14;
	mov.u32 	%r83, %r29;

$L__BB0_22:
	add.s64 	%rd66, %rd66, 16;
	add.s32 	%r82, %r82, -4;
	setp.ne.s32 	%p19, %r82, 0;
	@%p19 bra 	$L__BB0_14;

$L__BB0_23:
	setp.eq.s32 	%p20, %r90, 0;
	@%p20 bra 	$L__BB0_28;

	mul.wide.s32 	%rd51, %r87, 4;
	add.s64 	%rd52, %rd2, %rd51;
	ld.global.nc.f32 	%f88, [%rd52];
	add.s32 	%r69, %r87, 1;
	mul.wide.s32 	%rd53, %r69, 4;
	add.s64 	%rd67, %rd2, %rd53;

$L__BB0_25:
	.pragma "nounroll";
	ld.global.nc.f32 	%f21, [%rd67];
	sub.ftz.f32 	%f63, %f21, %f88;
	mov.f32 	%f64, 0f3F800000;
	fma.rn.ftz.f32 	%f65, %f63, %f63, %f64;
	sqrt.approx.ftz.f32 	%f22, %f65;
	add.ftz.f32 	%f90, %f90, %f22;
	setp.gt.s32 	%p21, %r2, 256;
	@%p21 bra 	$L__BB0_27;

	add.s32 	%r36, %r83, 1;
	mul.wide.s32 	%rd54, %r83, 4;
	add.s64 	%rd55, %rd3, %rd54;
	st.local.f32 	[%rd55], %f22;
	mov.u32 	%r83, %r36;

$L__BB0_27:
	add.s64 	%rd67, %rd67, 4;
	add.s32 	%r90, %r90, -1;
	setp.ne.s32 	%p22, %r90, 0;
	mov.f32 	%f88, %f21;
	@%p22 bra 	$L__BB0_25;
	bra.uni 	$L__BB0_28;

$L__BB0_11:
	mov.f32 	%f90, 0f00000000;

$L__BB0_28:
	cvt.rn.f32.s32 	%f67, %r2;
	mul.wide.s32 	%rd56, %r93, 4;
	add.s64 	%rd57, %rd2, %rd56;
	ld.global.nc.f32 	%f91, [%rd57];
	add.s32 	%r71, %r48, 2;
	mul.wide.s32 	%rd58, %r71, 4;
	add.s64 	%rd71, %rd2, %rd58;
	neg.s32 	%r72, %r48;
	mov.u32 	%r94, 0;
	mul.wide.s32 	%rd59, %r72, 4;
	sub.s64 	%rd70, %rd2, %rd59;
	add.s32 	%r73, %r93, 1;
	mul.wide.s32 	%rd60, %r73, 4;
	add.s64 	%rd69, %rd2, %rd60;
	sub.s32 	%r92, %r93, %r47;
	mad.lo.s32 	%r74, %r47, %r1, %r93;
	mul.wide.s32 	%rd61, %r74, 4;
	add.s64 	%rd68, %rd1, %rd61;
	mov.f32 	%f68, 0f3F800000;
	sub.ftz.f32 	%f26, %f68, %f1;
	mul.ftz.f32 	%f27, %f67, %f67;
	mov.u16 	%rs6, 0;
	mov.f32 	%f66, 0f00000000;
	mov.f32 	%f92, %f66;

$L__BB0_29:
	.pragma "nounroll";
	ld.global.nc.f32 	%f70, [%rd70];
	sub.ftz.f32 	%f31, %f91, %f70;
	setp.leu.ftz.f32 	%p23, %f90, 0f00000000;
	mov.f32 	%f94, %f66;
	@%p23 bra 	$L__BB0_31;

	fma.rn.ftz.f32 	%f71, %f31, %f31, %f27;
	sqrt.approx.ftz.f32 	%f72, %f71;
	div.approx.ftz.f32 	%f73, %f72, %f90;
	mul.ftz.f32 	%f94, %f73, 0f42C80000;

$L__BB0_31:
	copysign.f32 	%f95, %f31, %f94;
	and.b16  	%rs5, %rs6, 255;
	setp.eq.s16 	%p24, %rs5, 0;
	mov.u16 	%rs7, 1;
	@%p24 bra 	$L__BB0_33;

	mul.ftz.f32 	%f74, %f26, %f92;
	fma.rn.ftz.f32 	%f95, %f1, %f95, %f74;
	mov.u16 	%rs7, %rs6;

$L__BB0_33:
	st.global.f32 	[%rd68], %f95;
	add.s32 	%r93, %r93, 1;
	setp.eq.s32 	%p25, %r92, -1;
	@%p25 bra 	$L__BB0_38;

	setp.lt.s32 	%p26, %r2, 257;
	ld.global.nc.f32 	%f37, [%rd69];
	sub.ftz.f32 	%f75, %f37, %f91;
	mov.f32 	%f76, 0f3F800000;
	fma.rn.ftz.f32 	%f77, %f75, %f75, %f76;
	sqrt.approx.ftz.f32 	%f38, %f77;
	@%p26 bra 	$L__BB0_36;
	bra.uni 	$L__BB0_35;

$L__BB0_36:
	mul.wide.s32 	%rd62, %r94, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.f32 	%f96, [%rd63];
	st.local.f32 	[%rd63], %f38;
	add.s32 	%r75, %r94, 1;
	rem.s32 	%r94, %r75, %r2;
	bra.uni 	$L__BB0_37;

$L__BB0_35:
	ld.global.nc.f32 	%f78, [%rd71];
	ld.global.nc.f32 	%f79, [%rd71+-4];
	sub.ftz.f32 	%f80, %f78, %f79;
	fma.rn.ftz.f32 	%f82, %f80, %f80, %f76;
	sqrt.approx.ftz.f32 	%f96, %f82;

$L__BB0_37:
	sub.ftz.f32 	%f83, %f38, %f96;
	add.ftz.f32 	%f90, %f90, %f83;
	add.s64 	%rd71, %rd71, 4;
	add.s64 	%rd70, %rd70, 4;
	add.s64 	%rd69, %rd69, 4;
	add.s32 	%r92, %r92, 1;
	add.s64 	%rd68, %rd68, 4;
	setp.lt.s32 	%p27, %r93, %r47;
	mov.f32 	%f91, %f37;
	mov.f32 	%f92, %f95;
	mov.u16 	%rs6, %rs7;
	@%p27 bra 	$L__BB0_29;

$L__BB0_38:
	ret;

}
	// .globl	pfe_batch_prefix_f32
.visible .entry pfe_batch_prefix_f32(
	.param .u64 pfe_batch_prefix_f32_param_0,
	.param .u64 pfe_batch_prefix_f32_param_1,
	.param .u32 pfe_batch_prefix_f32_param_2,
	.param .u32 pfe_batch_prefix_f32_param_3,
	.param .u64 pfe_batch_prefix_f32_param_4,
	.param .u64 pfe_batch_prefix_f32_param_5,
	.param .u32 pfe_batch_prefix_f32_param_6,
	.param .u64 pfe_batch_prefix_f32_param_7
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<49>;


	ld.param.u64 	%rd23, [pfe_batch_prefix_f32_param_0];
	ld.param.u64 	%rd24, [pfe_batch_prefix_f32_param_1];
	ld.param.u32 	%r17, [pfe_batch_prefix_f32_param_2];
	ld.param.u32 	%r18, [pfe_batch_prefix_f32_param_3];
	ld.param.u64 	%rd25, [pfe_batch_prefix_f32_param_4];
	ld.param.u64 	%rd26, [pfe_batch_prefix_f32_param_5];
	ld.param.u32 	%r19, [pfe_batch_prefix_f32_param_6];
	ld.param.u64 	%rd27, [pfe_batch_prefix_f32_param_7];
	cvta.to.global.u64 	%rd1, %rd27;
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r1, %r21, %r20, %r22;
	setp.ge.s32 	%p1, %r1, %r19;
	@%p1 bra 	$L__BB1_15;

	cvta.to.global.u64 	%rd28, %rd25;
	mul.wide.s32 	%rd29, %r1, 4;
	add.s64 	%rd30, %rd28, %rd29;
	cvta.to.global.u64 	%rd31, %rd26;
	add.s64 	%rd32, %rd31, %rd29;
	ld.global.nc.u32 	%r2, [%rd30];
	setp.lt.s32 	%p2, %r2, 1;
	ld.global.nc.u32 	%r3, [%rd32];
	setp.lt.s32 	%p3, %r3, 1;
	or.pred  	%p4, %p2, %p3;
	setp.gt.s32 	%p5, %r2, %r17;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB1_15;

	add.s32 	%r40, %r2, %r18;
	min.s32 	%r5, %r40, %r17;
	setp.lt.s32 	%p7, %r5, 1;
	@%p7 bra 	$L__BB1_9;

	add.s32 	%r24, %r5, -1;
	and.b32  	%r39, %r5, 3;
	setp.lt.u32 	%p8, %r24, 3;
	mov.u32 	%r38, 0;
	@%p8 bra 	$L__BB1_6;

	mul.lo.s32 	%r26, %r17, %r1;
	mul.wide.s32 	%rd33, %r26, 4;
	add.s64 	%rd42, %rd1, %rd33;
	not.b32 	%r27, %r40;
	not.b32 	%r28, %r17;
	max.s32 	%r29, %r28, %r27;
	add.s32 	%r30, %r29, %r39;
	neg.s32 	%r36, %r30;
	mov.u32 	%r38, 0;

$L__BB1_5:
	mov.u32 	%r31, 2147483647;
	st.global.u32 	[%rd42], %r31;
	st.global.u32 	[%rd42+4], %r31;
	st.global.u32 	[%rd42+8], %r31;
	st.global.u32 	[%rd42+12], %r31;
	add.s32 	%r38, %r38, 4;
	add.s64 	%rd42, %rd42, 16;
	add.s32 	%r36, %r36, -4;
	setp.ne.s32 	%p9, %r36, 1;
	@%p9 bra 	$L__BB1_5;

$L__BB1_6:
	setp.eq.s32 	%p10, %r39, 0;
	@%p10 bra 	$L__BB1_9;

	mad.lo.s32 	%r32, %r17, %r1, %r38;
	mul.wide.s32 	%rd34, %r32, 4;
	add.s64 	%rd43, %rd1, %rd34;

$L__BB1_8:
	.pragma "nounroll";
	mov.u32 	%r33, 2147483647;
	st.global.u32 	[%rd43], %r33;
	add.s64 	%rd43, %rd43, 4;
	add.s32 	%r39, %r39, -1;
	setp.ne.s32 	%p11, %r39, 0;
	@%p11 bra 	$L__BB1_8;

$L__BB1_9:
	setp.ge.s32 	%p12, %r40, %r17;
	@%p12 bra 	$L__BB1_15;

	cvta.to.global.u64 	%rd35, %rd24;
	cvta.to.global.u64 	%rd36, %rd23;
	cvt.rn.f32.s32 	%f10, %r2;
	mul.ftz.f32 	%f1, %f10, %f10;
	cvt.rn.f32.s32 	%f11, %r3;
	add.ftz.f32 	%f12, %f11, 0f3F800000;
	mov.f32 	%f13, 0f40000000;
	div.approx.ftz.f32 	%f2, %f13, %f12;
	neg.s32 	%r34, %r18;
	mul.wide.s32 	%rd37, %r34, 8;
	sub.s64 	%rd48, %rd35, %rd37;
	mul.wide.s32 	%rd38, %r34, 4;
	sub.s64 	%rd47, %rd36, %rd38;
	mul.wide.s32 	%rd39, %r40, 8;
	add.s64 	%rd46, %rd35, %rd39;
	mul.wide.s32 	%rd40, %r40, 4;
	add.s64 	%rd45, %rd36, %rd40;
	mad.lo.s32 	%r35, %r17, %r1, %r40;
	mul.wide.s32 	%rd41, %r35, 4;
	add.s64 	%rd44, %rd1, %rd41;
	mov.u16 	%rs7, 0;
	mov.f32 	%f26, 0f00000000;

$L__BB1_11:
	.pragma "nounroll";
	mov.u16 	%rs1, %rs7;
	mov.f32 	%f3, %f26;
	ld.global.nc.f64 	%fd1, [%rd48];
	ld.global.nc.f64 	%fd2, [%rd46];
	sub.f64 	%fd3, %fd2, %fd1;
	cvt.rn.ftz.f32.f64 	%f4, %fd3;
	setp.leu.ftz.f32 	%p13, %f4, 0f00000000;
	mov.f32 	%f27, 0f7FFFFFFF;
	@%p13 bra 	$L__BB1_14;

	ld.global.nc.f32 	%f15, [%rd47];
	ld.global.nc.f32 	%f16, [%rd45];
	sub.ftz.f32 	%f17, %f16, %f15;
	fma.rn.ftz.f32 	%f18, %f17, %f17, %f1;
	sqrt.approx.ftz.f32 	%f19, %f18;
	div.approx.ftz.f32 	%f20, %f19, %f4;
	mul.ftz.f32 	%f21, %f20, 0f42C80000;
	copysign.f32 	%f26, %f17, %f21;
	and.b16  	%rs5, %rs1, 255;
	setp.eq.s16 	%p14, %rs5, 0;
	mov.u16 	%rs7, 1;
	mov.f32 	%f27, %f26;
	@%p14 bra 	$L__BB1_14;

	mov.f32 	%f22, 0f3F800000;
	sub.ftz.f32 	%f23, %f22, %f2;
	mul.ftz.f32 	%f24, %f23, %f3;
	fma.rn.ftz.f32 	%f26, %f2, %f26, %f24;
	mov.u16 	%rs7, %rs1;
	mov.f32 	%f27, %f26;

$L__BB1_14:
	st.global.f32 	[%rd44], %f27;
	add.s64 	%rd48, %rd48, 8;
	add.s64 	%rd47, %rd47, 4;
	add.s64 	%rd46, %rd46, 8;
	add.s64 	%rd45, %rd45, 4;
	add.s64 	%rd44, %rd44, 4;
	add.s32 	%r40, %r40, 1;
	setp.lt.s32 	%p15, %r40, %r17;
	@%p15 bra 	$L__BB1_11;

$L__BB1_15:
	ret;

}
	// .globl	pfe_many_series_one_param_time_major_f32
.visible .entry pfe_many_series_one_param_time_major_f32(
	.param .u64 pfe_many_series_one_param_time_major_f32_param_0,
	.param .u64 pfe_many_series_one_param_time_major_f32_param_1,
	.param .u32 pfe_many_series_one_param_time_major_f32_param_2,
	.param .u32 pfe_many_series_one_param_time_major_f32_param_3,
	.param .u32 pfe_many_series_one_param_time_major_f32_param_4,
	.param .u32 pfe_many_series_one_param_time_major_f32_param_5,
	.param .u64 pfe_many_series_one_param_time_major_f32_param_6
)
{
	.reg .pred 	%p<29>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<84>;
	.reg .b32 	%r<95>;
	.reg .b64 	%rd<84>;


	ld.param.u64 	%rd42, [pfe_many_series_one_param_time_major_f32_param_0];
	ld.param.u64 	%rd43, [pfe_many_series_one_param_time_major_f32_param_1];
	ld.param.u32 	%r44, [pfe_many_series_one_param_time_major_f32_param_2];
	ld.param.u32 	%r45, [pfe_many_series_one_param_time_major_f32_param_3];
	ld.param.u32 	%r46, [pfe_many_series_one_param_time_major_f32_param_4];
	ld.param.u32 	%r47, [pfe_many_series_one_param_time_major_f32_param_5];
	ld.param.u64 	%rd44, [pfe_many_series_one_param_time_major_f32_param_6];
	cvta.to.global.u64 	%rd1, %rd44;
	cvta.to.global.u64 	%rd2, %rd42;
	mov.u32 	%r48, %ntid.x;
	mov.u32 	%r49, %ctaid.x;
	mov.u32 	%r50, %tid.x;
	mad.lo.s32 	%r1, %r49, %r48, %r50;
	setp.ge.s32 	%p1, %r1, %r44;
	setp.lt.s32 	%p2, %r46, 1;
	or.pred  	%p3, %p1, %p2;
	setp.lt.s32 	%p4, %r47, 1;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB2_32;
	bra.uni 	$L__BB2_1;

$L__BB2_32:
	ret;

$L__BB2_1:
	cvta.to.global.u64 	%rd45, %rd43;
	cvt.s64.s32 	%rd3, %r1;
	mul.wide.s32 	%rd46, %r1, 4;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.nc.u32 	%r2, [%rd47];
	setp.lt.s32 	%p6, %r2, 0;
	setp.ge.s32 	%p7, %r2, %r45;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB2_25;
	bra.uni 	$L__BB2_2;

$L__BB2_25:
	setp.lt.s32 	%p24, %r45, 1;
	@%p24 bra 	$L__BB2_32;

	add.s32 	%r74, %r45, -1;
	and.b32  	%r94, %r45, 3;
	setp.lt.u32 	%p25, %r74, 3;
	mov.u32 	%r93, 0;
	@%p25 bra 	$L__BB2_29;

	sub.s32 	%r92, %r45, %r94;
	shl.b64 	%rd69, %rd3, 2;
	add.s64 	%rd82, %rd1, %rd69;
	mul.wide.s32 	%rd35, %r44, 4;
	mov.u32 	%r93, 0;

$L__BB2_28:
	mov.u32 	%r76, 2147483647;
	st.global.u32 	[%rd82], %r76;
	add.s64 	%rd70, %rd82, %rd35;
	st.global.u32 	[%rd70], %r76;
	add.s64 	%rd71, %rd70, %rd35;
	st.global.u32 	[%rd71], %r76;
	add.s64 	%rd72, %rd71, %rd35;
	add.s64 	%rd82, %rd72, %rd35;
	st.global.u32 	[%rd72], %r76;
	add.s32 	%r93, %r93, 4;
	add.s32 	%r92, %r92, -4;
	setp.ne.s32 	%p26, %r92, 0;
	@%p26 bra 	$L__BB2_28;

$L__BB2_29:
	setp.eq.s32 	%p27, %r94, 0;
	@%p27 bra 	$L__BB2_32;

	mad.lo.s32 	%r77, %r93, %r44, %r1;
	mul.wide.s32 	%rd73, %r77, 4;
	add.s64 	%rd83, %rd1, %rd73;
	mul.wide.s32 	%rd39, %r44, 4;

$L__BB2_31:
	.pragma "nounroll";
	mov.u32 	%r78, 2147483647;
	st.global.u32 	[%rd83], %r78;
	add.s64 	%rd83, %rd83, %rd39;
	add.s32 	%r94, %r94, -1;
	setp.ne.s32 	%p28, %r94, 0;
	@%p28 bra 	$L__BB2_31;
	bra.uni 	$L__BB2_32;

$L__BB2_2:
	add.s32 	%r90, %r2, %r46;
	min.s32 	%r4, %r90, %r45;
	setp.lt.s32 	%p9, %r4, 1;
	@%p9 bra 	$L__BB2_9;

	add.s32 	%r52, %r4, -1;
	and.b32  	%r82, %r4, 3;
	setp.lt.u32 	%p10, %r52, 3;
	mov.u32 	%r81, 0;
	@%p10 bra 	$L__BB2_6;

	shl.b64 	%rd48, %rd3, 2;
	add.s64 	%rd74, %rd1, %rd48;
	not.b32 	%r54, %r90;
	not.b32 	%r55, %r45;
	max.s32 	%r56, %r55, %r54;
	add.s32 	%r57, %r56, %r82;
	neg.s32 	%r79, %r57;
	mov.u32 	%r81, 0;
	mul.wide.s32 	%rd5, %r44, 4;

$L__BB2_5:
	mov.u32 	%r58, 2147483647;
	st.global.u32 	[%rd74], %r58;
	add.s64 	%rd49, %rd74, %rd5;
	st.global.u32 	[%rd49], %r58;
	add.s64 	%rd50, %rd49, %rd5;
	st.global.u32 	[%rd50], %r58;
	add.s64 	%rd51, %rd50, %rd5;
	add.s64 	%rd74, %rd51, %rd5;
	st.global.u32 	[%rd51], %r58;
	add.s32 	%r81, %r81, 4;
	add.s32 	%r79, %r79, -4;
	setp.ne.s32 	%p11, %r79, 1;
	@%p11 bra 	$L__BB2_5;

$L__BB2_6:
	setp.eq.s32 	%p12, %r82, 0;
	@%p12 bra 	$L__BB2_9;

	mad.lo.s32 	%r59, %r81, %r44, %r1;
	mul.wide.s32 	%rd52, %r59, 4;
	add.s64 	%rd75, %rd1, %rd52;
	mul.wide.s32 	%rd9, %r44, 4;

$L__BB2_8:
	.pragma "nounroll";
	mov.u32 	%r60, 2147483647;
	st.global.u32 	[%rd75], %r60;
	add.s64 	%rd75, %rd75, %rd9;
	add.s32 	%r82, %r82, -1;
	setp.ne.s32 	%p13, %r82, 0;
	@%p13 bra 	$L__BB2_8;

$L__BB2_9:
	setp.ge.s32 	%p14, %r90, %r45;
	@%p14 bra 	$L__BB2_32;

	setp.gt.s32 	%p15, %r46, 0;
	@%p15 bra 	$L__BB2_12;
	bra.uni 	$L__BB2_11;

$L__BB2_12:
	add.s32 	%r14, %r2, 1;
	max.s32 	%r15, %r14, %r90;
	sub.s32 	%r61, %r15, %r2;
	and.b32  	%r84, %r61, 3;
	setp.eq.s32 	%p16, %r84, 0;
	mov.f32 	%f79, 0f00000000;
	mov.u32 	%r85, %r2;
	@%p16 bra 	$L__BB2_15;

	mad.lo.s32 	%r62, %r2, %r44, %r1;
	mul.wide.s32 	%rd53, %r62, 4;
	add.s64 	%rd77, %rd2, %rd53;
	mul.wide.s32 	%rd13, %r44, 4;
	mad.lo.s32 	%r63, %r44, %r14, %r1;
	mul.wide.s32 	%rd54, %r63, 4;
	add.s64 	%rd76, %rd2, %rd54;
	mov.f32 	%f79, 0f00000000;
	mov.u32 	%r85, %r2;

$L__BB2_14:
	.pragma "nounroll";
	add.s32 	%r85, %r85, 1;
	ld.global.nc.f32 	%f24, [%rd77];
	ld.global.nc.f32 	%f25, [%rd76];
	sub.ftz.f32 	%f26, %f25, %f24;
	mov.f32 	%f27, 0f3F800000;
	fma.rn.ftz.f32 	%f28, %f26, %f26, %f27;
	sqrt.approx.ftz.f32 	%f29, %f28;
	add.ftz.f32 	%f79, %f79, %f29;
	add.s64 	%rd77, %rd77, %rd13;
	add.s64 	%rd76, %rd76, %rd13;
	add.s32 	%r84, %r84, -1;
	setp.ne.s32 	%p17, %r84, 0;
	@%p17 bra 	$L__BB2_14;

$L__BB2_15:
	not.b32 	%r64, %r2;
	add.s32 	%r65, %r15, %r64;
	setp.lt.u32 	%p18, %r65, 3;
	@%p18 bra 	$L__BB2_18;

	add.s32 	%r66, %r85, 1;
	mad.lo.s32 	%r67, %r44, %r66, %r1;
	mul.wide.s32 	%rd55, %r67, 4;
	add.s64 	%rd78, %rd2, %rd55;
	mul.wide.s32 	%rd20, %r44, 4;
	neg.s32 	%r68, %r44;
	cvt.s64.s32 	%rd21, %r68;
	shl.b64 	%rd56, %rd21, 2;

$L__BB2_17:
	add.s64 	%rd57, %rd78, %rd56;
	ld.global.nc.f32 	%f30, [%rd57];
	ld.global.nc.f32 	%f31, [%rd78];
	sub.ftz.f32 	%f32, %f31, %f30;
	mov.f32 	%f33, 0f3F800000;
	fma.rn.ftz.f32 	%f34, %f32, %f32, %f33;
	sqrt.approx.ftz.f32 	%f35, %f34;
	add.ftz.f32 	%f36, %f79, %f35;
	add.s64 	%rd58, %rd78, %rd20;
	ld.global.nc.f32 	%f37, [%rd58];
	sub.ftz.f32 	%f38, %f37, %f31;
	fma.rn.ftz.f32 	%f39, %f38, %f38, %f33;
	sqrt.approx.ftz.f32 	%f40, %f39;
	add.ftz.f32 	%f41, %f36, %f40;
	add.s64 	%rd59, %rd58, %rd20;
	ld.global.nc.f32 	%f42, [%rd59];
	sub.ftz.f32 	%f43, %f42, %f37;
	fma.rn.ftz.f32 	%f44, %f43, %f43, %f33;
	sqrt.approx.ftz.f32 	%f45, %f44;
	add.ftz.f32 	%f46, %f41, %f45;
	add.s64 	%rd60, %rd59, %rd20;
	add.s64 	%rd78, %rd60, %rd20;
	ld.global.nc.f32 	%f47, [%rd60];
	sub.ftz.f32 	%f48, %f47, %f42;
	fma.rn.ftz.f32 	%f49, %f48, %f48, %f33;
	sqrt.approx.ftz.f32 	%f50, %f49;
	add.ftz.f32 	%f79, %f46, %f50;
	add.s32 	%r85, %r85, 4;
	setp.lt.s32 	%p19, %r85, %r90;
	@%p19 bra 	$L__BB2_17;
	bra.uni 	$L__BB2_18;

$L__BB2_11:
	mov.f32 	%f79, 0f00000000;

$L__BB2_18:
	cvt.rn.f32.s32 	%f52, %r46;
	cvt.rn.f32.s32 	%f53, %r47;
	add.ftz.f32 	%f54, %f53, 0f3F800000;
	mov.f32 	%f55, 0f40000000;
	div.approx.ftz.f32 	%f8, %f55, %f54;
	add.s32 	%r69, %r2, 1;
	mad.lo.s32 	%r89, %r44, %r69, %r1;
	mad.lo.s32 	%r88, %r2, %r44, %r1;
	sub.s32 	%r87, %r90, %r45;
	add.s32 	%r70, %r90, 1;
	mad.lo.s32 	%r71, %r44, %r70, %r1;
	mul.wide.s32 	%rd61, %r71, 4;
	add.s64 	%rd81, %rd2, %rd61;
	mad.lo.s32 	%r72, %r44, %r90, %r1;
	mul.wide.s32 	%rd62, %r72, 4;
	add.s64 	%rd80, %rd1, %rd62;
	add.s64 	%rd79, %rd2, %rd62;
	mul.ftz.f32 	%f9, %f52, %f52;
	mov.u16 	%rs6, 0;
	mov.f32 	%f51, 0f00000000;
	mov.f32 	%f80, %f51;

$L__BB2_19:
	.pragma "nounroll";
	mul.wide.s32 	%rd64, %r88, 4;
	add.s64 	%rd30, %rd2, %rd64;
	ld.global.nc.f32 	%f57, [%rd30];
	ld.global.nc.f32 	%f12, [%rd79];
	sub.ftz.f32 	%f13, %f12, %f57;
	setp.leu.ftz.f32 	%p20, %f79, 0f00000000;
	mov.f32 	%f82, %f51;
	@%p20 bra 	$L__BB2_21;

	fma.rn.ftz.f32 	%f58, %f13, %f13, %f9;
	sqrt.approx.ftz.f32 	%f59, %f58;
	div.approx.ftz.f32 	%f60, %f59, %f79;
	mul.ftz.f32 	%f82, %f60, 0f42C80000;

$L__BB2_21:
	copysign.f32 	%f83, %f13, %f82;
	and.b16  	%rs5, %rs6, 255;
	setp.eq.s16 	%p21, %rs5, 0;
	mov.u16 	%rs7, 1;
	@%p21 bra 	$L__BB2_23;

	mov.f32 	%f61, 0f3F800000;
	sub.ftz.f32 	%f62, %f61, %f8;
	mul.ftz.f32 	%f63, %f62, %f80;
	fma.rn.ftz.f32 	%f83, %f8, %f83, %f63;
	mov.u16 	%rs7, %rs6;

$L__BB2_23:
	st.global.f32 	[%rd80], %f83;
	add.s32 	%r90, %r90, 1;
	setp.eq.s32 	%p22, %r87, -1;
	@%p22 bra 	$L__BB2_32;

	ld.global.nc.f32 	%f64, [%rd81];
	sub.ftz.f32 	%f65, %f64, %f12;
	mul.wide.s32 	%rd66, %r89, 4;
	add.s64 	%rd67, %rd2, %rd66;
	ld.global.nc.f32 	%f66, [%rd30];
	ld.global.nc.f32 	%f67, [%rd67];
	sub.ftz.f32 	%f68, %f67, %f66;
	mov.f32 	%f69, 0f3F800000;
	fma.rn.ftz.f32 	%f70, %f65, %f65, %f69;
	sqrt.approx.ftz.f32 	%f71, %f70;
	fma.rn.ftz.f32 	%f72, %f68, %f68, %f69;
	sqrt.approx.ftz.f32 	%f73, %f72;
	sub.ftz.f32 	%f74, %f71, %f73;
	add.ftz.f32 	%f79, %f79, %f74;
	add.s32 	%r89, %r89, %r44;
	add.s32 	%r88, %r88, %r44;
	add.s32 	%r87, %r87, 1;
	mul.wide.s32 	%rd68, %r44, 4;
	add.s64 	%rd81, %rd81, %rd68;
	add.s64 	%rd80, %rd80, %rd68;
	add.s64 	%rd79, %rd79, %rd68;
	setp.lt.s32 	%p23, %r90, %r45;
	mov.f32 	%f80, %f83;
	mov.u16 	%rs6, %rs7;
	@%p23 bra 	$L__BB2_19;
	bra.uni 	$L__BB2_32;

}
	// .globl	pfe_build_steps_f32
.visible .entry pfe_build_steps_f32(
	.param .u64 pfe_build_steps_f32_param_0,
	.param .u32 pfe_build_steps_f32_param_1,
	.param .u64 pfe_build_steps_f32_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [pfe_build_steps_f32_param_0];
	ld.param.u32 	%r7, [pfe_build_steps_f32_param_1];
	ld.param.u64 	%rd4, [pfe_build_steps_f32_param_2];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r2, %r8, %r1, %r9;
	setp.ne.s32 	%p1, %r2, 0;
	@%p1 bra 	$L__BB3_2;

	mov.u32 	%r10, 0;
	st.global.u32 	[%rd1], %r10;

$L__BB3_2:
	add.s32 	%r12, %r2, 1;
	setp.ge.s32 	%p2, %r12, %r7;
	@%p2 bra 	$L__BB3_5;

	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r4, %r11, %r1;
	cvta.to.global.u64 	%rd2, %rd3;

$L__BB3_4:
	mul.wide.s32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.nc.f32 	%f1, [%rd6+-4];
	ld.global.nc.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	mov.f32 	%f4, 0f3F800000;
	fma.rn.ftz.f32 	%f5, %f3, %f3, %f4;
	sqrt.approx.ftz.f32 	%f6, %f5;
	add.s64 	%rd7, %rd1, %rd5;
	st.global.f32 	[%rd7], %f6;
	add.s32 	%r12, %r12, %r4;
	setp.lt.s32 	%p3, %r12, %r7;
	@%p3 bra 	$L__BB3_4;

$L__BB3_5:
	ret;

}
	// .globl	pfe_build_prefix_float2_serial
.visible .entry pfe_build_prefix_float2_serial(
	.param .u64 pfe_build_prefix_float2_serial_param_0,
	.param .u32 pfe_build_prefix_float2_serial_param_1,
	.param .u64 pfe_build_prefix_float2_serial_param_2,
	.param .u64 pfe_build_prefix_float2_serial_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<72>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd13, [pfe_build_prefix_float2_serial_param_0];
	ld.param.u32 	%r11, [pfe_build_prefix_float2_serial_param_1];
	ld.param.u64 	%rd14, [pfe_build_prefix_float2_serial_param_2];
	ld.param.u64 	%rd15, [pfe_build_prefix_float2_serial_param_3];
	cvta.to.global.u64 	%rd1, %rd13;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd14;
	mov.u32 	%r12, %tid.x;
	mov.u32 	%r13, %ctaid.x;
	or.b32  	%r14, %r12, %r13;
	setp.ne.s32 	%p1, %r14, 0;
	@%p1 bra 	$L__BB4_8;

	mov.u32 	%r15, 0;
	st.global.u32 	[%rd3], %r15;
	st.global.u32 	[%rd2], %r15;
	setp.lt.s32 	%p2, %r11, 2;
	@%p2 bra 	$L__BB4_8;

	add.s32 	%r1, %r11, -1;
	and.b32  	%r22, %r1, 3;
	add.s32 	%r17, %r11, -2;
	setp.lt.u32 	%p3, %r17, 3;
	mov.f32 	%f68, 0f00000000;
	mov.u32 	%r21, 1;
	mov.f32 	%f69, %f68;
	@%p3 bra 	$L__BB4_5;

	sub.s32 	%r20, %r1, %r22;
	mov.f32 	%f68, 0f00000000;
	mov.u32 	%r21, 1;

$L__BB4_4:
	mul.wide.s32 	%rd16, %r21, 4;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.nc.f32 	%f15, [%rd17];
	add.ftz.f32 	%f16, %f69, %f15;
	sub.ftz.f32 	%f17, %f16, %f69;
	sub.ftz.f32 	%f18, %f16, %f17;
	sub.ftz.f32 	%f19, %f69, %f18;
	sub.ftz.f32 	%f20, %f15, %f17;
	add.ftz.f32 	%f21, %f20, %f19;
	add.ftz.f32 	%f22, %f68, %f21;
	add.ftz.f32 	%f23, %f16, %f22;
	sub.ftz.f32 	%f24, %f23, %f16;
	sub.ftz.f32 	%f25, %f22, %f24;
	add.s64 	%rd18, %rd3, %rd16;
	st.global.f32 	[%rd18], %f23;
	add.s64 	%rd19, %rd2, %rd16;
	st.global.f32 	[%rd19], %f25;
	ld.global.nc.f32 	%f26, [%rd17+4];
	add.ftz.f32 	%f27, %f23, %f26;
	sub.ftz.f32 	%f28, %f27, %f23;
	sub.ftz.f32 	%f29, %f27, %f28;
	sub.ftz.f32 	%f30, %f23, %f29;
	sub.ftz.f32 	%f31, %f26, %f28;
	add.ftz.f32 	%f32, %f31, %f30;
	add.ftz.f32 	%f33, %f25, %f32;
	add.ftz.f32 	%f34, %f27, %f33;
	sub.ftz.f32 	%f35, %f34, %f27;
	sub.ftz.f32 	%f36, %f33, %f35;
	st.global.f32 	[%rd18+4], %f34;
	st.global.f32 	[%rd19+4], %f36;
	ld.global.nc.f32 	%f37, [%rd17+8];
	add.ftz.f32 	%f38, %f34, %f37;
	sub.ftz.f32 	%f39, %f38, %f34;
	sub.ftz.f32 	%f40, %f38, %f39;
	sub.ftz.f32 	%f41, %f34, %f40;
	sub.ftz.f32 	%f42, %f37, %f39;
	add.ftz.f32 	%f43, %f42, %f41;
	add.ftz.f32 	%f44, %f36, %f43;
	add.ftz.f32 	%f45, %f38, %f44;
	sub.ftz.f32 	%f46, %f45, %f38;
	sub.ftz.f32 	%f47, %f44, %f46;
	st.global.f32 	[%rd18+8], %f45;
	st.global.f32 	[%rd19+8], %f47;
	ld.global.nc.f32 	%f48, [%rd17+12];
	add.ftz.f32 	%f49, %f45, %f48;
	sub.ftz.f32 	%f50, %f49, %f45;
	sub.ftz.f32 	%f51, %f49, %f50;
	sub.ftz.f32 	%f52, %f45, %f51;
	sub.ftz.f32 	%f53, %f48, %f50;
	add.ftz.f32 	%f54, %f53, %f52;
	add.ftz.f32 	%f55, %f47, %f54;
	add.ftz.f32 	%f69, %f49, %f55;
	sub.ftz.f32 	%f56, %f69, %f49;
	sub.ftz.f32 	%f68, %f55, %f56;
	st.global.f32 	[%rd18+12], %f69;
	st.global.f32 	[%rd19+12], %f68;
	add.s32 	%r21, %r21, 4;
	add.s32 	%r20, %r20, -4;
	setp.ne.s32 	%p4, %r20, 0;
	@%p4 bra 	$L__BB4_4;

$L__BB4_5:
	setp.eq.s32 	%p5, %r22, 0;
	@%p5 bra 	$L__BB4_8;

	mul.wide.s32 	%rd20, %r21, 4;
	add.s64 	%rd23, %rd2, %rd20;
	add.s64 	%rd22, %rd3, %rd20;
	add.s64 	%rd21, %rd1, %rd20;

$L__BB4_7:
	.pragma "nounroll";
	ld.global.nc.f32 	%f57, [%rd21];
	add.ftz.f32 	%f58, %f69, %f57;
	sub.ftz.f32 	%f59, %f58, %f69;
	sub.ftz.f32 	%f60, %f58, %f59;
	sub.ftz.f32 	%f61, %f69, %f60;
	sub.ftz.f32 	%f62, %f57, %f59;
	add.ftz.f32 	%f63, %f62, %f61;
	add.ftz.f32 	%f64, %f68, %f63;
	add.ftz.f32 	%f69, %f58, %f64;
	sub.ftz.f32 	%f65, %f69, %f58;
	sub.ftz.f32 	%f68, %f64, %f65;
	st.global.f32 	[%rd22], %f69;
	st.global.f32 	[%rd23], %f68;
	add.s64 	%rd23, %rd23, 4;
	add.s64 	%rd22, %rd22, 4;
	add.s64 	%rd21, %rd21, 4;
	add.s32 	%r22, %r22, -1;
	setp.ne.s32 	%p6, %r22, 0;
	@%p6 bra 	$L__BB4_7;

$L__BB4_8:
	ret;

}
	// .globl	pfe_many_params_prefix_f32
.visible .entry pfe_many_params_prefix_f32(
	.param .u64 pfe_many_params_prefix_f32_param_0,
	.param .u64 pfe_many_params_prefix_f32_param_1,
	.param .u64 pfe_many_params_prefix_f32_param_2,
	.param .u32 pfe_many_params_prefix_f32_param_3,
	.param .u32 pfe_many_params_prefix_f32_param_4,
	.param .u64 pfe_many_params_prefix_f32_param_5,
	.param .u64 pfe_many_params_prefix_f32_param_6,
	.param .u32 pfe_many_params_prefix_f32_param_7,
	.param .u64 pfe_many_params_prefix_f32_param_8
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<8>;
	.reg .f32 	%f<34>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<57>;


	ld.param.u64 	%rd29, [pfe_many_params_prefix_f32_param_0];
	ld.param.u64 	%rd30, [pfe_many_params_prefix_f32_param_1];
	ld.param.u64 	%rd31, [pfe_many_params_prefix_f32_param_2];
	ld.param.u32 	%r17, [pfe_many_params_prefix_f32_param_3];
	ld.param.u32 	%r18, [pfe_many_params_prefix_f32_param_4];
	ld.param.u64 	%rd32, [pfe_many_params_prefix_f32_param_5];
	ld.param.u64 	%rd33, [pfe_many_params_prefix_f32_param_6];
	ld.param.u32 	%r19, [pfe_many_params_prefix_f32_param_7];
	ld.param.u64 	%rd34, [pfe_many_params_prefix_f32_param_8];
	cvta.to.global.u64 	%rd1, %rd34;
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r1, %r21, %r20, %r22;
	setp.ge.s32 	%p1, %r1, %r19;
	@%p1 bra 	$L__BB5_15;

	cvta.to.global.u64 	%rd35, %rd32;
	mul.wide.s32 	%rd36, %r1, 4;
	add.s64 	%rd37, %rd35, %rd36;
	cvta.to.global.u64 	%rd38, %rd33;
	add.s64 	%rd39, %rd38, %rd36;
	ld.global.nc.u32 	%r2, [%rd37];
	setp.lt.s32 	%p2, %r2, 1;
	ld.global.nc.u32 	%r3, [%rd39];
	setp.lt.s32 	%p3, %r3, 1;
	or.pred  	%p4, %p2, %p3;
	setp.gt.s32 	%p5, %r2, %r17;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB5_15;

	add.s32 	%r40, %r2, %r18;
	min.s32 	%r5, %r40, %r17;
	setp.lt.s32 	%p7, %r5, 1;
	@%p7 bra 	$L__BB5_9;

	add.s32 	%r24, %r5, -1;
	and.b32  	%r39, %r5, 3;
	setp.lt.u32 	%p8, %r24, 3;
	mov.u32 	%r38, 0;
	@%p8 bra 	$L__BB5_6;

	mul.lo.s32 	%r26, %r17, %r1;
	mul.wide.s32 	%rd40, %r26, 4;
	add.s64 	%rd48, %rd1, %rd40;
	not.b32 	%r27, %r40;
	not.b32 	%r28, %r17;
	max.s32 	%r29, %r28, %r27;
	add.s32 	%r30, %r29, %r39;
	neg.s32 	%r36, %r30;
	mov.u32 	%r38, 0;

$L__BB5_5:
	mov.u32 	%r31, 2147483647;
	st.global.u32 	[%rd48], %r31;
	st.global.u32 	[%rd48+4], %r31;
	st.global.u32 	[%rd48+8], %r31;
	st.global.u32 	[%rd48+12], %r31;
	add.s32 	%r38, %r38, 4;
	add.s64 	%rd48, %rd48, 16;
	add.s32 	%r36, %r36, -4;
	setp.ne.s32 	%p9, %r36, 1;
	@%p9 bra 	$L__BB5_5;

$L__BB5_6:
	setp.eq.s32 	%p10, %r39, 0;
	@%p10 bra 	$L__BB5_9;

	mad.lo.s32 	%r32, %r17, %r1, %r38;
	mul.wide.s32 	%rd41, %r32, 4;
	add.s64 	%rd49, %rd1, %rd41;

$L__BB5_8:
	.pragma "nounroll";
	mov.u32 	%r33, 2147483647;
	st.global.u32 	[%rd49], %r33;
	add.s64 	%rd49, %rd49, 4;
	add.s32 	%r39, %r39, -1;
	setp.ne.s32 	%p11, %r39, 0;
	@%p11 bra 	$L__BB5_8;

$L__BB5_9:
	setp.ge.s32 	%p12, %r40, %r17;
	@%p12 bra 	$L__BB5_15;

	cvta.to.global.u64 	%rd42, %rd31;
	cvta.to.global.u64 	%rd43, %rd30;
	cvta.to.global.u64 	%rd44, %rd29;
	cvt.rn.f32.s32 	%f10, %r2;
	mul.ftz.f32 	%f1, %f10, %f10;
	cvt.rn.f32.s32 	%f11, %r3;
	add.ftz.f32 	%f12, %f11, 0f3F800000;
	mov.f32 	%f13, 0f40000000;
	div.approx.ftz.f32 	%f2, %f13, %f12;
	neg.s32 	%r34, %r18;
	mul.wide.s32 	%rd45, %r34, 4;
	sub.s64 	%rd56, %rd42, %rd45;
	sub.s64 	%rd55, %rd43, %rd45;
	sub.s64 	%rd54, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r40, 4;
	add.s64 	%rd53, %rd42, %rd46;
	add.s64 	%rd52, %rd43, %rd46;
	add.s64 	%rd51, %rd44, %rd46;
	mad.lo.s32 	%r35, %r17, %r1, %r40;
	mul.wide.s32 	%rd47, %r35, 4;
	add.s64 	%rd50, %rd1, %rd47;
	mov.u16 	%rs7, 0;
	mov.f32 	%f32, 0f00000000;

$L__BB5_11:
	.pragma "nounroll";
	mov.u16 	%rs1, %rs7;
	mov.f32 	%f3, %f32;
	ld.global.nc.f32 	%f15, [%rd55];
	ld.global.nc.f32 	%f16, [%rd52];
	sub.ftz.f32 	%f17, %f16, %f15;
	ld.global.nc.f32 	%f18, [%rd56];
	ld.global.nc.f32 	%f19, [%rd53];
	sub.ftz.f32 	%f20, %f19, %f18;
	add.ftz.f32 	%f4, %f17, %f20;
	setp.leu.ftz.f32 	%p13, %f4, 0f00000000;
	mov.f32 	%f33, 0f7FFFFFFF;
	@%p13 bra 	$L__BB5_14;

	ld.global.nc.f32 	%f21, [%rd54];
	ld.global.nc.f32 	%f22, [%rd51];
	sub.ftz.f32 	%f23, %f22, %f21;
	fma.rn.ftz.f32 	%f24, %f23, %f23, %f1;
	sqrt.approx.ftz.f32 	%f25, %f24;
	div.approx.ftz.f32 	%f26, %f25, %f4;
	mul.ftz.f32 	%f27, %f26, 0f42C80000;
	copysign.f32 	%f32, %f23, %f27;
	and.b16  	%rs5, %rs1, 255;
	setp.eq.s16 	%p14, %rs5, 0;
	mov.u16 	%rs7, 1;
	mov.f32 	%f33, %f32;
	@%p14 bra 	$L__BB5_14;

	mov.f32 	%f28, 0f3F800000;
	sub.ftz.f32 	%f29, %f28, %f2;
	mul.ftz.f32 	%f30, %f29, %f3;
	fma.rn.ftz.f32 	%f32, %f2, %f32, %f30;
	mov.u16 	%rs7, %rs1;
	mov.f32 	%f33, %f32;

$L__BB5_14:
	st.global.f32 	[%rd50], %f33;
	add.s64 	%rd56, %rd56, 4;
	add.s64 	%rd55, %rd55, 4;
	add.s64 	%rd54, %rd54, 4;
	add.s64 	%rd53, %rd53, 4;
	add.s64 	%rd52, %rd52, 4;
	add.s64 	%rd51, %rd51, 4;
	add.s64 	%rd50, %rd50, 4;
	add.s32 	%r40, %r40, 1;
	setp.lt.s32 	%p15, %r40, %r17;
	@%p15 bra 	$L__BB5_11;

$L__BB5_15:
	ret;

}

