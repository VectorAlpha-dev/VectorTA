







.version 9.0
.target sm_89
.address_size 64

	
.func  (.param .align 8 .b8 func_retval0[16]) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0
)
;
.extern .shared .align 16 .b8 shraw[];
.global .align 16 .b8 __cudart_sin_cos_coeffs[128] = {70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};
.global .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry trendflex_batch_f32(
	.param .u64 trendflex_batch_f32_param_0,
	.param .u64 trendflex_batch_f32_param_1,
	.param .u32 trendflex_batch_f32_param_2,
	.param .u32 trendflex_batch_f32_param_3,
	.param .u32 trendflex_batch_f32_param_4,
	.param .u32 trendflex_batch_f32_param_5,
	.param .u64 trendflex_batch_f32_param_6
)
{
	.reg .pred 	%p<43>;
	.reg .f32 	%f<56>;
	.reg .b32 	%r<150>;
	.reg .f64 	%fd<317>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd26, [trendflex_batch_f32_param_0];
	ld.param.u64 	%rd25, [trendflex_batch_f32_param_1];
	ld.param.u32 	%r52, [trendflex_batch_f32_param_2];
	ld.param.u32 	%r55, [trendflex_batch_f32_param_3];
	ld.param.u32 	%r53, [trendflex_batch_f32_param_4];
	ld.param.u32 	%r54, [trendflex_batch_f32_param_5];
	ld.param.u64 	%rd27, [trendflex_batch_f32_param_6];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	mov.u32 	%r56, %ntid.x;
	mov.u32 	%r57, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r57, %r56, %r1;
	setp.ge.s32 	%p1, %r2, %r55;
	@%p1 bra 	$L__BB0_50;

	cvta.to.global.u64 	%rd28, %rd25;
	cvt.s64.s32 	%rd3, %r2;
	mul.wide.s32 	%rd29, %r2, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r3, [%rd30];
	setp.lt.s32 	%p2, %r3, 1;
	setp.lt.s32 	%p3, %r52, 1;
	or.pred  	%p4, %p3, %p2;
	setp.ge.s32 	%p5, %r3, %r52;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB0_50;

	setp.lt.s32 	%p7, %r53, 0;
	setp.le.s32 	%p8, %r52, %r53;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	$L__BB0_50;

	setp.lt.s32 	%p10, %r54, 1;
	setp.gt.s32 	%p11, %r3, %r54;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	$L__BB0_50;

	sub.s32 	%r4, %r52, %r53;
	setp.lt.s32 	%p13, %r4, %r3;
	@%p13 bra 	$L__BB0_50;

	cvt.rn.f32.s32 	%f17, %r3;
	mul.ftz.f32 	%f18, %f17, 0f3F000000;
	mov.f32 	%f19, 0f3F000000;
	copysign.f32 	%f20, %f18, %f19;
	add.rz.ftz.f32 	%f21, %f18, %f20;
	cvt.rzi.f32.f32 	%f22, %f21;
	cvt.rzi.ftz.s32.f32 	%r58, %f22;
	max.s32 	%r5, %r58, 1;
	setp.lt.s32 	%p14, %r4, %r5;
	@%p14 bra 	$L__BB0_50;

	cvt.rn.f64.s32 	%fd100, %r5;
	rcp.rn.f64 	%fd101, %fd100;
	mov.f64 	%fd102, 0d3FF0000000000000;
	mul.f64 	%fd1, %fd101, 0d4011C5831ADD62E4;
	mul.f64 	%fd103, %fd101, 0dC011C5831ADD62E4;
	mov.f64 	%fd104, 0d4338000000000000;
	mov.f64 	%fd105, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd106, %fd103, %fd105, %fd104;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r6, %temp}, %fd106;
	}
	mov.f64 	%fd107, 0dC338000000000000;
	add.rn.f64 	%fd108, %fd106, %fd107;
	mov.f64 	%fd109, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd110, %fd108, %fd109, %fd103;
	mov.f64 	%fd111, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd112, %fd108, %fd111, %fd110;
	mov.f64 	%fd113, 0d3E928AF3FCA213EA;
	mov.f64 	%fd114, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd115, %fd114, %fd112, %fd113;
	mov.f64 	%fd116, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd117, %fd115, %fd112, %fd116;
	mov.f64 	%fd118, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd119, %fd117, %fd112, %fd118;
	mov.f64 	%fd120, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd121, %fd119, %fd112, %fd120;
	mov.f64 	%fd122, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd123, %fd121, %fd112, %fd122;
	mov.f64 	%fd124, 0d3F81111111122322;
	fma.rn.f64 	%fd125, %fd123, %fd112, %fd124;
	mov.f64 	%fd126, 0d3FA55555555502A1;
	fma.rn.f64 	%fd127, %fd125, %fd112, %fd126;
	mov.f64 	%fd128, 0d3FC5555555555511;
	fma.rn.f64 	%fd129, %fd127, %fd112, %fd128;
	mov.f64 	%fd130, 0d3FE000000000000B;
	fma.rn.f64 	%fd131, %fd129, %fd112, %fd130;
	fma.rn.f64 	%fd132, %fd131, %fd112, %fd102;
	fma.rn.f64 	%fd133, %fd132, %fd112, %fd102;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd133;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd133;
	}
	shl.b32 	%r59, %r6, 20;
	add.s32 	%r60, %r8, %r59;
	mov.b64 	%fd276, {%r7, %r60};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd103;
	}
	mov.b32 	%f23, %r61;
	abs.ftz.f32 	%f1, %f23;
	setp.lt.ftz.f32 	%p15, %f1, 0f4086232B;
	@%p15 bra 	$L__BB0_9;

	setp.gt.f64 	%p16, %fd1, 0d8000000000000000;
	mov.f64 	%fd134, 0d7FF0000000000000;
	sub.f64 	%fd135, %fd134, %fd1;
	selp.f64 	%fd276, 0d0000000000000000, %fd135, %p16;
	setp.geu.ftz.f32 	%p17, %f1, 0f40874800;
	@%p17 bra 	$L__BB0_9;

	mov.f64 	%fd275, 0d4338000000000000;
	mov.f64 	%fd274, 0d3FF71547652B82FE;
	mul.f64 	%fd273, %fd101, 0dC011C5831ADD62E4;
	fma.rn.f64 	%fd272, %fd273, %fd274, %fd275;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r116, %temp}, %fd272;
	}
	shr.u32 	%r62, %r116, 31;
	add.s32 	%r63, %r116, %r62;
	shr.s32 	%r64, %r63, 1;
	shl.b32 	%r65, %r64, 20;
	add.s32 	%r66, %r8, %r65;
	mov.b64 	%fd136, {%r7, %r66};
	sub.s32 	%r67, %r116, %r64;
	shl.b32 	%r68, %r67, 20;
	add.s32 	%r69, %r68, 1072693248;
	mov.u32 	%r70, 0;
	mov.b64 	%fd137, {%r70, %r69};
	mul.f64 	%fd276, %fd136, %fd137;

$L__BB0_9:
	mul.f64 	%fd6, %fd276, %fd276;
	abs.f64 	%fd7, %fd1;
	setp.eq.f64 	%p18, %fd7, 0d7FF0000000000000;
	@%p18 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_10;

$L__BB0_12:
	mov.f64 	%fd146, 0d0000000000000000;
	mul.rn.f64 	%fd277, %fd1, %fd146;
	mov.u32 	%r134, 0;
	bra.uni 	$L__BB0_13;

$L__BB0_10:
	mul.f64 	%fd138, %fd1, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r134, %fd138;
	cvt.rn.f64.s32 	%fd139, %r134;
	neg.f64 	%fd140, %fd139;
	mov.f64 	%fd141, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd142, %fd140, %fd141, %fd1;
	mov.f64 	%fd143, 0d3C91A62633145C00;
	fma.rn.f64 	%fd144, %fd140, %fd143, %fd142;
	mov.f64 	%fd145, 0d397B839A252049C0;
	fma.rn.f64 	%fd277, %fd140, %fd145, %fd144;
	setp.ltu.f64 	%p19, %fd7, 0d41E0000000000000;
	@%p19 bra 	$L__BB0_13;

	{ 
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1;
	.param .align 8 .b8 retval0[16];
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0
	);
	ld.param.f64 	%fd277, [retval0+0];
	ld.param.b32 	%r134, [retval0+8];
	} 

$L__BB0_13:
	add.s32 	%r12, %r134, 1;
	and.b32  	%r72, %r12, 1;
	shl.b32 	%r73, %r12, 3;
	and.b32  	%r74, %r73, 8;
	mul.wide.u32 	%rd31, %r74, 8;
	mov.u64 	%rd32, __cudart_sin_cos_coeffs;
	add.s64 	%rd33, %rd32, %rd31;
	setp.eq.s32 	%p20, %r72, 0;
	selp.f64 	%fd147, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p20;
	ld.global.nc.v2.f64 	{%fd148, %fd149}, [%rd33];
	mul.rn.f64 	%fd12, %fd277, %fd277;
	fma.rn.f64 	%fd152, %fd147, %fd12, %fd148;
	fma.rn.f64 	%fd153, %fd152, %fd12, %fd149;
	ld.global.nc.v2.f64 	{%fd154, %fd155}, [%rd33+16];
	fma.rn.f64 	%fd158, %fd153, %fd12, %fd154;
	fma.rn.f64 	%fd159, %fd158, %fd12, %fd155;
	ld.global.nc.v2.f64 	{%fd160, %fd161}, [%rd33+32];
	fma.rn.f64 	%fd164, %fd159, %fd12, %fd160;
	fma.rn.f64 	%fd13, %fd164, %fd12, %fd161;
	fma.rn.f64 	%fd279, %fd13, %fd277, %fd277;
	@%p20 bra 	$L__BB0_15;

	mov.f64 	%fd165, 0d3FF0000000000000;
	fma.rn.f64 	%fd279, %fd13, %fd12, %fd165;

$L__BB0_15:
	and.b32  	%r75, %r12, 2;
	setp.eq.s32 	%p21, %r75, 0;
	@%p21 bra 	$L__BB0_17;

	mov.f64 	%fd166, 0d0000000000000000;
	mov.f64 	%fd167, 0dBFF0000000000000;
	fma.rn.f64 	%fd279, %fd279, %fd167, %fd166;

$L__BB0_17:
	ld.param.u32 	%r117, [trendflex_batch_f32_param_4];
	add.f64 	%fd168, %fd6, 0d3FF0000000000000;
	add.f64 	%fd169, %fd276, %fd276;
	mul.f64 	%fd19, %fd169, %fd279;
	sub.f64 	%fd170, %fd168, %fd19;
	mul.f64 	%fd20, %fd170, 0d3FE0000000000000;
	add.s32 	%r13, %r3, %r117;
	min.s32 	%r14, %r13, %r52;
	cvt.s64.s32 	%rd34, %r52;
	mul.lo.s64 	%rd4, %rd34, %rd3;
	setp.lt.s32 	%p22, %r14, 1;
	@%p22 bra 	$L__BB0_24;

	add.s32 	%r77, %r14, -1;
	and.b32  	%r138, %r14, 3;
	setp.lt.u32 	%p23, %r77, 3;
	mov.u32 	%r137, 0;
	@%p23 bra 	$L__BB0_21;

	sub.s32 	%r136, %r14, %r138;
	mov.u32 	%r137, 0;

$L__BB0_20:
	cvt.s64.s32 	%rd35, %r137;
	add.s64 	%rd36, %rd4, %rd35;
	shl.b64 	%rd37, %rd36, 2;
	add.s64 	%rd38, %rd1, %rd37;
	mov.u32 	%r79, 2147483647;
	st.global.u32 	[%rd38], %r79;
	st.global.u32 	[%rd38+4], %r79;
	st.global.u32 	[%rd38+8], %r79;
	st.global.u32 	[%rd38+12], %r79;
	add.s32 	%r137, %r137, 4;
	add.s32 	%r136, %r136, -4;
	setp.ne.s32 	%p24, %r136, 0;
	@%p24 bra 	$L__BB0_20;

$L__BB0_21:
	setp.eq.s32 	%p25, %r138, 0;
	@%p25 bra 	$L__BB0_24;

	cvt.s64.s32 	%rd39, %r137;
	add.s64 	%rd40, %rd4, %rd39;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd61, %rd1, %rd41;

$L__BB0_23:
	.pragma "nounroll";
	mov.u32 	%r80, 2147483647;
	st.global.u32 	[%rd61], %r80;
	add.s64 	%rd61, %rd61, 4;
	add.s32 	%r138, %r138, -1;
	setp.ne.s32 	%p26, %r138, 0;
	@%p26 bra 	$L__BB0_23;

$L__BB0_24:
	setp.ge.s32 	%p27, %r13, %r52;
	@%p27 bra 	$L__BB0_50;

	mov.u32 	%r120, %tid.x;
	ld.param.u32 	%r119, [trendflex_batch_f32_param_5];
	ld.param.u32 	%r118, [trendflex_batch_f32_param_4];
	mul.lo.s32 	%r24, %r120, %r119;
	shl.b32 	%r81, %r24, 2;
	mov.u32 	%r82, shraw;
	add.s32 	%r25, %r82, %r81;
	mul.wide.s32 	%rd42, %r118, 4;
	add.s64 	%rd9, %rd2, %rd42;
	ld.global.nc.f32 	%f24, [%rd9];
	cvt.ftz.f64.f32 	%fd298, %f24;
	st.shared.f32 	[%r25], %f24;
	setp.lt.s32 	%p28, %r3, 2;
	mov.f64 	%fd299, %fd298;
	mov.f64 	%fd300, %fd298;
	@%p28 bra 	$L__BB0_27;

	ld.global.nc.f32 	%f25, [%rd9+4];
	cvt.ftz.f64.f32 	%fd300, %f25;
	st.shared.f32 	[%r25+4], %f25;
	add.f64 	%fd299, %fd298, %fd300;

$L__BB0_27:
	setp.lt.s32 	%p29, %r3, 3;
	mov.f64 	%fd301, %fd300;
	@%p29 bra 	$L__BB0_34;

	add.s32 	%r26, %r3, -2;
	and.b32  	%r27, %r26, 3;
	add.s32 	%r84, %r3, -3;
	setp.lt.u32 	%p30, %r84, 3;
	mov.u32 	%r141, 2;
	mov.f64 	%fd301, %fd300;
	@%p30 bra 	$L__BB0_31;

	sub.s32 	%r140, %r26, %r27;
	mov.u32 	%r141, 2;
	mov.f64 	%fd301, %fd300;

$L__BB0_30:
	mul.wide.s32 	%rd43, %r141, 4;
	add.s64 	%rd44, %rd9, %rd43;
	ld.global.nc.f32 	%f26, [%rd44];
	cvt.ftz.f64.f32 	%fd172, %f26;
	add.f64 	%fd173, %fd301, %fd172;
	mul.f64 	%fd174, %fd6, %fd298;
	neg.f64 	%fd175, %fd174;
	fma.rn.f64 	%fd176, %fd19, %fd300, %fd175;
	fma.rn.f64 	%fd177, %fd20, %fd173, %fd176;
	cvt.rn.ftz.f32.f64 	%f27, %fd177;
	shl.b32 	%r86, %r141, 2;
	add.s32 	%r87, %r25, %r86;
	st.shared.f32 	[%r87], %f27;
	cvt.ftz.f64.f32 	%fd178, %f27;
	add.f64 	%fd179, %fd299, %fd178;
	ld.global.nc.f32 	%f28, [%rd44+4];
	cvt.ftz.f64.f32 	%fd180, %f28;
	add.f64 	%fd181, %fd172, %fd180;
	mul.f64 	%fd182, %fd6, %fd300;
	neg.f64 	%fd183, %fd182;
	fma.rn.f64 	%fd184, %fd19, %fd177, %fd183;
	fma.rn.f64 	%fd185, %fd20, %fd181, %fd184;
	cvt.rn.ftz.f32.f64 	%f29, %fd185;
	st.shared.f32 	[%r87+4], %f29;
	cvt.ftz.f64.f32 	%fd186, %f29;
	add.f64 	%fd187, %fd179, %fd186;
	ld.global.nc.f32 	%f30, [%rd44+8];
	cvt.ftz.f64.f32 	%fd188, %f30;
	add.f64 	%fd189, %fd180, %fd188;
	mul.f64 	%fd190, %fd6, %fd177;
	neg.f64 	%fd191, %fd190;
	fma.rn.f64 	%fd192, %fd19, %fd185, %fd191;
	fma.rn.f64 	%fd298, %fd20, %fd189, %fd192;
	cvt.rn.ftz.f32.f64 	%f31, %fd298;
	st.shared.f32 	[%r87+8], %f31;
	cvt.ftz.f64.f32 	%fd193, %f31;
	add.f64 	%fd194, %fd187, %fd193;
	ld.global.nc.f32 	%f32, [%rd44+12];
	cvt.ftz.f64.f32 	%fd301, %f32;
	add.f64 	%fd195, %fd188, %fd301;
	mul.f64 	%fd196, %fd6, %fd185;
	neg.f64 	%fd197, %fd196;
	fma.rn.f64 	%fd198, %fd19, %fd298, %fd197;
	fma.rn.f64 	%fd300, %fd20, %fd195, %fd198;
	cvt.rn.ftz.f32.f64 	%f33, %fd300;
	st.shared.f32 	[%r87+12], %f33;
	cvt.ftz.f64.f32 	%fd199, %f33;
	add.f64 	%fd299, %fd194, %fd199;
	add.s32 	%r141, %r141, 4;
	add.s32 	%r140, %r140, -4;
	setp.ne.s32 	%p31, %r140, 0;
	@%p31 bra 	$L__BB0_30;

$L__BB0_31:
	add.s32 	%r125, %r3, -2;
	and.b32  	%r124, %r125, 3;
	setp.eq.s32 	%p32, %r124, 0;
	@%p32 bra 	$L__BB0_34;

	ld.param.u32 	%r129, [trendflex_batch_f32_param_4];
	cvt.s64.s32 	%rd56, %r129;
	mov.u32 	%r128, shraw;
	add.s32 	%r127, %r3, -2;
	and.b32  	%r143, %r127, 3;
	shl.b32 	%r89, %r141, 2;
	add.s32 	%r90, %r81, %r89;
	add.s32 	%r142, %r128, %r90;
	cvt.s64.s32 	%rd45, %r141;
	add.s64 	%rd46, %rd45, %rd56;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd62, %rd2, %rd47;
	mov.f64 	%fd294, %fd301;
	mov.f64 	%fd297, %fd298;

$L__BB0_33:
	.pragma "nounroll";
	mov.f64 	%fd298, %fd300;
	ld.global.nc.f32 	%f34, [%rd62];
	cvt.ftz.f64.f32 	%fd301, %f34;
	add.f64 	%fd200, %fd294, %fd301;
	mul.f64 	%fd201, %fd6, %fd297;
	neg.f64 	%fd202, %fd201;
	fma.rn.f64 	%fd203, %fd19, %fd298, %fd202;
	fma.rn.f64 	%fd300, %fd20, %fd200, %fd203;
	cvt.rn.ftz.f32.f64 	%f35, %fd300;
	st.shared.f32 	[%r142], %f35;
	cvt.ftz.f64.f32 	%fd204, %f35;
	add.f64 	%fd299, %fd299, %fd204;
	add.s32 	%r142, %r142, 4;
	add.s64 	%rd62, %rd62, 4;
	add.s32 	%r143, %r143, -1;
	setp.ne.s32 	%p33, %r143, 0;
	mov.f64 	%fd294, %fd301;
	mov.f64 	%fd297, %fd298;
	@%p33 bra 	$L__BB0_33;

$L__BB0_34:
	ld.param.u32 	%r131, [trendflex_batch_f32_param_4];
	add.s32 	%r147, %r3, %r131;
	ld.param.u32 	%r121, [trendflex_batch_f32_param_4];
	cvt.rn.f64.s32 	%fd53, %r3;
	rcp.rn.f64 	%fd54, %fd53;
	sub.s32 	%r92, %r52, %r3;
	sub.s32 	%r93, %r92, %r121;
	and.b32  	%r146, %r93, 3;
	setp.eq.s32 	%p34, %r146, 0;
	mov.f64 	%fd307, 0d0000000000000000;
	@%p34 bra 	$L__BB0_39;

	ld.param.u32 	%r133, [trendflex_batch_f32_param_4];
	add.s32 	%r147, %r3, %r133;
	ld.param.u64 	%rd58, [trendflex_batch_f32_param_6];
	cvta.to.global.u64 	%rd57, %rd58;
	cvt.s64.s32 	%rd48, %r147;
	add.s64 	%rd49, %rd4, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd64, %rd57, %rd50;
	mul.wide.s32 	%rd51, %r147, 4;
	add.s64 	%rd63, %rd2, %rd51;
	mov.f64 	%fd307, 0d0000000000000000;
	mov.u32 	%r144, %r3;
	mov.f64 	%fd303, %fd301;
	mov.f64 	%fd306, %fd298;

$L__BB0_36:
	.pragma "nounroll";
	mov.f64 	%fd298, %fd300;
	ld.global.nc.f32 	%f37, [%rd63];
	cvt.ftz.f64.f32 	%fd301, %f37;
	add.f64 	%fd207, %fd303, %fd301;
	mul.f64 	%fd208, %fd6, %fd306;
	neg.f64 	%fd209, %fd208;
	fma.rn.f64 	%fd210, %fd19, %fd298, %fd209;
	fma.rn.f64 	%fd300, %fd20, %fd207, %fd210;
	cvt.rn.ftz.f32.f64 	%f2, %fd300;
	cvt.ftz.f64.f32 	%fd62, %f2;
	mul.f64 	%fd211, %fd53, %fd62;
	sub.f64 	%fd212, %fd211, %fd299;
	mul.f64 	%fd63, %fd54, %fd212;
	mul.f64 	%fd213, %fd63, %fd63;
	mul.f64 	%fd214, %fd307, 0d3FEEB851EB851EB8;
	mov.f64 	%fd215, 0d3FA47AE147AE147B;
	fma.rn.f64 	%fd307, %fd215, %fd213, %fd214;
	setp.leu.f64 	%p35, %fd307, 0d0000000000000000;
	mov.f32 	%f51, 0f00000000;
	@%p35 bra 	$L__BB0_38;

	sqrt.rn.f64 	%fd216, %fd307;
	div.rn.f64 	%fd217, %fd63, %fd216;
	cvt.rn.ftz.f32.f64 	%f51, %fd217;

$L__BB0_38:
	st.global.f32 	[%rd64], %f51;
	rem.s32 	%r94, %r144, %r3;
	shl.b32 	%r95, %r94, 2;
	add.s32 	%r96, %r25, %r95;
	ld.shared.f32 	%f38, [%r96];
	cvt.ftz.f64.f32 	%fd218, %f38;
	st.shared.f32 	[%r96], %f2;
	sub.f64 	%fd219, %fd62, %fd218;
	add.f64 	%fd299, %fd299, %fd219;
	add.s32 	%r147, %r147, 1;
	add.s32 	%r144, %r144, 1;
	add.s64 	%rd64, %rd64, 4;
	add.s64 	%rd63, %rd63, 4;
	add.s32 	%r146, %r146, -1;
	setp.ne.s32 	%p36, %r146, 0;
	mov.f64 	%fd303, %fd301;
	mov.f64 	%fd306, %fd298;
	@%p36 bra 	$L__BB0_36;

$L__BB0_39:
	ld.param.u32 	%r122, [trendflex_batch_f32_param_4];
	not.b32 	%r97, %r3;
	add.s32 	%r98, %r97, %r52;
	sub.s32 	%r99, %r98, %r122;
	setp.lt.u32 	%p37, %r99, 3;
	@%p37 bra 	$L__BB0_50;

	ld.param.u64 	%rd60, [trendflex_batch_f32_param_6];
	cvta.to.global.u64 	%rd59, %rd60;
	ld.param.u32 	%r123, [trendflex_batch_f32_param_4];
	add.s32 	%r100, %r147, 3;
	sub.s32 	%r148, %r100, %r123;
	cvt.s64.s32 	%rd52, %r147;
	add.s64 	%rd53, %rd4, %rd52;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd65, %rd59, %rd54;
	mul.wide.s32 	%rd55, %r147, 4;
	add.s64 	%rd66, %rd2, %rd55;

$L__BB0_41:
	ld.global.nc.f32 	%f40, [%rd66];
	cvt.ftz.f64.f32 	%fd76, %f40;
	add.f64 	%fd220, %fd301, %fd76;
	mul.f64 	%fd221, %fd6, %fd298;
	neg.f64 	%fd222, %fd221;
	fma.rn.f64 	%fd223, %fd19, %fd300, %fd222;
	fma.rn.f64 	%fd77, %fd20, %fd220, %fd223;
	cvt.rn.ftz.f32.f64 	%f5, %fd77;
	cvt.ftz.f64.f32 	%fd78, %f5;
	mul.f64 	%fd224, %fd53, %fd78;
	sub.f64 	%fd225, %fd224, %fd299;
	mul.f64 	%fd79, %fd54, %fd225;
	mul.f64 	%fd226, %fd79, %fd79;
	mul.f64 	%fd227, %fd307, 0d3FEEB851EB851EB8;
	mov.f64 	%fd228, 0d3FA47AE147AE147B;
	fma.rn.f64 	%fd80, %fd228, %fd226, %fd227;
	setp.leu.f64 	%p38, %fd80, 0d0000000000000000;
	mov.f32 	%f53, 0f00000000;
	mov.f32 	%f52, %f53;
	@%p38 bra 	$L__BB0_43;

	sqrt.rn.f64 	%fd229, %fd80;
	div.rn.f64 	%fd230, %fd79, %fd229;
	cvt.rn.ftz.f32.f64 	%f52, %fd230;

$L__BB0_43:
	st.global.f32 	[%rd65], %f52;
	add.s32 	%r101, %r148, -3;
	rem.s32 	%r102, %r101, %r3;
	shl.b32 	%r103, %r102, 2;
	add.s32 	%r104, %r25, %r103;
	ld.shared.f32 	%f42, [%r104];
	cvt.ftz.f64.f32 	%fd231, %f42;
	st.shared.f32 	[%r104], %f5;
	sub.f64 	%fd232, %fd78, %fd231;
	add.f64 	%fd81, %fd299, %fd232;
	ld.global.nc.f32 	%f43, [%rd66+4];
	cvt.ftz.f64.f32 	%fd82, %f43;
	add.f64 	%fd233, %fd76, %fd82;
	mul.f64 	%fd234, %fd6, %fd300;
	neg.f64 	%fd235, %fd234;
	fma.rn.f64 	%fd236, %fd19, %fd77, %fd235;
	fma.rn.f64 	%fd83, %fd20, %fd233, %fd236;
	cvt.rn.ftz.f32.f64 	%f8, %fd83;
	cvt.ftz.f64.f32 	%fd84, %f8;
	mul.f64 	%fd237, %fd53, %fd84;
	sub.f64 	%fd238, %fd237, %fd81;
	mul.f64 	%fd85, %fd54, %fd238;
	mul.f64 	%fd239, %fd85, %fd85;
	mul.f64 	%fd240, %fd80, 0d3FEEB851EB851EB8;
	fma.rn.f64 	%fd86, %fd228, %fd239, %fd240;
	setp.leu.f64 	%p39, %fd86, 0d0000000000000000;
	@%p39 bra 	$L__BB0_45;

	sqrt.rn.f64 	%fd242, %fd86;
	div.rn.f64 	%fd243, %fd85, %fd242;
	cvt.rn.ftz.f32.f64 	%f53, %fd243;

$L__BB0_45:
	st.global.f32 	[%rd65+4], %f53;
	add.s32 	%r105, %r148, -2;
	rem.s32 	%r106, %r105, %r3;
	shl.b32 	%r107, %r106, 2;
	add.s32 	%r108, %r25, %r107;
	ld.shared.f32 	%f45, [%r108];
	cvt.ftz.f64.f32 	%fd244, %f45;
	st.shared.f32 	[%r108], %f8;
	sub.f64 	%fd245, %fd84, %fd244;
	add.f64 	%fd87, %fd81, %fd245;
	ld.global.nc.f32 	%f46, [%rd66+8];
	cvt.ftz.f64.f32 	%fd88, %f46;
	add.f64 	%fd246, %fd82, %fd88;
	mul.f64 	%fd247, %fd6, %fd77;
	neg.f64 	%fd248, %fd247;
	fma.rn.f64 	%fd249, %fd19, %fd83, %fd248;
	fma.rn.f64 	%fd298, %fd20, %fd246, %fd249;
	cvt.rn.ftz.f32.f64 	%f11, %fd298;
	cvt.ftz.f64.f32 	%fd90, %f11;
	mul.f64 	%fd250, %fd53, %fd90;
	sub.f64 	%fd251, %fd250, %fd87;
	mul.f64 	%fd91, %fd54, %fd251;
	mul.f64 	%fd252, %fd91, %fd91;
	mul.f64 	%fd253, %fd86, 0d3FEEB851EB851EB8;
	mov.f64 	%fd254, 0d3FA47AE147AE147B;
	fma.rn.f64 	%fd92, %fd254, %fd252, %fd253;
	setp.leu.f64 	%p40, %fd92, 0d0000000000000000;
	mov.f32 	%f55, 0f00000000;
	mov.f32 	%f54, %f55;
	@%p40 bra 	$L__BB0_47;

	sqrt.rn.f64 	%fd255, %fd92;
	div.rn.f64 	%fd256, %fd91, %fd255;
	cvt.rn.ftz.f32.f64 	%f54, %fd256;

$L__BB0_47:
	st.global.f32 	[%rd65+8], %f54;
	add.s32 	%r109, %r148, -1;
	rem.s32 	%r110, %r109, %r3;
	shl.b32 	%r111, %r110, 2;
	add.s32 	%r112, %r25, %r111;
	ld.shared.f32 	%f48, [%r112];
	cvt.ftz.f64.f32 	%fd257, %f48;
	st.shared.f32 	[%r112], %f11;
	sub.f64 	%fd258, %fd90, %fd257;
	add.f64 	%fd93, %fd87, %fd258;
	ld.global.nc.f32 	%f49, [%rd66+12];
	cvt.ftz.f64.f32 	%fd301, %f49;
	add.f64 	%fd259, %fd88, %fd301;
	mul.f64 	%fd260, %fd6, %fd83;
	neg.f64 	%fd261, %fd260;
	fma.rn.f64 	%fd262, %fd19, %fd298, %fd261;
	fma.rn.f64 	%fd300, %fd20, %fd259, %fd262;
	cvt.rn.ftz.f32.f64 	%f14, %fd300;
	cvt.ftz.f64.f32 	%fd96, %f14;
	mul.f64 	%fd263, %fd53, %fd96;
	sub.f64 	%fd264, %fd263, %fd93;
	mul.f64 	%fd97, %fd54, %fd264;
	mul.f64 	%fd265, %fd97, %fd97;
	mul.f64 	%fd266, %fd92, 0d3FEEB851EB851EB8;
	fma.rn.f64 	%fd307, %fd254, %fd265, %fd266;
	setp.leu.f64 	%p41, %fd307, 0d0000000000000000;
	@%p41 bra 	$L__BB0_49;

	sqrt.rn.f64 	%fd268, %fd307;
	div.rn.f64 	%fd269, %fd97, %fd268;
	cvt.rn.ftz.f32.f64 	%f55, %fd269;

$L__BB0_49:
	add.s64 	%rd66, %rd66, 16;
	add.s64 	%rd24, %rd65, 16;
	st.global.f32 	[%rd65+12], %f55;
	rem.s32 	%r113, %r148, %r3;
	shl.b32 	%r114, %r113, 2;
	add.s32 	%r115, %r25, %r114;
	ld.shared.f32 	%f50, [%r115];
	cvt.ftz.f64.f32 	%fd270, %f50;
	st.shared.f32 	[%r115], %f14;
	sub.f64 	%fd271, %fd96, %fd270;
	add.f64 	%fd299, %fd93, %fd271;
	add.s32 	%r148, %r148, 4;
	add.s32 	%r147, %r147, 4;
	setp.lt.s32 	%p42, %r147, %r52;
	mov.u64 	%rd65, %rd24;
	@%p42 bra 	$L__BB0_41;

$L__BB0_50:
	ret;

}
	
.visible .entry trendflex_many_series_one_param_f32(
	.param .u64 trendflex_many_series_one_param_f32_param_0,
	.param .u64 trendflex_many_series_one_param_f32_param_1,
	.param .u32 trendflex_many_series_one_param_f32_param_2,
	.param .u32 trendflex_many_series_one_param_f32_param_3,
	.param .u32 trendflex_many_series_one_param_f32_param_4,
	.param .u64 trendflex_many_series_one_param_f32_param_5,
	.param .u64 trendflex_many_series_one_param_f32_param_6
)
{
	.reg .pred 	%p<40>;
	.reg .f32 	%f<229>;
	.reg .b32 	%r<117>;
	.reg .f64 	%fd<96>;
	.reg .b64 	%rd<96>;


	ld.param.u64 	%rd48, [trendflex_many_series_one_param_f32_param_0];
	ld.param.u64 	%rd47, [trendflex_many_series_one_param_f32_param_1];
	ld.param.u32 	%r48, [trendflex_many_series_one_param_f32_param_2];
	ld.param.u32 	%r49, [trendflex_many_series_one_param_f32_param_3];
	ld.param.u32 	%r50, [trendflex_many_series_one_param_f32_param_4];
	ld.param.u64 	%rd49, [trendflex_many_series_one_param_f32_param_5];
	ld.param.u64 	%rd50, [trendflex_many_series_one_param_f32_param_6];
	cvta.to.global.u64 	%rd1, %rd50;
	cvta.to.global.u64 	%rd2, %rd49;
	cvta.to.global.u64 	%rd3, %rd48;
	mov.u32 	%r51, %ntid.x;
	mov.u32 	%r52, %ctaid.x;
	mov.u32 	%r53, %tid.x;
	mad.lo.s32 	%r1, %r52, %r51, %r53;
	setp.ge.s32 	%p1, %r1, %r48;
	@%p1 bra 	$L__BB1_48;

	setp.lt.s32 	%p2, %r49, 1;
	setp.lt.s32 	%p3, %r50, 1;
	or.pred  	%p4, %p2, %p3;
	setp.ge.s32 	%p5, %r50, %r49;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB1_48;

	cvta.to.global.u64 	%rd51, %rd47;
	mul.wide.s32 	%rd52, %r1, 4;
	add.s64 	%rd53, %rd51, %rd52;
	ld.global.nc.u32 	%r2, [%rd53];
	setp.lt.s32 	%p7, %r2, 0;
	setp.ge.s32 	%p8, %r2, %r49;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	$L__BB1_48;

	sub.s32 	%r3, %r49, %r2;
	setp.lt.s32 	%p10, %r3, %r50;
	@%p10 bra 	$L__BB1_48;

	cvt.rn.f32.s32 	%f1, %r50;
	mul.ftz.f32 	%f87, %f1, 0f3F000000;
	mov.f32 	%f88, 0f3F000000;
	copysign.f32 	%f89, %f87, %f88;
	add.rz.ftz.f32 	%f90, %f87, %f89;
	cvt.rzi.f32.f32 	%f91, %f90;
	cvt.rzi.ftz.s32.f32 	%r54, %f91;
	max.s32 	%r4, %r54, 1;
	setp.lt.s32 	%p11, %r3, %r4;
	@%p11 bra 	$L__BB1_48;

	cvt.rn.f64.s32 	%fd18, %r4;
	rcp.rn.f64 	%fd19, %fd18;
	mov.f64 	%fd20, 0d3FF0000000000000;
	mul.f64 	%fd1, %fd19, 0d4011C5831ADD62E4;
	mul.f64 	%fd21, %fd19, 0dC011C5831ADD62E4;
	mov.f64 	%fd22, 0d4338000000000000;
	mov.f64 	%fd23, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd24, %fd21, %fd23, %fd22;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r5, %temp}, %fd24;
	}
	mov.f64 	%fd25, 0dC338000000000000;
	add.rn.f64 	%fd26, %fd24, %fd25;
	mov.f64 	%fd27, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd28, %fd26, %fd27, %fd21;
	mov.f64 	%fd29, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd30, %fd26, %fd29, %fd28;
	mov.f64 	%fd31, 0d3E928AF3FCA213EA;
	mov.f64 	%fd32, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd33, %fd32, %fd30, %fd31;
	mov.f64 	%fd34, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd35, %fd33, %fd30, %fd34;
	mov.f64 	%fd36, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd37, %fd35, %fd30, %fd36;
	mov.f64 	%fd38, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd39, %fd37, %fd30, %fd38;
	mov.f64 	%fd40, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd41, %fd39, %fd30, %fd40;
	mov.f64 	%fd42, 0d3F81111111122322;
	fma.rn.f64 	%fd43, %fd41, %fd30, %fd42;
	mov.f64 	%fd44, 0d3FA55555555502A1;
	fma.rn.f64 	%fd45, %fd43, %fd30, %fd44;
	mov.f64 	%fd46, 0d3FC5555555555511;
	fma.rn.f64 	%fd47, %fd45, %fd30, %fd46;
	mov.f64 	%fd48, 0d3FE000000000000B;
	fma.rn.f64 	%fd49, %fd47, %fd30, %fd48;
	fma.rn.f64 	%fd50, %fd49, %fd30, %fd20;
	fma.rn.f64 	%fd51, %fd50, %fd30, %fd20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r6, %temp}, %fd51;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r7}, %fd51;
	}
	shl.b32 	%r55, %r5, 20;
	add.s32 	%r56, %r7, %r55;
	mov.b64 	%fd92, {%r6, %r56};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r57}, %fd21;
	}
	mov.b32 	%f92, %r57;
	abs.ftz.f32 	%f2, %f92;
	setp.lt.ftz.f32 	%p12, %f2, 0f4086232B;
	@%p12 bra 	$L__BB1_8;

	setp.gt.f64 	%p13, %fd1, 0d8000000000000000;
	mov.f64 	%fd52, 0d7FF0000000000000;
	sub.f64 	%fd53, %fd52, %fd1;
	selp.f64 	%fd92, 0d0000000000000000, %fd53, %p13;
	setp.geu.ftz.f32 	%p14, %f2, 0f40874800;
	@%p14 bra 	$L__BB1_8;

	shr.u32 	%r58, %r5, 31;
	add.s32 	%r59, %r5, %r58;
	shr.s32 	%r60, %r59, 1;
	shl.b32 	%r61, %r60, 20;
	add.s32 	%r62, %r7, %r61;
	mov.b64 	%fd54, {%r6, %r62};
	sub.s32 	%r63, %r5, %r60;
	shl.b32 	%r64, %r63, 20;
	add.s32 	%r65, %r64, 1072693248;
	mov.u32 	%r66, 0;
	mov.b64 	%fd55, {%r66, %r65};
	mul.f64 	%fd92, %fd54, %fd55;

$L__BB1_8:
	abs.f64 	%fd6, %fd1;
	setp.eq.f64 	%p15, %fd6, 0d7FF0000000000000;
	@%p15 bra 	$L__BB1_11;
	bra.uni 	$L__BB1_9;

$L__BB1_11:
	mov.f64 	%fd64, 0d0000000000000000;
	mul.rn.f64 	%fd93, %fd1, %fd64;
	mov.u32 	%r102, 0;
	bra.uni 	$L__BB1_12;

$L__BB1_9:
	mul.f64 	%fd56, %fd1, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64 	%r102, %fd56;
	cvt.rn.f64.s32 	%fd57, %r102;
	neg.f64 	%fd58, %fd57;
	mov.f64 	%fd59, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd60, %fd58, %fd59, %fd1;
	mov.f64 	%fd61, 0d3C91A62633145C00;
	fma.rn.f64 	%fd62, %fd58, %fd61, %fd60;
	mov.f64 	%fd63, 0d397B839A252049C0;
	fma.rn.f64 	%fd93, %fd58, %fd63, %fd62;
	setp.ltu.f64 	%p16, %fd6, 0d41E0000000000000;
	@%p16 bra 	$L__BB1_12;

	{ 
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1;
	.param .align 8 .b8 retval0[16];
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0
	);
	ld.param.f64 	%fd93, [retval0+0];
	ld.param.b32 	%r102, [retval0+8];
	} 

$L__BB1_12:
	add.s32 	%r11, %r102, 1;
	and.b32  	%r68, %r11, 1;
	shl.b32 	%r69, %r11, 3;
	and.b32  	%r70, %r69, 8;
	mul.wide.u32 	%rd54, %r70, 8;
	mov.u64 	%rd55, __cudart_sin_cos_coeffs;
	add.s64 	%rd56, %rd55, %rd54;
	setp.eq.s32 	%p17, %r68, 0;
	selp.f64 	%fd65, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p17;
	ld.global.nc.v2.f64 	{%fd66, %fd67}, [%rd56];
	mul.rn.f64 	%fd11, %fd93, %fd93;
	fma.rn.f64 	%fd70, %fd65, %fd11, %fd66;
	fma.rn.f64 	%fd71, %fd70, %fd11, %fd67;
	ld.global.nc.v2.f64 	{%fd72, %fd73}, [%rd56+16];
	fma.rn.f64 	%fd76, %fd71, %fd11, %fd72;
	fma.rn.f64 	%fd77, %fd76, %fd11, %fd73;
	ld.global.nc.v2.f64 	{%fd78, %fd79}, [%rd56+32];
	fma.rn.f64 	%fd82, %fd77, %fd11, %fd78;
	fma.rn.f64 	%fd12, %fd82, %fd11, %fd79;
	fma.rn.f64 	%fd95, %fd12, %fd93, %fd93;
	@%p17 bra 	$L__BB1_14;

	mov.f64 	%fd83, 0d3FF0000000000000;
	fma.rn.f64 	%fd95, %fd12, %fd11, %fd83;

$L__BB1_14:
	and.b32  	%r71, %r11, 2;
	setp.eq.s32 	%p18, %r71, 0;
	@%p18 bra 	$L__BB1_16;

	mov.f64 	%fd84, 0d0000000000000000;
	mov.f64 	%fd85, 0dBFF0000000000000;
	fma.rn.f64 	%fd95, %fd95, %fd85, %fd84;

$L__BB1_16:
	ld.param.u32 	%r94, [trendflex_many_series_one_param_f32_param_4];
	mul.f64 	%fd86, %fd92, %fd92;
	add.f64 	%fd87, %fd86, 0d3FF0000000000000;
	add.f64 	%fd88, %fd92, %fd92;
	mul.f64 	%fd89, %fd88, %fd95;
	sub.f64 	%fd90, %fd87, %fd89;
	mul.f64 	%fd91, %fd90, 0d3FE0000000000000;
	cvt.rn.ftz.f32.f64 	%f3, %fd86;
	cvt.rn.ftz.f32.f64 	%f4, %fd89;
	cvt.rn.ftz.f32.f64 	%f5, %fd91;
	add.s32 	%r114, %r2, %r94;
	min.s32 	%r13, %r114, %r49;
	setp.lt.s32 	%p19, %r13, 1;
	@%p19 bra 	$L__BB1_23;

	add.s32 	%r73, %r13, -1;
	and.b32  	%r106, %r13, 3;
	setp.lt.u32 	%p20, %r73, 3;
	mov.u32 	%r105, 0;
	@%p20 bra 	$L__BB1_20;

	sub.s32 	%r104, %r13, %r106;
	mul.wide.s32 	%rd4, %r48, 4;
	mov.u32 	%r105, 0;

$L__BB1_19:
	mad.lo.s32 	%r75, %r105, %r48, %r1;
	mul.wide.s32 	%rd57, %r75, 4;
	add.s64 	%rd58, %rd1, %rd57;
	mov.u32 	%r76, 2147483647;
	st.global.u32 	[%rd58], %r76;
	add.s64 	%rd59, %rd58, %rd4;
	st.global.u32 	[%rd59], %r76;
	add.s64 	%rd60, %rd59, %rd4;
	st.global.u32 	[%rd60], %r76;
	add.s64 	%rd61, %rd60, %rd4;
	st.global.u32 	[%rd61], %r76;
	add.s32 	%r105, %r105, 4;
	add.s32 	%r104, %r104, -4;
	setp.ne.s32 	%p21, %r104, 0;
	@%p21 bra 	$L__BB1_19;

$L__BB1_20:
	setp.eq.s32 	%p22, %r106, 0;
	@%p22 bra 	$L__BB1_23;

	mad.lo.s32 	%r77, %r105, %r48, %r1;
	mul.wide.s32 	%rd62, %r77, 4;
	add.s64 	%rd87, %rd1, %rd62;
	mul.wide.s32 	%rd6, %r48, 4;

$L__BB1_22:
	.pragma "nounroll";
	mov.u32 	%r78, 2147483647;
	st.global.u32 	[%rd87], %r78;
	add.s64 	%rd87, %rd87, %rd6;
	add.s32 	%r106, %r106, -1;
	setp.ne.s32 	%p23, %r106, 0;
	@%p23 bra 	$L__BB1_22;

$L__BB1_23:
	setp.ge.s32 	%p24, %r114, %r49;
	@%p24 bra 	$L__BB1_48;

	sub.s32 	%r95, %r49, %r2;
	mad.lo.s32 	%r111, %r2, %r48, %r1;
	mul.wide.s32 	%rd63, %r111, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.global.nc.f32 	%f205, [%rd64];
	add.s64 	%rd65, %rd2, %rd63;
	st.global.f32 	[%rd65], %f205;
	setp.lt.s32 	%p25, %r95, 2;
	@%p25 bra 	$L__BB1_48;

	ld.param.u32 	%r96, [trendflex_many_series_one_param_f32_param_4];
	add.s32 	%r79, %r111, %r48;
	mul.wide.s32 	%rd66, %r79, 4;
	add.s64 	%rd67, %rd3, %rd66;
	ld.global.nc.f32 	%f207, [%rd67];
	add.s64 	%rd68, %rd2, %rd66;
	st.global.f32 	[%rd68], %f207;
	add.ftz.f32 	%f206, %f205, %f207;
	setp.lt.s32 	%p26, %r96, 3;
	mov.f32 	%f208, %f207;
	@%p26 bra 	$L__BB1_32;

	ld.param.u32 	%r101, [trendflex_many_series_one_param_f32_param_4];
	add.s32 	%r24, %r101, -2;
	and.b32  	%r110, %r24, 3;
	add.s32 	%r81, %r101, -3;
	setp.lt.u32 	%p27, %r81, 3;
	mov.u32 	%r109, 2;
	mov.f32 	%f208, %f207;
	@%p27 bra 	$L__BB1_29;

	sub.s32 	%r108, %r24, %r110;
	mul.wide.s32 	%rd9, %r48, 4;
	mov.u32 	%r109, 2;
	mov.f32 	%f208, %f207;

$L__BB1_28:
	add.s32 	%r83, %r109, %r2;
	mad.lo.s32 	%r84, %r83, %r48, %r1;
	mul.wide.s32 	%rd69, %r84, 4;
	add.s64 	%rd70, %rd3, %rd69;
	ld.global.nc.f32 	%f94, [%rd70];
	add.ftz.f32 	%f95, %f208, %f94;
	mul.ftz.f32 	%f96, %f205, %f3;
	neg.ftz.f32 	%f97, %f96;
	fma.rn.ftz.f32 	%f98, %f4, %f207, %f97;
	fma.rn.ftz.f32 	%f99, %f5, %f95, %f98;
	add.s64 	%rd71, %rd2, %rd69;
	st.global.f32 	[%rd71], %f99;
	add.ftz.f32 	%f100, %f206, %f99;
	add.s64 	%rd72, %rd70, %rd9;
	ld.global.nc.f32 	%f101, [%rd72];
	add.ftz.f32 	%f102, %f94, %f101;
	mul.ftz.f32 	%f103, %f207, %f3;
	neg.ftz.f32 	%f104, %f103;
	fma.rn.ftz.f32 	%f105, %f4, %f99, %f104;
	fma.rn.ftz.f32 	%f106, %f5, %f102, %f105;
	add.s64 	%rd73, %rd71, %rd9;
	st.global.f32 	[%rd73], %f106;
	add.ftz.f32 	%f107, %f100, %f106;
	add.s64 	%rd74, %rd72, %rd9;
	ld.global.nc.f32 	%f108, [%rd74];
	add.ftz.f32 	%f109, %f101, %f108;
	mul.ftz.f32 	%f110, %f99, %f3;
	neg.ftz.f32 	%f111, %f110;
	fma.rn.ftz.f32 	%f112, %f4, %f106, %f111;
	fma.rn.ftz.f32 	%f205, %f5, %f109, %f112;
	add.s64 	%rd75, %rd73, %rd9;
	st.global.f32 	[%rd75], %f205;
	add.ftz.f32 	%f113, %f107, %f205;
	add.s64 	%rd76, %rd74, %rd9;
	ld.global.nc.f32 	%f208, [%rd76];
	add.ftz.f32 	%f114, %f108, %f208;
	mul.ftz.f32 	%f115, %f106, %f3;
	neg.ftz.f32 	%f116, %f115;
	fma.rn.ftz.f32 	%f117, %f4, %f205, %f116;
	fma.rn.ftz.f32 	%f207, %f5, %f114, %f117;
	add.s64 	%rd77, %rd75, %rd9;
	st.global.f32 	[%rd77], %f207;
	add.ftz.f32 	%f206, %f113, %f207;
	add.s32 	%r109, %r109, 4;
	add.s32 	%r108, %r108, -4;
	setp.ne.s32 	%p28, %r108, 0;
	@%p28 bra 	$L__BB1_28;

$L__BB1_29:
	setp.eq.s32 	%p29, %r110, 0;
	@%p29 bra 	$L__BB1_32;

	add.s32 	%r85, %r109, %r2;
	mad.lo.s32 	%r86, %r48, %r85, %r1;
	mul.wide.s32 	%rd78, %r86, 4;
	add.s64 	%rd89, %rd2, %rd78;
	mul.wide.s32 	%rd11, %r48, 4;
	add.s64 	%rd88, %rd3, %rd78;
	mov.f32 	%f201, %f208;
	mov.f32 	%f204, %f205;

$L__BB1_31:
	.pragma "nounroll";
	mov.f32 	%f205, %f207;
	ld.global.nc.f32 	%f208, [%rd88];
	add.ftz.f32 	%f118, %f201, %f208;
	mul.ftz.f32 	%f119, %f204, %f3;
	neg.ftz.f32 	%f120, %f119;
	fma.rn.ftz.f32 	%f121, %f4, %f205, %f120;
	fma.rn.ftz.f32 	%f207, %f5, %f118, %f121;
	st.global.f32 	[%rd89], %f207;
	add.ftz.f32 	%f206, %f206, %f207;
	add.s64 	%rd89, %rd89, %rd11;
	add.s64 	%rd88, %rd88, %rd11;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p30, %r110, 0;
	mov.f32 	%f201, %f208;
	mov.f32 	%f204, %f205;
	@%p30 bra 	$L__BB1_31;

$L__BB1_32:
	sub.s32 	%r98, %r49, %r2;
	ld.param.u32 	%r97, [trendflex_many_series_one_param_f32_param_4];
	rcp.approx.ftz.f32 	%f36, %f1;
	sub.s32 	%r87, %r98, %r97;
	and.b32  	%r113, %r87, 3;
	setp.eq.s32 	%p31, %r113, 0;
	mov.f32 	%f215, 0f00000000;
	@%p31 bra 	$L__BB1_37;

	mad.lo.s32 	%r88, %r48, %r114, %r1;
	mul.wide.s32 	%rd79, %r88, 4;
	add.s64 	%rd92, %rd2, %rd79;
	mul.wide.s32 	%rd18, %r48, 4;
	add.s64 	%rd91, %rd1, %rd79;
	add.s64 	%rd90, %rd3, %rd79;
	mov.f32 	%f123, 0f00000000;
	mov.f32 	%f215, %f123;
	mov.f32 	%f210, %f208;
	mov.f32 	%f213, %f205;

$L__BB1_34:
	.pragma "nounroll";
	mov.f32 	%f205, %f207;
	ld.global.nc.f32 	%f208, [%rd90];
	add.ftz.f32 	%f125, %f210, %f208;
	mul.ftz.f32 	%f126, %f213, %f3;
	neg.ftz.f32 	%f127, %f126;
	fma.rn.ftz.f32 	%f128, %f4, %f205, %f127;
	fma.rn.ftz.f32 	%f207, %f5, %f125, %f128;
	mul.ftz.f32 	%f129, %f207, %f1;
	sub.ftz.f32 	%f130, %f129, %f206;
	mul.ftz.f32 	%f44, %f36, %f130;
	mul.ftz.f32 	%f131, %f44, %f44;
	mul.ftz.f32 	%f132, %f215, 0f3F75C28F;
	mov.f32 	%f133, 0f3D23D70A;
	fma.rn.ftz.f32 	%f215, %f133, %f131, %f132;
	setp.leu.ftz.f32 	%p32, %f215, 0f00000000;
	mov.f32 	%f214, %f123;
	@%p32 bra 	$L__BB1_36;

	rsqrt.approx.ftz.f32 	%f134, %f215;
	mul.ftz.f32 	%f214, %f44, %f134;

$L__BB1_36:
	st.global.f32 	[%rd91], %f214;
	mul.wide.s32 	%rd80, %r111, 4;
	add.s64 	%rd81, %rd2, %rd80;
	ld.global.f32 	%f135, [%rd81];
	sub.ftz.f32 	%f136, %f207, %f135;
	add.ftz.f32 	%f206, %f206, %f136;
	st.global.f32 	[%rd92], %f207;
	add.s32 	%r114, %r114, 1;
	add.s32 	%r111, %r111, %r48;
	add.s64 	%rd92, %rd92, %rd18;
	add.s64 	%rd91, %rd91, %rd18;
	add.s64 	%rd90, %rd90, %rd18;
	add.s32 	%r113, %r113, -1;
	setp.ne.s32 	%p33, %r113, 0;
	mov.f32 	%f210, %f208;
	mov.f32 	%f213, %f205;
	@%p33 bra 	$L__BB1_34;

$L__BB1_37:
	ld.param.u32 	%r99, [trendflex_many_series_one_param_f32_param_4];
	not.b32 	%r89, %r2;
	add.s32 	%r90, %r89, %r49;
	sub.s32 	%r91, %r90, %r99;
	setp.lt.u32 	%p34, %r91, 3;
	@%p34 bra 	$L__BB1_48;

	ld.param.u32 	%r100, [trendflex_many_series_one_param_f32_param_4];
	shl.b32 	%r42, %r48, 2;
	sub.s32 	%r92, %r114, %r100;
	mad.lo.s32 	%r115, %r48, %r92, %r1;
	mad.lo.s32 	%r93, %r114, %r48, %r1;
	mul.wide.s32 	%rd82, %r93, 4;
	add.s64 	%rd93, %rd1, %rd82;
	mul.wide.s32 	%rd28, %r48, 4;
	add.s64 	%rd94, %rd3, %rd82;
	add.s64 	%rd95, %rd2, %rd82;

$L__BB1_39:
	ld.global.nc.f32 	%f59, [%rd94];
	add.ftz.f32 	%f138, %f208, %f59;
	mul.ftz.f32 	%f139, %f205, %f3;
	neg.ftz.f32 	%f140, %f139;
	fma.rn.ftz.f32 	%f141, %f4, %f207, %f140;
	fma.rn.ftz.f32 	%f60, %f5, %f138, %f141;
	mul.ftz.f32 	%f142, %f60, %f1;
	sub.ftz.f32 	%f143, %f142, %f206;
	mul.ftz.f32 	%f61, %f36, %f143;
	mul.ftz.f32 	%f144, %f61, %f61;
	mul.ftz.f32 	%f145, %f215, 0f3F75C28F;
	mov.f32 	%f146, 0f3D23D70A;
	fma.rn.ftz.f32 	%f62, %f146, %f144, %f145;
	setp.leu.ftz.f32 	%p35, %f62, 0f00000000;
	mov.f32 	%f226, 0f00000000;
	mov.f32 	%f225, %f226;
	@%p35 bra 	$L__BB1_41;

	rsqrt.approx.ftz.f32 	%f147, %f62;
	mul.ftz.f32 	%f225, %f61, %f147;

$L__BB1_41:
	st.global.f32 	[%rd93], %f225;
	mul.wide.s32 	%rd83, %r115, 4;
	add.s64 	%rd34, %rd2, %rd83;
	ld.global.f32 	%f149, [%rd34];
	sub.ftz.f32 	%f150, %f60, %f149;
	add.ftz.f32 	%f65, %f206, %f150;
	st.global.f32 	[%rd95], %f60;
	add.s64 	%rd35, %rd94, %rd28;
	ld.global.nc.f32 	%f66, [%rd35];
	add.ftz.f32 	%f151, %f59, %f66;
	mul.ftz.f32 	%f152, %f207, %f3;
	neg.ftz.f32 	%f153, %f152;
	fma.rn.ftz.f32 	%f154, %f4, %f60, %f153;
	fma.rn.ftz.f32 	%f67, %f5, %f151, %f154;
	mul.ftz.f32 	%f155, %f67, %f1;
	sub.ftz.f32 	%f156, %f155, %f65;
	mul.ftz.f32 	%f68, %f36, %f156;
	mul.ftz.f32 	%f157, %f68, %f68;
	mul.ftz.f32 	%f158, %f62, 0f3F75C28F;
	fma.rn.ftz.f32 	%f69, %f146, %f157, %f158;
	setp.leu.ftz.f32 	%p36, %f69, 0f00000000;
	@%p36 bra 	$L__BB1_43;

	rsqrt.approx.ftz.f32 	%f160, %f69;
	mul.ftz.f32 	%f226, %f68, %f160;

$L__BB1_43:
	add.s64 	%rd36, %rd93, %rd28;
	st.global.f32 	[%rd36], %f226;
	add.s64 	%rd37, %rd34, %rd28;
	ld.global.f32 	%f162, [%rd37];
	sub.ftz.f32 	%f163, %f67, %f162;
	add.ftz.f32 	%f72, %f65, %f163;
	add.s64 	%rd38, %rd95, %rd28;
	st.global.f32 	[%rd38], %f67;
	add.s64 	%rd39, %rd35, %rd28;
	ld.global.nc.f32 	%f73, [%rd39];
	add.ftz.f32 	%f164, %f66, %f73;
	mul.ftz.f32 	%f165, %f60, %f3;
	neg.ftz.f32 	%f166, %f165;
	fma.rn.ftz.f32 	%f167, %f4, %f67, %f166;
	fma.rn.ftz.f32 	%f205, %f5, %f164, %f167;
	mul.ftz.f32 	%f168, %f205, %f1;
	sub.ftz.f32 	%f169, %f168, %f72;
	mul.ftz.f32 	%f75, %f36, %f169;
	mul.ftz.f32 	%f170, %f75, %f75;
	mul.ftz.f32 	%f171, %f69, 0f3F75C28F;
	mov.f32 	%f172, 0f3D23D70A;
	fma.rn.ftz.f32 	%f76, %f172, %f170, %f171;
	setp.leu.ftz.f32 	%p37, %f76, 0f00000000;
	mov.f32 	%f228, 0f00000000;
	mov.f32 	%f227, %f228;
	@%p37 bra 	$L__BB1_45;

	rsqrt.approx.ftz.f32 	%f173, %f76;
	mul.ftz.f32 	%f227, %f75, %f173;

$L__BB1_45:
	add.s64 	%rd40, %rd36, %rd28;
	st.global.f32 	[%rd40], %f227;
	add.s64 	%rd41, %rd37, %rd28;
	ld.global.f32 	%f175, [%rd41];
	sub.ftz.f32 	%f176, %f205, %f175;
	add.ftz.f32 	%f79, %f72, %f176;
	add.s64 	%rd42, %rd38, %rd28;
	st.global.f32 	[%rd42], %f205;
	add.s64 	%rd43, %rd39, %rd28;
	ld.global.nc.f32 	%f208, [%rd43];
	add.ftz.f32 	%f177, %f73, %f208;
	mul.ftz.f32 	%f178, %f67, %f3;
	neg.ftz.f32 	%f179, %f178;
	fma.rn.ftz.f32 	%f180, %f4, %f205, %f179;
	fma.rn.ftz.f32 	%f207, %f5, %f177, %f180;
	mul.ftz.f32 	%f181, %f207, %f1;
	sub.ftz.f32 	%f182, %f181, %f79;
	mul.ftz.f32 	%f82, %f36, %f182;
	mul.ftz.f32 	%f183, %f82, %f82;
	mul.ftz.f32 	%f184, %f76, 0f3F75C28F;
	fma.rn.ftz.f32 	%f215, %f172, %f183, %f184;
	setp.leu.ftz.f32 	%p38, %f215, 0f00000000;
	@%p38 bra 	$L__BB1_47;

	rsqrt.approx.ftz.f32 	%f186, %f215;
	mul.ftz.f32 	%f228, %f82, %f186;

$L__BB1_47:
	add.s64 	%rd94, %rd43, %rd28;
	add.s64 	%rd84, %rd40, %rd28;
	add.s64 	%rd93, %rd84, %rd28;
	st.global.f32 	[%rd84], %f228;
	add.s64 	%rd85, %rd41, %rd28;
	ld.global.f32 	%f187, [%rd85];
	sub.ftz.f32 	%f188, %f207, %f187;
	add.ftz.f32 	%f206, %f79, %f188;
	add.s64 	%rd86, %rd42, %rd28;
	add.s64 	%rd95, %rd86, %rd28;
	st.global.f32 	[%rd86], %f207;
	add.s32 	%r115, %r115, %r42;
	add.s32 	%r114, %r114, 4;
	setp.lt.s32 	%p39, %r114, %r49;
	@%p39 bra 	$L__BB1_39;

$L__BB1_48:
	ret;

}
.func  (.param .align 8 .b8 func_retval0[16]) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0
)
{
	.local .align 8 .b8 	__local_depot2[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<77>;


	mov.u64 	%SPL, __local_depot2;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	add.u64 	%rd1, %SPL, 0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	shr.u32 	%r12, %r1, 20;
	and.b32  	%r2, %r12, 2047;
	setp.eq.s32 	%p1, %r2, 2047;
	mov.u32 	%r37, 0;
	@%p1 bra 	$L__BB2_7;

	add.s32 	%r13, %r2, -1024;
	shr.u32 	%r14, %r13, 6;
	mov.u32 	%r15, 16;
	sub.s32 	%r16, %r15, %r14;
	mov.u32 	%r17, 15;
	sub.s32 	%r3, %r17, %r14;
	mov.u32 	%r18, 19;
	sub.s32 	%r19, %r18, %r14;
	setp.gt.s32 	%p2, %r16, 14;
	selp.b32 	%r4, 18, %r19, %p2;
	setp.gt.s32 	%p3, %r16, %r4;
	mov.u64 	%rd74, 0;
	mov.u32 	%r36, %r3;
	@%p3 bra 	$L__BB2_4;

	mul.wide.s32 	%rd21, %r3, 8;
	mov.u64 	%rd22, __cudart_i2opi_d;
	add.s64 	%rd72, %rd22, %rd21;
	mov.b64 	%rd23, %fd4;
	shl.b64 	%rd24, %rd23, 11;
	or.b64  	%rd3, %rd24, -9223372036854775808;
	mov.u64 	%rd74, 0;
	mov.u64 	%rd71, %rd1;
	mov.u32 	%r36, %r3;

$L__BB2_3:
	.pragma "nounroll";
	ld.global.nc.u64 	%rd25, [%rd72];
	{
	.reg .u32 %r0, %r1, %r2, %r3, %alo, %ahi, %blo, %bhi, %clo, %chi;
	mov.b64 	{%alo,%ahi}, %rd25;
	mov.b64 	{%blo,%bhi}, %rd3;
	mov.b64 	{%clo,%chi}, %rd74;
	mad.lo.cc.u32 	%r0, %alo, %blo, %clo;
	madc.hi.cc.u32 	%r1, %alo, %blo, %chi;
	madc.hi.u32 	%r2, %alo, %bhi, 0;
	mad.lo.cc.u32 	%r1, %alo, %bhi, %r1;
	madc.hi.cc.u32 	%r2, %ahi, %blo, %r2;
	madc.hi.u32 	%r3, %ahi, %bhi, 0;
	mad.lo.cc.u32 	%r1, %ahi, %blo, %r1;
	madc.lo.cc.u32 	%r2, %ahi, %bhi, %r2;
	addc.u32 	%r3, %r3, 0;
	mov.b64 	%rd26, {%r0,%r1};
	mov.b64 	%rd74, {%r2,%r3};
	}
	st.local.u64 	[%rd71], %rd26;
	add.s64 	%rd72, %rd72, 8;
	add.s64 	%rd71, %rd71, 8;
	add.s32 	%r36, %r36, 1;
	setp.lt.s32 	%p4, %r36, %r4;
	@%p4 bra 	$L__BB2_3;

$L__BB2_4:
	sub.s32 	%r20, %r36, %r3;
	mul.wide.s32 	%rd27, %r20, 8;
	add.s64 	%rd28, %rd1, %rd27;
	st.local.u64 	[%rd28], %rd74;
	add.s32 	%r22, %r12, -1024;
	and.b32  	%r8, %r22, 63;
	ld.local.u64 	%rd76, [%rd1+16];
	ld.local.u64 	%rd75, [%rd1+24];
	setp.eq.s32 	%p5, %r8, 0;
	@%p5 bra 	$L__BB2_6;

	mov.u32 	%r23, 64;
	sub.s32 	%r24, %r23, %r8;
	shl.b64 	%rd29, %rd75, %r8;
	shr.u64 	%rd30, %rd76, %r24;
	or.b64  	%rd75, %rd29, %rd30;
	shl.b64 	%rd31, %rd76, %r8;
	ld.local.u64 	%rd32, [%rd1+8];
	shr.u64 	%rd33, %rd32, %r24;
	or.b64  	%rd76, %rd33, %rd31;

$L__BB2_6:
	shr.u64 	%rd34, %rd75, 62;
	cvt.u32.u64 	%r25, %rd34;
	shr.u64 	%rd35, %rd76, 62;
	shl.b64 	%rd36, %rd75, 2;
	or.b64  	%rd37, %rd35, %rd36;
	shr.u64 	%rd38, %rd75, 61;
	cvt.u32.u64 	%r26, %rd38;
	and.b32  	%r27, %r26, 1;
	add.s32 	%r28, %r27, %r25;
	and.b32  	%r29, %r1, -2147483648;
	setp.eq.s32 	%p6, %r29, 0;
	neg.s32 	%r30, %r28;
	selp.b32 	%r37, %r28, %r30, %p6;
	setp.eq.s32 	%p7, %r27, 0;
	shl.b64 	%rd39, %rd76, 2;
	mov.u64 	%rd40, 0;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %a0, %a1, %a2, %a3, %b0, %b1, %b2, %b3;
	mov.b64 	{%a0,%a1}, %rd40;
	mov.b64 	{%a2,%a3}, %rd40;
	mov.b64 	{%b0,%b1}, %rd39;
	mov.b64 	{%b2,%b3}, %rd37;
	sub.cc.u32 	%r0, %a0, %b0;
	subc.cc.u32 	%r1, %a1, %b1;
	subc.cc.u32 	%r2, %a2, %b2;
	subc.u32 	%r3, %a3, %b3;
	mov.b64 	%rd41, {%r0,%r1};
	mov.b64 	%rd42, {%r2,%r3};
	}
	xor.b32  	%r31, %r29, -2147483648;
	selp.b64 	%rd43, %rd37, %rd42, %p7;
	selp.b64 	%rd44, %rd39, %rd41, %p7;
	selp.b32 	%r32, %r29, %r31, %p7;
	clz.b64 	%r33, %rd43;
	cvt.u64.u32 	%rd45, %r33;
	setp.eq.s64 	%p8, %rd45, 0;
	shl.b64 	%rd46, %rd43, %r33;
	mov.u64 	%rd47, 64;
	sub.s64 	%rd48, %rd47, %rd45;
	cvt.u32.u64 	%r34, %rd48;
	shr.u64 	%rd49, %rd44, %r34;
	or.b64  	%rd50, %rd49, %rd46;
	selp.b64 	%rd51, %rd43, %rd50, %p8;
	mov.u64 	%rd52, -3958705157555305931;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %alo, %ahi, %blo, %bhi;
	mov.b64 	{%alo,%ahi}, %rd51;
	mov.b64 	{%blo,%bhi}, %rd52;
	mul.lo.u32 	%r0, %alo, %blo;
	mul.hi.u32 	%r1, %alo, %blo;
	mad.lo.cc.u32 	%r1, %alo, %bhi, %r1;
	madc.hi.u32 	%r2, %alo, %bhi, 0;
	mad.lo.cc.u32 	%r1, %ahi, %blo, %r1;
	madc.hi.cc.u32 	%r2, %ahi, %blo, %r2;
	madc.hi.u32 	%r3, %ahi, %bhi, 0;
	mad.lo.cc.u32 	%r2, %ahi, %bhi, %r2;
	addc.u32 	%r3, %r3, 0;
	mov.b64 	%rd53, {%r0,%r1};
	mov.b64 	%rd54, {%r2,%r3};
	}
	setp.gt.s64 	%p9, %rd54, 0;
	{
	.reg .u32 %r0, %r1, %r2, %r3, %a0, %a1, %a2, %a3, %b0, %b1, %b2, %b3;
	mov.b64 	{%a0,%a1}, %rd53;
	mov.b64 	{%a2,%a3}, %rd54;
	mov.b64 	{%b0,%b1}, %rd53;
	mov.b64 	{%b2,%b3}, %rd54;
	add.cc.u32 	%r0, %a0, %b0;
	addc.cc.u32 	%r1, %a1, %b1;
	addc.cc.u32 	%r2, %a2, %b2;
	addc.u32 	%r3, %a3, %b3;
	mov.b64 	%rd55, {%r0,%r1};
	mov.b64 	%rd56, {%r2,%r3};
	}
	selp.b64 	%rd57, %rd56, %rd54, %p9;
	selp.u64 	%rd58, 1, 0, %p9;
	add.s64 	%rd59, %rd45, %rd58;
	cvt.u64.u32 	%rd60, %r32;
	shl.b64 	%rd61, %rd60, 32;
	shl.b64 	%rd62, %rd59, 52;
	mov.u64 	%rd63, 4602678819172646912;
	sub.s64 	%rd64, %rd63, %rd62;
	add.s64 	%rd65, %rd57, 1;
	shr.u64 	%rd66, %rd65, 10;
	add.s64 	%rd67, %rd66, 1;
	shr.u64 	%rd68, %rd67, 1;
	add.s64 	%rd69, %rd64, %rd68;
	or.b64  	%rd70, %rd69, %rd61;
	mov.b64 	%fd4, %rd70;

$L__BB2_7:
	st.param.f64 	[func_retval0+0], %fd4;
	st.param.b32 	[func_retval0+8], %r37;
	ret;

}

